Index: shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java
===================================================================
--- shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java	(revision 116)
+++ shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java	(working copy)
@@ -466,6 +466,8 @@
 
     RecordReader getRecordReader(JobConf job, InputSplitShim split, Reporter reporter,
         Class<RecordReader<K, V>> rrClass) throws IOException;
+
+    //void setMaxSplitSize(long maxSplitSize);
   }
 
   public HCatHadoopShims getHCatShim();
Index: build.properties
===================================================================
--- build.properties	(revision 116)
+++ build.properties	(working copy)
@@ -77,8 +77,8 @@
 # full profile
 #iterate.hive.full.all=ql
 iterate.hive.full.all=ant,shims,common,serde,metastore,ql,contrib,service,cli,jdbc,beeline,hwi,hbase-handler,testutils
-#iterate.hive.full.modules=ant,shims,common,serde,metastore,ql,contrib,service,cli,jdbc,beeline,hwi,hbase-handler,testutils
-iterate.hive.full.modules=metastore,ql
+iterate.hive.full.modules=ant,shims,common,serde,metastore,ql,contrib,service,cli,jdbc,beeline,hwi,hbase-handler,testutils
+#iterate.hive.full.modules=ql
 iterate.hive.full.tests=ql,contrib,hbase-handler,hwi,jdbc,beeline,metastore,odbc,serde,service,hcatalog
 iterate.hive.full.thrift=ql,service,metastore,serde
 iterate.hive.full.protobuf=ql
Index: build.xml
===================================================================
--- build.xml	(revision 116)
+++ build.xml	(working copy)
@@ -230,7 +230,7 @@
   <target name="ivy-download" depends="ivy-init-dirs"
           description="To download ivy" unless="offline">
     <echo message="Project: ${ant.project.name}"/>
-    <!--<get src="${ivy_repo_url}" dest="${ivy.jar}" usetimestamp="true"/>-->
+    <get src="${ivy_repo_url}" dest="${ivy.jar}" usetimestamp="true"/>
   </target>
   
   <!--
@@ -1459,5 +1459,5 @@
         input.file="${mvn.pom.dir}/hive-shims-${version}.pom"
         output.file="${mvn.pom.dir}/hive-shims-${version}.pom.asc"
         gpg.passphrase="${gpg.passphrase}"/>
-  </target>  
+  </target>
 </project>
Index: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
===================================================================
--- metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java	(revision 116)
+++ metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java	(working copy)
@@ -1300,17 +1300,31 @@
       ,Calendar  startDate , Calendar endDate) throws IOException{
     FileStatus srcs[] = fs.globStatus( new Path(path));
     Arrays.sort(srcs);
+    long maxtime = 0;
+    Path lastNewPath = null;
     for( FileStatus src : srcs ){
       if( src.isDir()){
         addPath( fs , path+"/*", retPathList ,startDate ,endDate);
       }else{
         long time = src.getModificationTime();
         Date currentDate = new Date(time);
-        if( currentDate.after(startDate.getTime())  && currentDate.before(endDate.getTime())){
+        LOG.info( "file modification time is: " + currentDate.toString());
+        LOG.info( "start time is: " + startDate.getTime().toString() );
+        LOG.info( "end time is: "+ endDate.getTime().toString());
+        //if( currentDate.after(startDate.getTime())  && currentDate.before(endDate.getTime())){
+        if( currentDate.after(startDate.getTime()) ){
           retPathList.add(src.getPath());
         }
+      //for test, only the last modification file is new file
+//        if(time > maxtime){
+//          maxtime = time;
+//          lastNewPath = src.getPath();
+//        }
       }
+
+
     }
+//    retPathList.add(lastNewPath);
   }
 
 
Index: build-common.xml
===================================================================
--- build-common.xml	(revision 116)
+++ build-common.xml	(working copy)
@@ -153,7 +153,7 @@
     <echo message="Project: ${ant.project.name}"/>
 	<ivy:retrieve settingsRef="${ant.project.name}.ivy.settings"
       pattern="${build.ivy.lib.dir}/${ivy.artifact.retrieve.pattern}"
-	  log="${ivyresolvelog}"/> 
+	  log="${ivyresolvelog}"/>
   </target>
   <target name="ivy-resolve-test" depends="ivy-init-settings" unless="offline">
     <echo message="Project: ${ant.project.name}"/>
Index: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
===================================================================
--- common/src/java/org/apache/hadoop/hive/conf/HiveConf.java	(revision 116)
+++ common/src/java/org/apache/hadoop/hive/conf/HiveConf.java	(working copy)
@@ -187,7 +187,7 @@
     HIVEMULTIQUERYNUM("hive.multiquerynum", 15),
     // to determine whether to run inc query , the default value is false
     HIVEINCQUERY("hive.incquery", true),
-    HIVEINCTIMES("hive.inctimes", 3),
+    HIVEINCTIMES("hive.inctimes", 1),
     HIVEINCEXTRACTTMP("hive.inc.extracttmp",true),
     HIVEINCFLATTENSUBQS("hive.inc.flattensubqs",true),
 
Index: common/src/gen/org/apache/hive/common/package-info.java
===================================================================
--- common/src/gen/org/apache/hive/common/package-info.java	(revision 116)
+++ common/src/gen/org/apache/hive/common/package-info.java	(working copy)
@@ -1,7 +1,7 @@
 /*
  * Generated by saveVersion.sh
  */
-@HiveVersionAnnotation(version="0.12.0", revision="114", branch="branches/kangyanli/hive-0.12.0/src",
-                         user="kangyanli", date="Fri Jun 19 17:42:08 CST 2015", url="svn://10.3.0.220/hive/branches/kangyanli/hive-0.12.0/src",
-                         srcChecksum="b5805c11765f7b5d0bf82314d744c275")
+@HiveVersionAnnotation(version="0.12.0", revision="116", branch="branches/kangyanli/hive-0.12.0/src",
+                         user="kangyanli", date="Tue Dec 22 20:43:27 CST 2015", url="svn://10.3.0.220/hive/branches/kangyanli/hive-0.12.0/src",
+                         srcChecksum="4c5310f30be5e5fd59170da29f2c4ed6")
 package org.apache.hive.common;
Index: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java	(working copy)
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
+import org.mortbay.log.Log;
 
 /**
  * Processor for the rule - reduce sink followed by reduce sink.
@@ -63,6 +64,12 @@
         .getOpTaskMap();
     Task<? extends Serializable> oldTask = opTaskMap.get(reducer);
 
+    //store result of this RS
+    if(op.getStoreDirName() != null){
+      Log.info("Find join input cache point in taskDAG. DirName is " + op.getStoreDirName());
+      ctx.getStoreTaskTODir().put(currTask, op.getStoreDirName());
+    }
+
     ctx.setCurrAliasId(currAliasId);
     ctx.setCurrTask(currTask);
 
Index: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java	(working copy)
@@ -49,6 +49,7 @@
  * visited so far.
  */
 public class GenMRProcContext implements NodeProcessorCtx {
+  Map<Task<? extends Serializable>,String> storeTaskTODir = new HashMap<Task<? extends Serializable>,String>();
 
   /**
    * GenMapRedCtx is used to keep track of the current state.
@@ -459,4 +460,8 @@
       Map<FileSinkDesc, Task<? extends Serializable>> linkedFileDescTasks) {
     this.linkedFileDescTasks = linkedFileDescTasks;
   }
+
+  public Map<Task<? extends Serializable>,String> getStoreTaskTODir(){
+    return storeTaskTODir;
+  }
 }
Index: ql/src/java/org/apache/hadoop/hive/ql/plan/MoveWork.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/plan/MoveWork.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/plan/MoveWork.java	(working copy)
@@ -37,6 +37,7 @@
   private LoadTableDesc loadTableWork;
   private LoadFileDesc loadFileWork;
   private LoadMultiFilesDesc loadMultiFilesWork;
+  private CopyFileDesc copyFileWork;
 
   private boolean checkFileFormat;
   ArrayList<String> dpSpecPaths; // dynamic partition specified paths -- the root of DP columns
@@ -103,6 +104,15 @@
     this.loadMultiFilesWork = lmfd;
   }
 
+  public CopyFileDesc getCopyFileWork() {
+    return copyFileWork;
+  }
+
+  public void setCopyFileWork(CopyFileDesc cfw) {
+    this.copyFileWork = cfw;
+  }
+
+
   public void setLoadFileWork(final LoadFileDesc loadFileWork) {
     this.loadFileWork = loadFileWork;
   }
Index: ql/src/java/org/apache/hadoop/hive/ql/IncDriver.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/IncDriver.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/IncDriver.java	(working copy)
@@ -56,6 +56,7 @@
 import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;
 import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatter;
 import org.apache.hadoop.hive.ql.parse.ASTNode;
+import org.apache.hadoop.hive.ql.parse.ASTNodeUtils;
 import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;
 import org.apache.hadoop.hive.ql.parse.ExtractTmpSemanticAnalyzer;
 import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHook;
@@ -73,6 +74,7 @@
 import org.apache.hadoop.hive.ql.plan.HiveOperation;
 import org.apache.hadoop.hive.ql.plan.MapWork;
 import org.apache.hadoop.hive.ql.plan.MapredWork;
+import org.apache.hadoop.hive.ql.plan.PartitionDesc;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.processors.CommandProcessor;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory;
@@ -84,7 +86,7 @@
 
 public class IncDriver implements CommandProcessor {
 
-  static final private Log LOG = LogFactory.getLog(Driver.class.getName());
+  static final private Log LOG = LogFactory.getLog(IncDriver.class.getName());
   static final private LogHelper console = new LogHelper(LOG);
 
   private static final Object compileMonitor = new Object();
@@ -97,12 +99,16 @@
   private Context ctx;
   private QueryPlan tmpTablePlan;
   //create a qblist to store the qb of the two Phases
-  private ArrayList<QB> qblist ;
+  //private ArrayList<QB> qblist ;
+  private ArrayList<QB> gbyCacheQBlist = new ArrayList<QB>();
+  private ArrayList<QB> joinCacheQBlist = new ArrayList<QB>();
+  private QB finalQB = null;
   private IncSplitSemanticAnalyzer semcache;
   private SemanticAnalyzer sem;
-  private QueryPlan cacheTablePlan;
+  private ArrayList<QueryPlan> cacheTablePlan = null;
   private QueryPlan FinalPlan;
   private QueryPlan OriginalPlan;
+  private ASTNode originalTree;
   private incCtx  incCtx;
   private Schema schema;
   private HiveLockManager hiveLockMgr;
@@ -122,9 +128,10 @@
   private final AtomicInteger counter = new AtomicInteger(0);
   private volatile boolean run = true;
   private boolean isINC = false;
+  private final HashSet<String> toRemovePath = new HashSet<String>();
 
   //inc table name mapped to Table object of currently inc table
-  private HashMap< String , Table > tabNameToTab;
+  //private HashMap< String , Table > tabNameToTab;
   private static long start = System.currentTimeMillis();
   private static long lastEnd = 0;//last end time is begin time of current inc run
 
@@ -326,13 +333,13 @@
       return parserRes;
     }
 
-    public String getIncTableName(){
+    /*public String getIncTableName(){
       return parserRes.getIncTableName();
     }
 
     public ASTNode getIncTabRef(){
       return (ASTNode) parserRes.getIncToken().getParent();
-    }
+    }*/
 
   }
 
@@ -378,7 +385,8 @@
       //parser inc-query :1,Integrity Check .2,extract parameters
       IncQueryParser incParser = new IncQueryParser(tree);
       perfLogger.PerfLogBegin(LOG, PerfLogger.ANALYZE);
-      if(incParser.getIncToken( ) != null){
+      incParser.analyzeIncAlias();
+      if(incParser.getIncTableAlias().size() != 0){
         incParser.checkIncTokenParams();
         isINC = true;
       }else{
@@ -386,7 +394,7 @@
         genOriginalQueryPlan(tree,command,perfLogger);
         return 0;
       }
-
+      originalTree = ASTNodeUtils.clone(tree);
       //Initial incCtx, store store of inc-query parser
       incCtx = new incCtx();
       incCtx.parserRes = incParser;
@@ -398,13 +406,18 @@
       //    create a qblist to store the qb of the two Phases,use getQBList in semantic analyze class to get it
       //    qblist.get(0) is the IncPhase1's QB ,It's used in genExtractCacheQueryPlan
       //    QB will be used in semantic analyze class's method analyzeQB(QB) to generate the MR tasks
-      qblist = genExtractCacheQueryPlan(tree,command,perfLogger);
+      genExtractCacheQueryPlan(tree,command,perfLogger);
+      //LOG.info("IncDriver qblist.size = " + qblist.size());
       //Final semantic analyze stage , generate the final query plan
       //    IncPhase2:select the final result from cache table;
       //    qblist.get(1) is the IncPhase2's QB ,It's used in genFinalQueryPlan as the parameter.
       //    QB will be used in semantic analyze class's method analyzeQB(QB) to generate the MR tasks
-      genFinalQueryPlan(tree,command,qblist.get(1),perfLogger);
 
+      /*for(int i=0;i<qblist.size()-1;i++){
+        cacheTablePlan.add(genQueryPlan(tree,command,qblist.get(i),perfLogger));
+      }
+      FinalPlan = genQueryPlan(tree,command,qblist.get(qblist.size()-1),perfLogger);*/
+
       return 0;
     } catch (Exception e) {
       ErrorMsg error = ErrorMsg.getErrorMsg(e.getMessage());
@@ -434,6 +447,7 @@
 
   private void genOriginalQueryPlan(ASTNode tree, String command, PerfLogger perfLogger ) throws Exception {
 
+    LOG.info("genOriginalQueryPlan for: " + tree.dump());
     //when TOK_INC is nonexistent, step into normally compile flow
     BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(conf, tree);
     List<HiveSemanticAnalyzerHook> saHooks =
@@ -492,8 +506,9 @@
   }
 
 
-  private void genFinalQueryPlan(ASTNode tree, String command, QB qb, PerfLogger perfLogger) throws Exception {
-    sem =new SemanticAnalyzer(conf);
+  private QueryPlan genQueryPlan(ASTNode tree, String command, QB qb, PerfLogger perfLogger) throws Exception {
+    QueryPlan ret = null;
+    sem =new IncSplitSemanticAnalyzer(conf,tree,incCtx);
     sem.initSem(ctx);
     List<HiveSemanticAnalyzerHook> saHooks =
         getHooks(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK,
@@ -521,16 +536,18 @@
     sem.validate();
     perfLogger.PerfLogEnd(LOG, PerfLogger.ANALYZE);
 
-    FinalPlan = new QueryPlan(command, sem, perfLogger.getStartTime(PerfLogger.DRIVER_RUN));
+    ret = new QueryPlan(command, sem, perfLogger.getStartTime(PerfLogger.DRIVER_RUN));
 
     // initialize FetchTask right here
-    if (FinalPlan.getFetchTask() != null) {
-      FinalPlan.getFetchTask().initialize(conf, FinalPlan, null);
+    if (ret.getFetchTask() != null) {
+      ret.getFetchTask().initialize(conf, FinalPlan, null);
     }
 
     // get the output schema
     schema = getSchema(sem, conf);
 
+    return ret;
+
   }
   /**
    * Generates IncPhase1's Query Plan
@@ -547,7 +564,7 @@
    * @throws SemanticException IncSplitSemanticAnalyzer Compile
    */
 
-  private ArrayList<QB> genExtractCacheQueryPlan(ASTNode tree, String command, PerfLogger perfLogger) throws Exception {
+  private void genExtractCacheQueryPlan(ASTNode tree, String command, PerfLogger perfLogger) throws Exception {
 
     //SemanticAnalyzer to do incremental optimization
     //split the original query into two queries ,one for insert cache table ,one for get the final result
@@ -567,22 +584,30 @@
       semcache.genAndrunCrtCacheQuery();
       //Spiting origin query to generate insert ql and query ql,then generate the MR tasks
       semcache.doIncSplitCompile();
-      semcache.genLogicalPlan();
+      /*semcache.genLogicalPlan();
       semcache.mapReudceCompiler();
       hookCtx.update(semcache);
       for (HiveSemanticAnalyzerHook hook : saHooks) {
         hook.postAnalyze(hookCtx, semcache.getRootTasks());
-      }
+      }*/
     } else {
       semcache.genAndrunCrtCacheQuery();
       semcache.doIncSplitCompile();
-      semcache.genLogicalPlan();
-      semcache.mapReudceCompiler();
+
+      /*semcache.genLogicalPlan();
+      semcache.mapReudceCompiler();*/
     }
+    this.gbyCacheQBlist = semcache.getGbyCacheQBList();
+    this.joinCacheQBlist = semcache.getJoinCacheQBList();
+    this.finalQB = semcache.getFinalQB();
 
-    tabNameToTab = semcache.getTabNameToTab();
+    incCtx.getParserRes().setIncAliasToTable(semcache.getDb());
+    incCtx.getParserRes().setScanAllIncAliases(semcache.getScanAllIncAliases());
+
+    //tabNameToTab = semcache.getTabNameToTab();
     LOG.info("Semantic Analysis Completed");
-    // validate the plan
+
+    /*// validate the plan
     semcache.validate();
     perfLogger.PerfLogEnd(LOG, PerfLogger.ANALYZE);
 
@@ -596,19 +621,20 @@
     //qblist.get(1) is the IncPhase2's QB ,It's used in genFinalQueryPlan as the parameter.
     ArrayList<QB> qblist = semcache.getQBList();
 
-    if(qblist.size() != 2){
-      throw new SemanticException("ERROR:In IncSplitSemanticAnalyzer generating qblist get error! ");
-    }
 
+
     //insert into cache table ,schema is useless
-    //schema = getSchema(semcache, conf);
-    return qblist;
+    //schema = getSchema(semcache, conf);*/
+    /*if(qblist.size() < 2){
+      throw new SemanticException("ERROR:In IncSplitSemanticAnalyzer generating qblist get error! ");
+    }*/
+    //return qblist;
   }
 
 
-  public QueryPlan getCacheTablePlan() {
+  /*public QueryPlan getCacheTablePlan() {
     return cacheTablePlan;
-  }
+  }*/
   public QueryPlan getFinalPlan() {
     return FinalPlan;
   }
@@ -1246,9 +1272,18 @@
 
   public int close( ) {
     try {
-      QueryPlan[] plans = {tmpTablePlan , cacheTablePlan,FinalPlan,OriginalPlan };
-      for( int i = 0 ; i < plans.length ; i++){
-        QueryPlan plan = plans[i];
+      ArrayList<QueryPlan> plans = new ArrayList<QueryPlan>();//tmpTablePlan , cacheTablePlan.toArray(),FinalPlan,OriginalPlan };
+      plans.add(tmpTablePlan);
+      plans.add(FinalPlan);
+      plans.add(OriginalPlan);
+      if(cacheTablePlan != null){
+        for(QueryPlan plan:cacheTablePlan){
+          plans.add(plan);
+        }
+      }
+
+      for( int i = 0 ; i < plans.size() ; i++){
+        QueryPlan plan = plans.get(i);
         if (plan != null) {
           FetchTask fetchTask = plan.getFetchTask();
           if (null != fetchTask) {
@@ -1300,14 +1335,17 @@
    * @param mapTasks mapwork
    */
   private  void incMapTasks(List<Task<? extends Serializable>> tasks, List<MapWork> mapTasks) {
-    String alias_id = incCtx.parserRes.getIncTableAlias();
+    ArrayList<String> aliasSet = incCtx.parserRes.getIncTableAlias();
     for (Task<? extends Serializable> task : tasks) {
        Serializable work = task.getWork();
       if (work instanceof MapredWork && !mapTasks.contains(((MapredWork) work).getMapWork())) {
         MapWork mapTask = ((MapredWork) work).getMapWork();
-        if(mapTask.getAliases().contains(alias_id)){
-          mapTasks.add(mapTask);
+        for(String alias_id:aliasSet){
+          if(mapTask.getAliases().contains(alias_id)){
+            mapTasks.add(mapTask);
+          }
         }
+
       }
       if (task.getDependentTasks() != null) {
         incMapTasks(task.getDependentTasks(), mapTasks);
@@ -1343,55 +1381,98 @@
     return tab.getIncFilePaths( startDate , endDate);
   }
 
-  private void changeInputPath(long curStart ){
-    ArrayList<Task<? extends Serializable>> tasks = cacheTablePlan.getRootTasks();
+  private void changeInputPath(long curStart,QueryPlan curPlan ){
+    ArrayList<Task<? extends Serializable>> tasks = curPlan.getRootTasks();
     List<MapWork> mapTasks = new  ArrayList<MapWork>();
     incMapTasks( tasks , mapTasks);
+    ArrayList<String> scanNewFileAlias = incCtx.parserRes.getScanNewFileIncAliases();
+    int i=0;
     for(MapWork mt:  mapTasks){
+      LOG.info("Change input path for " + i++ + " map task");
        LinkedHashMap<String, ArrayList<String>> pathToAlias = mt.getPathToAliases();
-       for( Table tab : tabNameToTab.values()){
-         ArrayList<Path> paths = incFiles(tab,curStart);
-         if(paths.isEmpty()){
-           LOG.info("Do not find incremental data file in HDFS!");
-         }
-         String incAlias = incCtx.parserRes.getIncTableAlias();
-         ArrayList<String> newPaths  = new ArrayList<String>();
-         ArrayList<String> aliasSet  = new ArrayList<String>();
-         ArrayList<String> deletePath  = new ArrayList<String>();
-         aliasSet.add(incAlias);
-         for(Path path : paths ){
-           newPaths.add(path.toString());
-         }
-     //  if alias existed , remove map of data file path to table alias
-         for(String path: pathToAlias.keySet()){
-           LOG.info(" Old input path : " +path + " alias :: "+ pathToAlias.get(path));
-           ArrayList<String> alias = pathToAlias.get(path);
-           if( alias.contains(incAlias)){
-             alias.remove(incAlias);
+       LinkedHashMap<String, PartitionDesc> pathToPartitionInfo = mt.getPathToPartitionInfo();
+       for(String path: pathToAlias.keySet()){
+         LOG.info("Before change, input path : " +path + " alias :: "+ pathToAlias.get(path));
+       }
+
+       Iterator<Map.Entry<String,ArrayList<String>>> it = pathToAlias.entrySet().iterator();
+       ArrayList<String> removedAlias = new ArrayList<String>();
+       while(it.hasNext()){
+         Map.Entry<String,ArrayList<String>> cur = it.next();
+         String path = cur.getKey();
+         ArrayList<String> aliases = cur.getValue();
+         Iterator<String> itlist = aliases.iterator();
+         while(itlist.hasNext()){
+           String curAlias = itlist.next();
+           if(scanNewFileAlias.contains(curAlias)){
+             removedAlias.add(curAlias);
+             itlist.remove();
            }
-           if(alias.isEmpty()){
-             deletePath.add(path);
-           }
          }
-         //delete map of redundancy within pathToAlias
-         for( String delPath : deletePath){
-           pathToAlias.remove( delPath);
+         if(aliases.isEmpty()){
+           it.remove();
+           pathToPartitionInfo.remove(path);
          }
-         //add new path
-         for( String path : newPaths){
-           LOG.info("Add input path : " +path);
-           if( pathToAlias.containsKey(path)){
-             pathToAlias.get(path).addAll(aliasSet);
-           }else{
-             pathToAlias.put(path, aliasSet);
+       }
+       for(String incAlias:removedAlias){
+         Table tab = incCtx.getParserRes().getTableForAlias(incAlias);
+         ArrayList<Path> newPaths = incFiles(tab,curStart);
+         for( Path newPath : newPaths){
+           String newPathName = newPath.toString();
+           if( ! pathToAlias.containsKey(newPathName)){
+             pathToAlias.put(newPathName, new ArrayList<String>());
            }
-           mt.getPathToPartitionInfo().put(path, mt.getAliasToPartnInfo().get(incAlias));
+           pathToAlias.get(newPathName).add(incAlias);
+           pathToPartitionInfo.put(newPathName, mt.getAliasToPartnInfo().get(incAlias));
          }
+       }
 
-         for(String path: pathToAlias.keySet()){
-           LOG.info(" New input path : " +path + " alias :: "+ pathToAlias.get(path));
-         }
+       for(String path: pathToAlias.keySet()){
+         LOG.info("After change, input path : " +path + " alias :: "+ pathToAlias.get(path));
        }
+//       for( String curAlias : scanNewFileAlias){
+//         LOG.info("Begin find new added file for alias " + curAlias + " after time " + curStart);
+//         Table tab = incCtx.getParserRes().getTableForAlias(curAlias);
+//         LOG.info("Begin Find new added file in table " + tab.getTableName() + " after time " + curStart);
+//         ArrayList<Path> paths = incFiles(tab,curStart);
+//         if(paths.isEmpty()){
+//           LOG.info("Do not find incremental data file in HDFS!");
+//         }
+//
+//         ArrayList<String> newPaths  = new ArrayList<String>();
+//         for(Path path : paths ){
+//           newPaths.add(path.toString());
+//         }
+//
+//         ArrayList<String> deletePath  = new ArrayList<String>();
+//         for(String path: pathToAlias.keySet()){
+//           ArrayList<String> pathAlias = pathToAlias.get(path);
+//           if(pathAlias.contains(curAlias)){
+//             pathAlias.remove(curAlias);
+//           }
+//           if(pathAlias.isEmpty()){
+//             deletePath.add(path);
+//           }
+//         }
+//
+//         for( String delPath : deletePath){
+//           pathToAlias.remove( delPath);
+//         }
+//         //add new path
+//         for( String path : newPaths){
+//           LOG.info("Added input path : " +path);
+//           if( ! pathToAlias.containsKey(path)){
+//             pathToAlias.put(path, new ArrayList<String>());
+//           }
+//           pathToAlias.get(path).add(curAlias);
+//           mt.getPathToPartitionInfo().put(path, mt.getAliasToPartnInfo().get(curAlias));
+//         }
+//
+//         for(String path: pathToAlias.keySet()){
+//           LOG.info("After change, input path : " +path + " alias :: "+ pathToAlias.get(path));
+//         }
+//
+//       }
     }
   }
 
@@ -1404,23 +1485,11 @@
     }
 
     if(!isINC ){
-      ret = execute(conf ,  OriginalPlan);
-      try {
-        int resultcounter = outputRes(OriginalPlan) ;
-        long end = System.currentTimeMillis();
-        double timeTaken = (end  - start) / 1000.0;
-        console.printInfo("Time taken: " + timeTaken + " seconds" +
-            (resultcounter == 0 ? "" : ", Fetched: " + resultcounter + " row(s)"));
-      } catch (IOException e) {
-        console.printError("Failed with exception " + e.getClass().getName() + ":"
-            + e.getMessage(), "\n"
-            + org.apache.hadoop.util.StringUtils.stringifyException(e));
-      }
-      return ret;
+      return runOriginalPlan();
     }
 
     long period = 1000;
-    if( incCtx.parserRes.getIncToken() != null){
+    if( incCtx.parserRes.getIncTableAlias().size() != 0){
       period = incCtx.parserRes.getIntervalSec()*1000;
     }
     timer.schedule(new IncrementalExec(command),
@@ -1431,9 +1500,28 @@
         return ret;
       }
     }
+
   }
 
+  private int runOriginalPlan() throws CommandNeedRetryException{
 
+    start = System.currentTimeMillis();
+    int ret = execute(conf ,  OriginalPlan);
+    try {
+      int resultcounter = outputRes(OriginalPlan) ;
+      long end = System.currentTimeMillis();
+      double timeTaken = (end  - start) / 1000.0;
+      console.printInfo("Time taken: " + timeTaken + " seconds" +
+          (resultcounter == 0 ? "" : ", Fetched: " + resultcounter + " row(s)"));
+    } catch (IOException e) {
+      console.printError("Failed with exception " + e.getClass().getName() + ":"
+          + e.getMessage(), "\n"
+          + org.apache.hadoop.util.StringUtils.stringifyException(e));
+    }
+    return ret;
+
+  }
+
   private int  outputRes(QueryPlan queryplan) throws IOException, CommandNeedRetryException{
 
     SessionState ss = SessionState.get();
@@ -1469,45 +1557,75 @@
         SessionState.start(ss);
       /*  resetMRTasks(cacheTablePlan.getRootTasks());
         resetMRTasks(FinalPlan.getRootTasks());*/
+        if(counter.get() >= 1){
+          for(String tabName: incCtx.getParserRes().getIncTabNames()){
 
-        if(counter.get() >= 1){
+            try {
+              String addedFile = "/home/kangyanli/tpch/100G/"+tabName + "/"+tabName + "Inc"+counter.get()+".tbl";
+              toRemovePath.add(addedFile);
+              String putTableCommand = "hdfs dfs -put /home/kangyanli/tpch_2_15.0/dbgen/100G/"+tabName + ".tbl " +
+              addedFile;
+              Process process = Runtime.getRuntime().exec(putTableCommand);
+              LOG.info(putTableCommand);
+              int exitValue = process.waitFor();
+              if (0 != exitValue) {
+                LOG.error("call shell failed. error code is :" + exitValue);
+              }
+            } catch (Throwable e) {
+              LOG.error("call shell failed. " + e);
+            }
+          }
+
+        }
+        cacheTablePlan = new ArrayList<QueryPlan>();
+        //if(counter.get() >= 1){
           start = System.currentTimeMillis();
-          semcache.reset();
+          LOG.info("=====Begin "+ counter.get() + " incremental run, current time is: "+start + "=====");
+          //LOG.info("qblist.size is " + qblist.size());
+          /*semcache.reset();
           semcache.genLogicalPlan();
           semcache.mapReudceCompiler();
           semcache.validate();
           cacheTablePlan = new QueryPlan(command, semcache, 0L);
           PerfLogger perfLogger = PerfLogger.getPerfLogger();
           perfLogger.PerfLogBegin(LOG, PerfLogger.ANALYZE);
-          genFinalQueryPlan( null,command,qblist.get(1),perfLogger);
-        }
+          genFinalQueryPlan( null,command,qblist.get(1),perfLogger);*/
+          PerfLogger perfLogger = PerfLogger.getPerfLogger();
+          perfLogger.PerfLogBegin(LOG, PerfLogger.ANALYZE);
+          for(int i=0;i<gbyCacheQBlist.size();i++){
+            LOG.info("=====Begin generate " + i + " cacheTablePlan=====");
+            cacheTablePlan.add(genQueryPlan(null,command,gbyCacheQBlist.get(i),perfLogger));
+          }
+          LOG.info("=====Begin generate FinalPlan=====");
+          FinalPlan = genQueryPlan(null,command,finalQB,perfLogger);
+          toRemovePath.addAll(ctx.getJoinInputCache());
+        //}
 
         // initialize FetchTask right here
-        if (cacheTablePlan.getFetchTask() != null) {
-          cacheTablePlan.getFetchTask().initialize(conf, cacheTablePlan, null);
+        for(int i=0;i<cacheTablePlan.size();i++){
+          QueryPlan curCachePlan = cacheTablePlan.get(i);
+          if (curCachePlan.getFetchTask() != null) {
+            curCachePlan.getFetchTask().initialize(conf, curCachePlan, null);
+          }
         }
 
+
         if (FinalPlan.getFetchTask() != null) {
           FinalPlan.getFetchTask().initialize(conf, FinalPlan, null);
         }
-        if(counter.get() >= 1){
-            LOG.info("change inputpath, lastEnd time is:" + lastEnd);
-            try {
-              Process process = Runtime.getRuntime().exec("hdfs dfs -put /home/kangyanli/tpch_2_15.0/dbgen/lineitem.tbl " +
-                  "/home/kangyanli/tpch/lineitem/lineitemInc"+counter.get()+".tbl");
-              LOG.info("hdfs dfs -put /home/kangyanli/tpch_2_15.0/dbgen/lineitem.tbl " +
-                  "/home/kangyanli/tpch/lineitem/lineitemInc"+counter.get()+".tbl");
-              int exitValue = process.waitFor();
-              if (0 != exitValue) {
-                LOG.error("call shell failed. error code is :" + exitValue);
-              }
-            } catch (Throwable e) {
-              LOG.error("call shell failed. " + e);
-            }
+
+        LOG.info("Incremental Aliases are: " + incCtx.getParserRes().getIncTableAlias().toString());
+        LOG.info("Scan All incremental Aliases are: " + incCtx.getParserRes().getScanAllIncAliases().toString());
+        LOG.info("Scan new file incremental Aliases are: " + incCtx.getParserRes().getScanNewFileIncAliases().toString());
+        start = System.currentTimeMillis();
+        for(int i=0;i<cacheTablePlan.size();i++){
+          QueryPlan curCachePlan = cacheTablePlan.get(i);
+          LOG.info("==========Begin execute " + i + " cacheTablePlan==========");
+          changeInputPath(lastEnd,curCachePlan);
+          incDrv.execute(conf,curCachePlan);
         }
-        changeInputPath(lastEnd);
-
-        incDrv.execute(conf,cacheTablePlan);
+        LOG.info("==========Begin execute FinalPlan==========");
+        changeInputPath(lastEnd,FinalPlan);
         incDrv.execute(conf,FinalPlan);
 
         int resultcounter = outputRes(FinalPlan) ;
@@ -1519,6 +1637,12 @@
         }
         console.printInfo("Time taken: " + timeTaken + " seconds" +
             (resultcounter == 0 ? "" : ", Fetched: " + resultcounter + " row(s)"));
+
+        LOG.info("=====Begin test origianl plan======");
+        genOriginalQueryPlan(originalTree,command,perfLogger);
+        runOriginalPlan();
+
+
       }catch ( Exception e) {
         console.printError("Failed with exception " + e.getClass().getName() + ":"
             + e.getMessage(), "\n"
@@ -1528,18 +1652,25 @@
       // if open debug mode , run just once. or default execution 3 times
       int incTimes = conf.getIntVar(HiveConf.ConfVars.HIVEINCTIMES);
       if( LOG.isDebugEnabled() || counter.get() >= incTimes ){
-        timer.cancel();
-        run = false;
-        try {
-          Process process = Runtime.getRuntime().exec("hdfs dfs -rm /home/kangyanli/tpch/lineitem/lineitemInc*");
-          LOG.info("hdfs dfs -rm /home/kangyanli/tpch/lineitem/lineitemInc*");
-          int exitValue = process.waitFor();
-          if (0 != exitValue) {
-              LOG.error("call shell failed. error code is :" + exitValue);
+
+        LOG.info("toRemovedPaths are: " + toRemovePath.toString());
+        for(String path:toRemovePath){
+          try {
+            String deleteTableCommand = "hdfs dfs -rm -r "+path;
+            Process process = Runtime.getRuntime().exec(deleteTableCommand);
+            LOG.info(deleteTableCommand);
+            int exitValue = process.waitFor();
+            if (0 != exitValue) {
+                LOG.error("call shell failed. error code is :" + exitValue);
+            }
+          } catch (Throwable e) {
+             LOG.error("call shell failed. " + e);
           }
-        } catch (Throwable e) {
-           LOG.error("call shell failed. " + e);
+
         }
+        run = false;
+        lastEnd = 0;
+        timer.cancel();
 
 
       }
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/QBJoinTree.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/QBJoinTree.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/QBJoinTree.java	(working copy)
@@ -76,6 +76,8 @@
   private List<String> streamAliases;
 
   private List<String> leftSrc;
+  private Map<Integer,String> storeChild = new HashMap<Integer,String>();
+
   /**
    * constructor.
    */
@@ -356,4 +358,11 @@
     return this.leftSrc;
   }
 
+  public void setStoreChild(Map<Integer, String> input) {
+    this.storeChild = input;
+  }
+
+  public Map<Integer, String> getStoreChild() {
+    return storeChild;
+  }
 }
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/IncQueryParser.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/IncQueryParser.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/IncQueryParser.java	(working copy)
@@ -1,18 +1,21 @@
 package org.apache.hadoop.hive.ql.parse;
 
 import java.text.SimpleDateFormat;
+import java.util.ArrayList;
 import java.util.Calendar;
 import java.util.Date;
 import java.util.HashMap;
 import java.util.HashSet;
-import java.util.LinkedList;
-import java.util.Queue;
+import java.util.Map;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
-import org.antlr.runtime.tree.Tree;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.ql.ErrorMsg;
+import org.apache.hadoop.hive.ql.metadata.Hive;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.metadata.Table;
 
 public class IncQueryParser {
   private static final Log LOG = LogFactory.getLog("hive.ql.parse.IncQueryParser");
@@ -23,13 +26,28 @@
   private String constantUnit;
   private int intervalTime;
   private String intervalUnit;
-  private String incTableName;
-  private String incTableAlias;
+  //private String incTableName;
+  //private String incTableAlias;
   private final ASTNode  originalTree;
-  private ASTNode  incToken;
+  //private ASTNode  incToken;
   private boolean isConstantParam = false;
   private boolean isAfterParam = true;
 
+  /**
+   * TOK_TABREF
+   *  TOK_TABLENAME(tablename)
+   *  TOK_INCRE
+   *  aliasname
+   */
+  private final ArrayList<String> incAliases = new ArrayList<String>();
+  private final HashMap<String,String> incAliasToTabName = new HashMap<String,String>();
+  private final HashMap<String,Table> incAliasToTable = new HashMap<String,Table>();
+  private final ArrayList<ASTNode> incTabRef = new ArrayList<ASTNode>();
+  private ArrayList<String> scanAllIncAliases = new ArrayList<String>();
+  private ArrayList<String> scanNewFileIncAliases = null;
+  private final ArrayList<String> aliasNoOuter = new ArrayList<String>();
+  private final HashSet<String> incTabName = new HashSet<String>();
+
   private static HashMap<String , Integer> dateToId = new HashMap<String , Integer>();
 
   static{
@@ -105,9 +123,10 @@
 
   public  boolean checkIncTokenParams( ) throws SemanticException{
     int constant = 0 ,interval = INTERVAL_TIMES;
-    if(incToken == null){
+    if(incTabRef .size() == 0){
       return false;
     }
+    ASTNode incToken = (ASTNode) incTabRef.get(0).getChild(1);
     int childCount = incToken.getChildCount();
     if(childCount < 1){
       throw new SemanticException("Inc-query parameters less than 1 ");
@@ -227,50 +246,48 @@
     return sec;
   }
 
-
-  public String getIncTableName( ){
-    if( incTableName != null ){
-      return incTableName;
-    }
-    Tree pToken = incToken.getParent();
-    for(int i = 0 ; i < pToken.getChildCount() ; i++){
-      Tree child = pToken.getChild(i);
-      if( child.getType() == HiveParser.TOK_TABNAME ){
-        incTableName = child.getChild(0).getText();
-      }
-    }
-    return incTableName;
+  public void analyzeIncAlias() throws SemanticException{
+    analyzeIncAlias(originalTree , null);
+    LOG.info("Find inc Aliases: " + incAliases.toString());
   }
 
+  private void analyzeIncAlias(ASTNode ast, String outerAlias) throws SemanticException {
+    if (ast.getToken() != null) {
+      switch(ast.getToken().getType()){
+      case HiveParser.TOK_SUBQUERY:
+        if (ast.getChildCount() != 2) {
+          throw new SemanticException(ErrorMsg.NO_SUBQUERY_ALIAS.getMsg(ast));
+        }
+        String subQalias = BaseSemanticAnalyzer.unescapeIdentifier(ast.getChild(1).getText());
+        outerAlias = QB.getAppendedAliasFromId(outerAlias,subQalias);
+        analyzeIncAlias((ASTNode) ast.getChild(0),outerAlias);
+        break;
+      case HiveParser.TOK_TABREF:
 
-  public ASTNode getIncToken(  ){
-    if( incToken != null){
-      return incToken;
-    }
-    HashSet<ASTNode> visited = new  HashSet<ASTNode>();
-    traverseTree(originalTree , visited);
-    for( ASTNode v : visited){
-      if( v.getToken().getType() == HiveParser.TOK_INCRE &&
-          v.getParent().getType() ==  HiveParser.TOK_TABREF){
-         incToken = v;;
+        if(ASTNodeUtils.isIncTabRef(ast)){
+          String tabalias = ASTNodeUtils.getAliasId(ast);
+          String tabName = ASTNodeUtils.getTabeName(ast);
+          String longAlias = QB.getAppendedAliasFromId(outerAlias,tabalias);
+          incAliases.add(longAlias);
+          incAliasToTabName.put(longAlias, tabName);
+          incTabRef.add(ast);
+          aliasNoOuter.add(tabalias);
+          incTabName.add(tabName);
+        }
+        break;
+      case HiveParser.TOK_INSERT:
+      case HiveParser.TOK_INSERT_INTO:
+        break;
+      default:
+        for(int i=0;i<ast.getChildCount();i++){
+          analyzeIncAlias((ASTNode) ast.getChild(i),outerAlias);
+        }
       }
     }
-    return incToken;
   }
 
-
-  private void traverseTree( ASTNode ast , HashSet<ASTNode> visited){
-    Queue<ASTNode> poll = new LinkedList<ASTNode>();
-    poll.add(ast);
-    visited.add(ast);
-    while( poll.peek() != null ){
-        ASTNode node = poll.remove();
-        for(int i = 0 ; i  < node.getChildCount() ; i++){;
-          if( !visited.contains((ASTNode)node.getChild(i)  ) ){
-            traverseTree((ASTNode)node.getChild(i) , visited);
-          }
-        }
-    }
+  public ArrayList<ASTNode> getIncTabRef(){
+    return incTabRef;
   }
 
   // convert date object to String , eg :2014-03-04 11:05:00
@@ -279,31 +296,23 @@
     return format.format(date);
   }
 
-  public String getIncTableAlias( ){
-    if( incTableAlias != null ){
-      return incTableAlias;
-    }
+  public ArrayList<String> getIncTableAlias( ){
+    return incAliases;
+  }
 
-    Tree pToken = incToken.getParent();
-    int aliasIndex = 0;
-    for (int index = 1; index < pToken.getChildCount(); index++) {
-      ASTNode ct = (ASTNode) pToken.getChild(index);
-      if (ct.getToken().getType() != HiveParser.TOK_TABLEBUCKETSAMPLE &&
-          ct.getToken().getType() != HiveParser.TOK_TABLESPLITSAMPLE &&
-          ct.getToken().getType() != HiveParser.TOK_TABLEPROPERTIES &&
-          ct.getToken().getType() != HiveParser.TOK_INCRE ){
-        aliasIndex = index;
-      }
+  public void setIncAliasToTable(Hive db) throws HiveException{
+    for(Map.Entry<String,String> cur :incAliasToTabName.entrySet()){
+      String alias = cur.getKey();
+      String tableName = cur.getValue();
+      Table table = db.getTable(tableName);
+      incAliasToTable.put(alias, table);
     }
-    if (aliasIndex != 0) {
-      incTableAlias = BaseSemanticAnalyzer.unescapeIdentifier(pToken.getChild(aliasIndex).getText());
-    }
-    else {
-      incTableAlias = BaseSemanticAnalyzer.getUnescapedUnqualifiedTableName((ASTNode) (pToken.getChild(0)));
-    }
-   return incTableAlias;
   }
 
+  public Table getTableForAlias(String alias){
+    return incAliasToTable.get(alias);
+  }
+
   private boolean dateUnitCheck(String date , int type ){
     String pattern = "^(year|month|day|hour|minute|second|week)s?$";
     Pattern r = Pattern.compile(pattern , Pattern.CASE_INSENSITIVE);
@@ -331,4 +340,32 @@
     return isAfterParam;
   }
 
+  public void setScanAllIncAliases(ArrayList<String> scanAllInc) {
+    this.scanAllIncAliases = scanAllInc;
+  }
+
+  public ArrayList<String> getScanAllIncAliases(){
+    return scanAllIncAliases;
+  }
+
+  public ArrayList<String> getScanNewFileIncAliases(){
+    if(scanNewFileIncAliases!=null){
+      return scanNewFileIncAliases;
+    }
+    scanNewFileIncAliases = new ArrayList<String>();
+    for(String alias:incAliases){
+      if(!scanAllIncAliases.contains(alias)){
+        scanNewFileIncAliases.add(alias);
+      }
+    }
+    return scanNewFileIncAliases;
+  }
+
+  public ArrayList<String> getAliasNoOuter() {
+   return aliasNoOuter;
+  }
+
+  public HashSet<String> getIncTabNames(){
+    return incTabName;
+  }
 }
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNodeUtils.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNodeUtils.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNodeUtils.java	(working copy)
@@ -7,6 +7,7 @@
 import org.antlr.runtime.CommonToken;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.ql.lib.Node;
 import org.apache.hadoop.hive.ql.parse.IncSplitSemanticAnalyzer.CacheColumn;
 import org.apache.hadoop.hive.ql.parse.IncSplitSemanticAnalyzer.ExprFunction;
 
@@ -409,7 +410,16 @@
 
   public static ASTNode clone(ASTNode ast){
     //ToDO
-    return ast;
+    ASTNode newAST = new ASTNode(ast);
+    if(ast.getChildren() != null){
+      for(Node child: ast.getChildren()){
+        ASTNode newChild = clone((ASTNode)child);
+        newAST.addChild(newChild);
+        newChild.setParent(newAST);
+      }
+    }
+
+    return newAST;
   }
 
 
@@ -493,4 +503,42 @@
       }
     }
   }
+  public static boolean isIncTabRef(ASTNode tabref){
+    boolean isInc = false;
+    for (int index = 1; index < tabref.getChildCount(); index++) {
+      ASTNode ct = (ASTNode) tabref.getChild(index);
+      if(ct.getToken().getType() == HiveParser.TOK_INCRE){
+        isInc = true;
+        break;
+      }
+    }
+    return isInc;
+  }
+
+  public static String getAliasId(ASTNode tabref){
+    int aliasIndex = 0;
+    String tabalias = null;
+    for (int index = 1; index < tabref.getChildCount(); index++) {
+      ASTNode ct = (ASTNode) tabref.getChild(index);
+      if (ct.getToken().getType() != HiveParser.TOK_TABLEBUCKETSAMPLE &&
+          ct.getToken().getType() != HiveParser.TOK_TABLESPLITSAMPLE &&
+          ct.getToken().getType() != HiveParser.TOK_TABLEPROPERTIES &&
+          ct.getToken().getType() != HiveParser.TOK_INCRE ){
+        aliasIndex = index;
+      }
+    }
+    String tableName = BaseSemanticAnalyzer.getUnescapedUnqualifiedTableName((ASTNode) (tabref.getChild(0)));
+    if (aliasIndex != 0) {
+      tabalias = BaseSemanticAnalyzer.unescapeIdentifier(tabref.getChild(aliasIndex).getText());
+    }
+    else {
+      tabalias = tableName;
+    }
+    return tabalias;
+  }
+
+  public static String getTabeName(ASTNode tabref) {
+    return BaseSemanticAnalyzer.getUnescapedUnqualifiedTableName((ASTNode) (tabref.getChild(0)));
+  }
+
 }
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/ExtractTmpSemanticAnalyzer.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/ExtractTmpSemanticAnalyzer.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/ExtractTmpSemanticAnalyzer.java	(working copy)
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.parse;
 
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.Calendar;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -86,8 +87,8 @@
   private incCtx incCtx = null;
   private final List<ASTNode> addedSubQuery = new ArrayList<ASTNode>();
   private boolean isCreateTmpPhase = false;
-  private String incTable = null;
-  private ASTNode  incToken = null;
+  private ArrayList<String> incTable = null;
+  private ArrayList<ASTNode>  incToken = null;
   private boolean isInsertSubQuery = false;
   private final HashMap<String, String> aliasToTabs = new HashMap<String,String>();
   private final HashMap<String,ASTNode> subQueryaliasToSel = new HashMap<String,ASTNode>();
@@ -119,8 +120,8 @@
      super(conf);
      this.ast = tree;
      incCtx = ctx;
-     incTable = incCtx.getIncTableName();
-     incToken = incCtx.getIncTabRef();
+     incTable = incCtx.getParserRes().getAliasNoOuter();
+     incToken = incCtx.getParserRes().getIncTabRef();
   }
 
   @Override
@@ -252,7 +253,7 @@
     //2.1 joinReorder to move the incTable outest
     //2.2 extract the pos of tmp table
     HashMap<ASTNode,Boolean> includeIncToken = new HashMap<ASTNode,Boolean>();//List of TOK_TAB,TOK_SUBQ which include incTable
-    extractPosOfTmp(child,child,includeIncToken);
+    joinReordeAndRecordPosOfTmp(child,child,includeIncToken);
     if(isInsertSubQuery){
       LOG.info("After joinReorder, whole ast is :" + child.dump());
     }else{
@@ -1024,7 +1025,7 @@
    * @throws SerDeException
    * @throws HiveException
    */
-  private void extractPosOfTmp(ASTNode curr,ASTNode root, HashMap<ASTNode,Boolean> includeIncToken) throws HiveException, SerDeException{
+  private void joinReordeAndRecordPosOfTmp(ASTNode curr,ASTNode root, HashMap<ASTNode,Boolean> includeIncToken) throws HiveException, SerDeException{
 
     if(curr.getToken().getType() == HiveParser.TOK_FROM){
       ASTNode frm = (ASTNode) curr.getChild(0);
@@ -1038,14 +1039,12 @@
           return;
         }
 
-        int pos = getIncPos(ASTJoins,includeIncToken);
-        if(pos == -1){
+        List<Integer> incPoses = getIncPos(ASTJoins,includeIncToken);
+        if(incPoses.size() == 0){
           LOG.info("No inc Table in this from clause!");
 
         }else{
-          List<Integer> incPoses = new ArrayList<Integer>();
-          incPoses.add(pos);
-
+          LOG.info("Before joinReorder, Inc joins pos are: " + incPoses.toString());
           QBJoinTree joinTree = genJoinTreeInc(frm);
           // make array with QBJoinTree : outer most(0) --> inner most(n)
           List<QBJoinTree> QBJoinTrees = new ArrayList<QBJoinTree>();
@@ -1054,21 +1053,24 @@
           }
 
           joinReorderAST(QBJoinTrees, ASTJoins,incPoses);
-
+          LOG.info("After joinReorder, Inc Joins pos are: " + incPoses.toString());
           //let incJoin's rightChild as tmpTable, leftChild include incTable;
           for(int i=0 ;i< incPoses.size();i++){
             int currIncPos = incPoses.get(i);
             ASTNode incJoin = ASTJoins.get(currIncPos);
 
             if(isJoinToken( (ASTNode) incJoin.getChild(0))){
-
+              LOG.info("Exchange left and right child of join " + currIncPos);
               ASTNode tmp = (ASTNode) incJoin.getChild(0);
               incJoin.setChild(0,incJoin.getChild(1));
               incJoin.setChild(1,tmp);
               ASTNodeUtils.changeJoinType(incJoin);
 
               String[] subQueryAliases = QBJoinTrees.get(currIncPos).getLeftAliases();
-              String subQName =addSubQuery(ASTJoins,subQueryAliases,currIncPos);
+              String subQName =addSubQuery(ASTJoins.get(currIncPos),subQueryAliases);
+
+              LOG.info("Add subQuery " + subQName + " to new right child: " + Arrays.asList(subQueryAliases).toString());
+
               subQNameToAlias.put(subQName, subQueryAliases);
               /*
                *  analyzeAlias dont't handle join here because only need change join conditions out of incPos,
@@ -1076,7 +1078,7 @@
                */
               incJoin = ASTJoins.get(currIncPos);
 
-              LOG.info("Begin analyze alias after surroud add subQ " + subQName);
+              //LOG.info("Begin analyze alias after surroud add subQ " + subQName);
               analyzeAliasBottomUp((ASTNode) root.getParent(),incJoin,subQName,subQueryAliases,false);
 
               subQNameToAST.put(subQName,(ASTNode) incJoin.getChild(1));
@@ -1106,11 +1108,184 @@
       }else{
         nextRoot = root;
       }
-      extractPosOfTmp(next,nextRoot,includeIncToken);
+      joinReordeAndRecordPosOfTmp(next,nextRoot,includeIncToken);
     }
 
   }
+  /**
+   * 1. add TOK_SUBQUERY to right child of incJoin in ASTtrees
+   * 2. change outer's aliases in subQueryAliases to subQuery's name
+   * @param currIncPos
+   */
+ private String addSubQuery(ASTNode incParent, String[] subQueryAliases){
 
+    /*  String name = "subQ"+addedsubQueryCount;
+    addedsubQueryCount ++;*/
+   /* UUID uuid  =  UUID.randomUUID();
+    String suuid = UUID.randomUUID().toString();*/
+    Calendar c = Calendar.getInstance();
+    long time = c.getTimeInMillis();
+    String subQueryName = "subQ"+ time;
+
+    //add subQuery as incParent's rightChild;
+    LOG.info("add TOK_SUBQUERY to " + incParent.toString() + "'s right child!");
+    ASTNode subQuery = insertTokSubQuery(incParent,1,subQueryAliases,subQueryName);
+
+    addedSubQuery.add(subQuery);
+    return subQueryName;
+
+  }
+
+ /**
+  *  if pos is 0 ,insert TOK_SUBQUERY as Parent's left Child
+  * if pos is 1,insert TOK_SUBQUERY as Parent's right Child
+  * construct like follows:
+  *
+  * paret
+  *  (TOK_SUBQUERY
+  *    (TOK_QUERY
+  *      (TOK_FROM child)
+  *      (TOK_INSERT
+  *        TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)
+  *        TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)
+  *      )
+  *    )
+  *    name
+  *  )
+  * @param subQueryName
+  *      */
+private ASTNode insertTokSubQuery(ASTNode parent,int pos,String[] subQueryAliases, String subQueryName){
+   ASTNode child = (ASTNode) parent.getChild(pos);
+
+   /*paret(TOK_SUBQUERY)*/
+   ASTNode subQuery = new ASTNode(new CommonToken(HiveParser.TOK_SUBQUERY,"TOK_SUBQUERY"));
+   parent.setChild(pos, subQuery);
+   subQuery.setParent(parent);
+
+   /*TOK_SUBQUERY(TOK_QUERY)*/
+   ASTNode query = new ASTNode(new CommonToken(HiveParser.TOK_QUERY,"TOK_QUERY"));
+   subQuery.addChild(query);
+   query.setParent(subQuery);
+
+   /*TOK_SUBQUERY(tmp)*/
+   ASTNode subQueryAlias = new ASTNode(new CommonToken(HiveParser.Identifier,subQueryName));
+   subQuery.addChild(subQueryAlias);
+   subQueryAlias.setParent(subQuery);
+
+   /*TOK_QUERY(TOK_FROM) TOK_FROM(child)*/
+   ASTNode from = new ASTNode(new CommonToken(HiveParser.TOK_FROM,"TOK_FROM"));
+   query.addChild(from);
+   from.setParent(query);
+   from.addChild(child);
+   child.setParent(from);
+
+   /*TOK_QUERY(TOK_INSERT)*/
+   ASTNode insert = new ASTNode(new CommonToken(HiveParser.TOK_INSERT,"TOK_INSERT"));
+   query.addChild(insert);
+   insert.setParent(query);
+
+   /*TOK_INSERT(TOK_DESTINATION (TOK_DIR TOK_TMP_FILE))*/
+   ASTNode destination = new ASTNode(new CommonToken(HiveParser.TOK_DESTINATION,"TOK_DESTINATION"));
+   ASTNode dir = new ASTNode(new CommonToken(HiveParser.TOK_DIR,"TOK_DIR"));
+   ASTNode tmpFile = new ASTNode(new CommonToken(HiveParser.TOK_TMP_FILE,"TOK_TMP_FILE"));
+   insert.addChild(destination);
+   destination.addChild(dir);
+   dir.addChild(tmpFile);
+   tmpFile.setParent(dir);
+   dir.setParent(destination);
+   destination.setParent(insert);
+
+   /*TOK_INSERT(TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF))*/
+   ASTNode select = new ASTNode(new CommonToken(HiveParser.TOK_SELECT,"TOK_SELECT"));
+   buildSelExprs(select,subQueryAliases);
+   insert.addChild(select);
+   select.setParent(insert);
+
+   return subQuery;
+
+ }
+
+/**
+ * Build Select expression of added SubQuery,to
+ * Rename every column of table in subQueryAliases as "tabName+colName"
+ * @param select
+ * @param subQueryAliases
+ */
+private void buildSelExprs(ASTNode select, String[] subQueryAliases) {
+  for(int i=0;i<subQueryAliases.length;i++){
+    String tabAlias = subQueryAliases[i];
+    String tabName = aliasToTabs.get(tabAlias);
+    try {
+      if(tabName != null){
+        Table tab = db.getTable(tabName);
+        StructObjectInspector rowObjectInspector = (StructObjectInspector) tab
+        .getDeserializer().getObjectInspector();
+        List<? extends StructField> fields = rowObjectInspector
+        .getAllStructFieldRefs();
+        for (int j = 0; j < fields.size(); j++) {
+          String oldColumn = fields.get(j).getFieldName();
+          buildSelExpr(tabAlias,oldColumn,select);
+        }
+      }else{
+        ASTNode suQselect = subQueryaliasToSel.get(tabAlias);
+        for(int j=0; j<suQselect.getChildCount(); j++){
+          ASTNode selExpr = (ASTNode) suQselect.getChild(j);
+          ASTNode child0 = (ASTNode) selExpr.getChild(0);
+          String colName = null;
+          if(child0.getToken().getType() == HiveParser.DOT){
+            String tab = child0.getChild(0).getChild(0).getText();
+            String col = child0.getChild(1).getText();
+            colName = tab + "." + col;
+          }else if(child0.getToken().getType() == HiveParser.TOK_TABLE_OR_COL){
+            colName = child0.getChild(0).getText().toLowerCase();
+          }
+          String oldColumn = selExpr.getChildCount() == 1 ? colName
+              : selExpr.getChild(1).getText();
+          buildSelExpr(tabAlias,oldColumn,select);
+        }
+      }
+    } catch (HiveException e) {
+      // TODO Auto-generated catch block
+      e.printStackTrace();
+    } catch (SerDeException e) {
+      // TODO Auto-generated catch block
+      e.printStackTrace();
+    }
+  }
+}
+
+/**
+ * Rename: select tabAlias.oldColumn as newColName
+ * @param tabAlias
+ * @param oldColumn
+ * @param select
+ */
+private void buildSelExpr(String tabAlias, String oldColumn, ASTNode select) {
+  String[] tabCol = new String[2];
+  tabCol[0] = tabAlias;
+  tabCol[1] = oldColumn;
+  String newColName = tabCol[0] + "_"+ tabCol[1];
+  colNameMap.put(tabCol, newColName);
+  ASTNode selExpr = new ASTNode(new CommonToken(HiveParser.TOK_SELEXPR,"TOK_SELEXPR"));
+  ASTNode dot = new ASTNode(new CommonToken(HiveParser.DOT,"."));
+  ASTNode taborcol = new ASTNode(new CommonToken(HiveParser.TOK_TABLE_OR_COL,"TOK_TABLE_OR_COL"));
+  ASTNode oldTabName = new ASTNode(new CommonToken(HiveParser.Identifier,tabAlias));
+  ASTNode oldColName = new ASTNode(new CommonToken(HiveParser.Identifier,tabCol[1]));
+  ASTNode newCol = new ASTNode(new CommonToken(HiveParser.Identifier,newColName));
+  select.addChild(selExpr);
+  selExpr.setParent(select);
+  selExpr.addChild(dot);
+  dot.setParent(selExpr);
+  dot.addChild(taborcol);
+  taborcol.setParent(dot);
+  dot.addChild(oldColName);
+  oldColName.setParent(dot);
+  taborcol.addChild(oldTabName);
+  oldTabName.setParent(taborcol);
+
+  selExpr.addChild(newCol);
+  newCol.setParent(selExpr);
+}
   private QBJoinTree genJoinTreeInc(ASTNode joinParseTree)
       throws SemanticException{
 
@@ -1261,7 +1436,8 @@
 
 }
 
-  private int getIncPos(List<ASTNode> ASTJoins, HashMap<ASTNode, Boolean> includeIncToken) {
+  private ArrayList<Integer> getIncPos(List<ASTNode> ASTJoins, HashMap<ASTNode, Boolean> includeIncToken) {
+    ArrayList<Integer> ans = new ArrayList<Integer>();
     for(int i= ASTJoins.size()-1; i>=0; i--){
 
       ASTNode left = (ASTNode) ASTJoins.get(i).getChild(0);
@@ -1275,8 +1451,7 @@
             .getText().toLowerCase());
         boolean isIncludeIncTable = isIncludeIncTable(left,includeIncToken);
         if(isIncludeIncTable){
-          incTable = alias;
-          return i;
+          ans.add(i);
         }
 
       }
@@ -1292,13 +1467,12 @@
             .getText().toLowerCase());
         boolean isIncludeIncTable = isIncludeIncTable(right,includeIncToken);
         if(isIncludeIncTable){
-          incTable = alias;
-          return i;
+          ans.add(i);
         }
 
       }
     }
-    return -1;
+    return ans;
   }
 
   private boolean isIncludeIncTable(ASTNode curr, HashMap<ASTNode, Boolean> includeIncToken) {
@@ -1311,7 +1485,7 @@
     }
     switch(curr.getToken().getType()){
     case HiveParser.TOK_TABREF:
-      if(curr==incToken){
+      if(incToken.contains(curr)){
         includeIncToken.put(curr,true);
         return true;
       }else{
@@ -1367,39 +1541,28 @@
     assert QBJoinTrees.size() == ASTtrees.size();
 
     Set<String> incAliases = new HashSet<String>();
-    incAliases.add(incTable);
-    int pre = incPoses.get(incPoses.size()-1);
-    int lastIncPos = pre;
+    for(String alias:incTable){
+      incAliases.add(alias);
+    }
+
+    int lastIncPos = incPoses.get(incPoses.size()-1);
     /*traverse the trees list to move the incTable outer*/
-    for (int i = pre-1; i >= 0; i--) {
-      QBJoinTree preQBJoinTree = QBJoinTrees.get(pre);
-      QBJoinTree currQBJoinTree = QBJoinTrees.get(i);
-      String[] leftAliases = preQBJoinTree.getLeftAliases();
-      String[] rightAliases = preQBJoinTree.getRightAliases();
+    for (int inner= lastIncPos,outer=inner-1; outer >= 0; inner--,outer--) {
+      QBJoinTree innerQBJoinTree = QBJoinTrees.get(inner);
+      QBJoinTree outerQBJoinTree = QBJoinTrees.get(outer);
+      String[] leftAliases = innerQBJoinTree.getLeftAliases();
+      String[] rightAliases = innerQBJoinTree.getRightAliases();
       boolean isExchange = false;
-      if(canExchange(incAliases,currQBJoinTree.getLeftSrc(),ASTtrees,i)){
-      //otherwise, incTable is needed in both curr join and pre join,
-        //so we can't move incTable to outer
-
+      if(canExchange(incAliases,outerQBJoinTree.getLeftSrc(),ASTtrees,outer)){
         /*exchange the incTable and currAST's right table in AST tree*/
         if(java.util.Arrays.asList(leftAliases).containsAll(incAliases)){//incTable in left
-          exchangeInc(ASTtrees, preQBJoinTree, currQBJoinTree, 0,i);
+          exchangeInc(ASTtrees, innerQBJoinTree, outerQBJoinTree, 0,outer,lastIncPos,incPoses);
           isExchange = true;
-          if(lastIncPos - i > 1){
-            lastIncPos = i;
-            incPoses.add(i);
-          }else{
-            incPoses.set(incPoses.size()-1, i);
-          }
+          lastIncPos = outer;
         }else if(java.util.Arrays.asList(rightAliases).containsAll(incAliases)){// incTable in right
-          exchangeInc(ASTtrees, preQBJoinTree, currQBJoinTree, 1,i);
+          exchangeInc(ASTtrees, innerQBJoinTree, outerQBJoinTree, 1,outer,lastIncPos,incPoses);
           isExchange =true;
-          if(lastIncPos - i > 1){
-            incPoses.add(i);
-          }else{
-            incPoses.set(incPoses.size()-1, i);
-          }
-          lastIncPos = i;
+          lastIncPos = outer;
         }else{
           isExchange = false;
         }
@@ -1409,10 +1572,21 @@
         CollectionUtils.addAll(incAliases,leftAliases);
         CollectionUtils.addAll(incAliases,rightAliases);
       }
-      pre = i;
     }
   }
 
+ /**
+  * Outer join's join condition don't contain incAlias, such as:( b can exchange with c, b is incAlias)
+  * a join b
+  *   on a.col1 = b.col1
+  * join c
+  *   on a.col2 = c.col2
+  * @param incAliases
+  * @param leftSrc: the join condition source table of left
+  * @param ASTtrees
+  * @param currPos
+  * @return
+  */
   private boolean canExchange(Set<String> incAliases, List<String> leftSrc, List<ASTNode> ASTtrees, int currPos) {
     for(String leftAlias:leftSrc){
       if(incAliases.contains(leftAlias)){
@@ -1430,10 +1604,13 @@
    *  incAST is left child of currAST
    * this function exchange the incAST's incAliasPos child and currAST's right child
    * @param currPos
+   * @param incPoses
+   * @param lastIncPos
    */
- private void exchangeInc(List<ASTNode> ASTtrees, QBJoinTree preQBJoinTree,QBJoinTree currQBJoinTree,int incAliasPos, int currPos){
-    ASTNode currAST = ASTtrees.get(currPos);
-    ASTNode preAST = ASTtrees.get( currPos + 1 );
+ private void exchangeInc(List<ASTNode> ASTtrees, QBJoinTree preQBJoinTree,QBJoinTree currQBJoinTree,
+     int incAliasPos, int outer, int lastIncPos, List<Integer> incPoses){
+    ASTNode currAST = ASTtrees.get(outer);
+    ASTNode preAST = ASTtrees.get( outer + 1 );
 
     ASTNode tmpTable = (ASTNode) preAST.getChild(incAliasPos);
     preAST.setChild(incAliasPos, currAST.getChild(1));
@@ -1461,14 +1638,9 @@
       preQBJoinTree.setRightAliases(currQBJoinTree.getRightAliases());
     }
 
-    LOG.info("Exchange ");
-    for(int i= 0;i<tmpAliases.length;i++){
-      LOG.info(tmpAliases[i]);
-    }
-    LOG.info(" with ");
-    for(int i=0; i<currQBJoinTree.getRightAliases().length;i++){
-      LOG.info(currQBJoinTree.getRightAliases()[i]);
-    }
+    LOG.info("Join Reorder: Exchange inner " + Arrays.asList(tmpAliases).toString() + " with outer "
+        + Arrays.asList(currQBJoinTree.getRightAliases()).toString());
+
     // set currQBJoin's rightAliases
     currQBJoinTree.setRightAliases(tmpAliases);
 
@@ -1484,182 +1656,14 @@
     }
     currQBJoinTree.setLeftAliases(leftAliases);
 
-  }
-
-  /**
-   *  if pos is 0 ,insert TOK_SUBQUERY as Parent's left Child
-   * if pos is 1,insert TOK_SUBQUERY as Parent's right Child
-   * construct like follows:
-   *
-   * paret
-   *  (TOK_SUBQUERY
-   *    (TOK_QUERY
-   *      (TOK_FROM child)
-   *      (TOK_INSERT
-   *        TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)
-   *        TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)
-   *      )
-   *    )
-   *    name
-   *  )
-   * @param subQueryName
-   *      */
- private void insertTokSubQuery(ASTNode parent,int pos,String[] subQueryAliases, String subQueryName){
-    ASTNode child = (ASTNode) parent.getChild(pos);
-
-    /*paret(TOK_SUBQUERY)*/
-    ASTNode subQuery = new ASTNode(new CommonToken(HiveParser.TOK_SUBQUERY,"TOK_SUBQUERY"));
-    parent.setChild(pos, subQuery);
-    subQuery.setParent(parent);
-
-    /*TOK_SUBQUERY(TOK_QUERY)*/
-    ASTNode query = new ASTNode(new CommonToken(HiveParser.TOK_QUERY,"TOK_QUERY"));
-    subQuery.addChild(query);
-    query.setParent(subQuery);
-
-    /*TOK_SUBQUERY(tmp)*/
-    ASTNode subQueryAlias = new ASTNode(new CommonToken(HiveParser.Identifier,subQueryName));
-    subQuery.addChild(subQueryAlias);
-    subQueryAlias.setParent(subQuery);
-
-    /*TOK_QUERY(TOK_FROM) TOK_FROM(child)*/
-    ASTNode from = new ASTNode(new CommonToken(HiveParser.TOK_FROM,"TOK_FROM"));
-    query.addChild(from);
-    from.setParent(query);
-    from.addChild(child);
-    child.setParent(from);
-
-    /*TOK_QUERY(TOK_INSERT)*/
-    ASTNode insert = new ASTNode(new CommonToken(HiveParser.TOK_INSERT,"TOK_INSERT"));
-    query.addChild(insert);
-    insert.setParent(query);
-
-    /*TOK_INSERT(TOK_DESTINATION (TOK_DIR TOK_TMP_FILE))*/
-    ASTNode destination = new ASTNode(new CommonToken(HiveParser.TOK_DESTINATION,"TOK_DESTINATION"));
-    ASTNode dir = new ASTNode(new CommonToken(HiveParser.TOK_DIR,"TOK_DIR"));
-    ASTNode tmpFile = new ASTNode(new CommonToken(HiveParser.TOK_TMP_FILE,"TOK_TMP_FILE"));
-    insert.addChild(destination);
-    destination.addChild(dir);
-    dir.addChild(tmpFile);
-    tmpFile.setParent(dir);
-    dir.setParent(destination);
-    destination.setParent(insert);
-
-    /*TOK_INSERT(TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF))*/
-    ASTNode select = new ASTNode(new CommonToken(HiveParser.TOK_SELECT,"TOK_SELECT"));
-    buildSelExprs(select,subQueryAliases);
-    insert.addChild(select);
-    select.setParent(insert);
-
-    addedSubQuery.add(subQuery);
-  }
-
-  /**
-   * Build Select expression of added SubQuery,to
-   * Rename every column of table in subQueryAliases as "tabName+colName"
-   * @param select
-   * @param subQueryAliases
-   */
-  private void buildSelExprs(ASTNode select, String[] subQueryAliases) {
-    for(int i=0;i<subQueryAliases.length;i++){
-      String tabAlias = subQueryAliases[i];
-      String tabName = aliasToTabs.get(tabAlias);
-      try {
-        if(tabName != null){
-          Table tab = db.getTable(tabName);
-          StructObjectInspector rowObjectInspector = (StructObjectInspector) tab
-          .getDeserializer().getObjectInspector();
-          List<? extends StructField> fields = rowObjectInspector
-          .getAllStructFieldRefs();
-          for (int j = 0; j < fields.size(); j++) {
-            String oldColumn = fields.get(j).getFieldName();
-            buildSelExpr(tabAlias,oldColumn,select);
-          }
-        }else{
-          ASTNode suQselect = subQueryaliasToSel.get(tabAlias);
-          for(int j=0; j<suQselect.getChildCount(); j++){
-            ASTNode selExpr = (ASTNode) suQselect.getChild(j);
-            ASTNode child0 = (ASTNode) selExpr.getChild(0);
-            String colName = null;
-            if(child0.getToken().getType() == HiveParser.DOT){
-              String tab = child0.getChild(0).getChild(0).getText();
-              String col = child0.getChild(1).getText();
-              colName = tab + "." + col;
-            }else if(child0.getToken().getType() == HiveParser.TOK_TABLE_OR_COL){
-              colName = child0.getChild(0).getText().toLowerCase();
-            }
-            String oldColumn = selExpr.getChildCount() == 1 ? colName
-                : selExpr.getChild(1).getText();
-            buildSelExpr(tabAlias,oldColumn,select);
-          }
-        }
-      } catch (HiveException e) {
-        // TODO Auto-generated catch block
-        e.printStackTrace();
-      } catch (SerDeException e) {
-        // TODO Auto-generated catch block
-        e.printStackTrace();
-      }
+    if(lastIncPos - outer > 1){
+      incPoses.add(outer);
+    }else{
+      incPoses.set(incPoses.size()-1, outer);
     }
-  }
 
-  private void buildSelExpr(String tabAlias, String oldColumn, ASTNode select) {
-    String[] tabCol = new String[2];
-    tabCol[0] = tabAlias;
-    tabCol[1] = oldColumn;
-    String newColName = tabCol[0] + "_"+ tabCol[1];
-    colNameMap.put(tabCol, newColName);
-    ASTNode selExpr = new ASTNode(new CommonToken(HiveParser.TOK_SELEXPR,"TOK_SELEXPR"));
-    ASTNode dot = new ASTNode(new CommonToken(HiveParser.DOT,"."));
-    ASTNode taborcol = new ASTNode(new CommonToken(HiveParser.TOK_TABLE_OR_COL,"TOK_TABLE_OR_COL"));
-    ASTNode oldTabName = new ASTNode(new CommonToken(HiveParser.Identifier,tabAlias));
-    ASTNode oldColName = new ASTNode(new CommonToken(HiveParser.Identifier,tabCol[1]));
-    ASTNode newCol = new ASTNode(new CommonToken(HiveParser.Identifier,newColName));
-    select.addChild(selExpr);
-    selExpr.setParent(select);
-    selExpr.addChild(dot);
-    dot.setParent(selExpr);
-    dot.addChild(taborcol);
-    taborcol.setParent(dot);
-    dot.addChild(oldColName);
-    oldColName.setParent(dot);
-    taborcol.addChild(oldTabName);
-    oldTabName.setParent(taborcol);
-
-    selExpr.addChild(newCol);
-    newCol.setParent(selExpr);
   }
 
-
-  /**
-   * 1. add TOK_SUBQUERY to right child of incJoin in ASTtrees
-   * 2. change outer's aliases in subQueryAliases to subQuery's name
-   * @param currIncPos
-   */
- private String addSubQuery(List<ASTNode> ASTtrees, String[] subQueryAliases, int currIncPos){
-
-    ASTNode incParent = ASTtrees.get(currIncPos);
-    String subQueryName = getSubQueryName();
-
-    //add subQuery as incParent's rightChild;
-    LOG.info("add TOK_SUBQUERY to " + incParent.toString() + "'s right child!");
-    insertTokSubQuery(incParent,1,subQueryAliases,subQueryName);
-
-    return subQueryName;
-
-  }
-
-  private String getSubQueryName() {
-  /*  String name = "subQ"+addedsubQueryCount;
-    addedsubQueryCount ++;*/
-   /* UUID uuid  =  UUID.randomUUID();
-    String suuid = UUID.randomUUID().toString();*/
-    Calendar c = Calendar.getInstance();
-    long time = c.getTimeInMillis();
-    String name = "subQ"+ time;
-    return name;
-  }
-
   private void analyzeAliasBottomUp(ASTNode stop, ASTNode curr, String destName,
       String[] aliasToChange, boolean isFinalAnalyze) throws HiveException, SerDeException {
     //insert,joinCondition
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/MapReduceCompiler.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/MapReduceCompiler.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/MapReduceCompiler.java	(working copy)
@@ -81,6 +81,7 @@
 import org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer;
 import org.apache.hadoop.hive.ql.plan.ColumnStatsDesc;
 import org.apache.hadoop.hive.ql.plan.ColumnStatsWork;
+import org.apache.hadoop.hive.ql.plan.CopyFileDesc;
 import org.apache.hadoop.hive.ql.plan.CreateTableDesc;
 import org.apache.hadoop.hive.ql.plan.DDLWork;
 import org.apache.hadoop.hive.ql.plan.FetchWork;
@@ -97,10 +98,10 @@
 
 public class MapReduceCompiler {
 
-  protected final Log LOG = LogFactory.getLog(MapReduceCompiler.class);
+  protected final static Log LOG = LogFactory.getLog(MapReduceCompiler.class);
   private Hive db;
   protected LogHelper console;
-  private HiveConf conf;
+  private static HiveConf conf;
 
 
   public MapReduceCompiler() {
@@ -360,8 +361,104 @@
         tsk.setRetryCmdWhenFail(true);
       }
     }
+
+    for(Map.Entry<Task<? extends Serializable>,String> entry:procCtx.getStoreTaskTODir().entrySet()){
+      try {
+        addMoveTask(pCtx,entry.getKey(),entry.getValue());
+      } catch (IOException e) {
+        // TODO Auto-generated catch block
+        e.printStackTrace();
+      }
+    }
+    /*For store history result for incremental query*/
+    /*List<Task<? extends Serializable>> queue = new ArrayList<Task<? extends Serializable>>();
+    List<Task<? extends Serializable>> hasSeen = new ArrayList<Task<? extends Serializable>>();
+    queue.addAll(rootTasks);
+    while(!queue.isEmpty()){
+      Task<? extends Serializable> tsk = queue.remove(0);
+      if(hasSeen.contains(tsk)){
+        continue;
+      }
+      hasSeen.add(tsk);
+      if(tsk instanceof ConditionalTask){
+        for(Task<? extends Serializable> cur: ((ConditionalTask) tsk).getListTasks()){
+          queue.add(cur);
+        }
+      }else{
+        if(tsk.getChildTasks()!=null) {
+          queue.addAll(tsk.getChildTasks());
+        }
+        if(tsk instanceof ExecDriver){
+          addMoveTask(pCtx,tsk);
+        }
+      }
+
+
+    }*/
+
   }
 
+  public static void addMoveTask(ParseContext parseCtx, Task<? extends Serializable> tsk,String dirName) throws IOException {
+    // generate the temporary file
+    // it must be on the same file system as the current destination
+
+    Context baseCtx = parseCtx.getContext();
+    LOG.info("Add moveTask after "+ tsk.getId() + " for storing result of it.");
+    /*java.net.URI uri = (new Path("inc_"+ tsk.getId())).toUri();
+    System.out.println(uri.getScheme()+'*'+uri.getAuthority());
+    String targetDir = baseCtx.getExternalTmpFileURI(uri);*/
+    String targetDir = baseCtx.getIncPath(dirName);
+    String sourceDir = null;
+    if(tsk instanceof MapRedTask){
+      MapRedTask cur = (MapRedTask)tsk;
+      sourceDir = Utilities.getDir(cur.getWork().getReduceWork());
+    }
+    LOG.info("   sourceDir is " + sourceDir);
+    LOG.info("   targetDir is " + targetDir);
+    MoveWork mvWork = new MoveWork(null, null, null,
+        null, false);
+    mvWork.setCopyFileWork(new CopyFileDesc(sourceDir,targetDir,true));
+    Task<? extends Serializable> mvTask = TaskFactory.get(mvWork,conf);
+
+    List<Task<? extends Serializable>> parentTasks = new ArrayList<Task<? extends Serializable>>();
+    parentTasks.add(tsk);
+    mvTask.setParentTasks(parentTasks);
+
+    if(tsk.getChildTasks()!=null){
+      mvTask.setChildTasks(tsk.getChildTasks());
+      List<Task<? extends Serializable>> queue = new ArrayList<Task<? extends Serializable>>();
+      for(Task<? extends Serializable> child:tsk.getChildTasks()){
+        //change the ts-op of child's sourceDir to targetDir, look for GenMapRedUtils.splitTasks, setTaskPlan
+        child.getParentTasks().set(child.getParentTasks().indexOf(tsk),mvTask);
+        if(child instanceof MapRedTask){
+          queue.add(child);
+        }else if(child instanceof ConditionalTask){
+          for(Task<? extends Serializable> cur: ((ConditionalTask) child).getListTasks()){
+            if(cur instanceof MapRedTask){
+              queue.add(cur);
+            }
+          }
+        }
+        //change the child task's source path
+        for(Task<? extends Serializable> cur:queue){
+          MapredWork cplan = (MapredWork) cur.getWork();
+          MapWork mplan = cplan.getMapWork();
+          List<String> paths = mplan.getPaths();
+          if (paths.contains(sourceDir)) {
+            mplan.getPathToAliases().put(targetDir, mplan.getPathToAliases().get(sourceDir));
+            mplan.getPathToPartitionInfo().put(targetDir, mplan.getPathToPartitionInfo().get(sourceDir));
+            mplan.getPathToAliases().remove(sourceDir);
+            mplan.getPathToPartitionInfo().remove(sourceDir);
+          }
+        }
+      }
+    }
+
+    List<Task<? extends Serializable>> childs= new ArrayList<Task<? extends Serializable>>();
+    childs.add(mvTask);
+    tsk.setChildTasks(childs);
+  }
+
   private void setInputFormat(MapWork work, Operator<? extends OperatorDesc> op) {
     if (op.isUseBucketizedHiveInputFormat()) {
       work.setUseBucketizedHiveInputFormat(true);
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java	(working copy)
@@ -159,6 +159,10 @@
     return id;
   }
 
+  public void setId(String str) {
+    id = str;
+  }
+
   public int getNumGbys() {
     return numGbys;
   }
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java	(working copy)
@@ -18,6 +18,7 @@
 
 package org.apache.hadoop.hive.ql.parse;
 
+import java.io.IOException;
 import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.HashMap;
@@ -93,6 +94,7 @@
 import org.apache.hadoop.hive.ql.optimizer.Optimizer;
 import org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext;
 import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.tableSpec.SpecType;
+import org.apache.hadoop.hive.ql.parse.IncSplitSemanticAnalyzer.SimpleQB;
 import org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderExpression;
 import org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderSpec;
 import org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PTFInputSpec;
@@ -6521,6 +6523,7 @@
     JoinOperator joinOp = (JoinOperator) genJoinOperatorChildren(joinTree,
       joinSrcOp, srcOps, omitOpts);
     joinContext.put(joinOp, joinTree);
+    LOG.info("Not In incSplitSem for genJoinOperator");
     return joinOp;
   }
 
@@ -8726,7 +8729,7 @@
 
   @Override
   @SuppressWarnings("nls")
-  public void analyzeInternal(ASTNode ast) throws SemanticException {
+  public void analyzeInternal(ASTNode ast) throws SemanticException{
     ASTNode child = ast;
     this.ast = ast;
     viewsExpanded = new ArrayList<String>();
@@ -10867,7 +10870,7 @@
     return pCtx;
   }
 
-  public void MultidoPhase2forTest(ParseContext pCtx) throws SemanticException {
+  public void MultidoPhase2forTest(ParseContext pCtx) throws SemanticException, IOException {
     initCtx(pCtx.getContext());
     init();
 
@@ -11055,7 +11058,7 @@
 
   }
 
-  public void mapReudceCompiler(  ) throws SemanticException{
+  public void mapReudceCompiler(  ) throws SemanticException, IOException{
     if (!ctx.getExplainLogical()) {
       // At this point we have the complete operator tree
       // from which we want to create the map-reduce plan
@@ -11071,4 +11074,11 @@
   public ParseContext getIncParseContext(){
     return incPctx;
   }
+
+  public boolean doPhase1Inc(ASTNode ast, QB qb, Phase1Ctx ctx_1, QB qb2, Phase1Ctx ctx_2,
+      SimpleQB curSQB) throws SemanticException {
+    // TODO Auto-generated method stub
+    return false;
+  }
+
 }
\ No newline at end of file
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java	(working copy)
@@ -20,6 +20,8 @@
 
 import java.io.Serializable;
 import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Map;
 
 import org.antlr.runtime.Token;
 import org.antlr.runtime.tree.CommonTree;
@@ -33,6 +35,7 @@
   private static final long serialVersionUID = 1L;
 
   private ASTNodeOrigin origin;
+  private final Map<Integer,String> storeChild = new HashMap<Integer,String>();
 
   public ASTNode() {
   }
@@ -120,4 +123,7 @@
     return sb.toString();
   }
 
+  public Map<Integer, String> getStoreChild() {
+    return storeChild;
+  }
 }
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/IncSplitSemanticAnalyzer.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/IncSplitSemanticAnalyzer.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/IncSplitSemanticAnalyzer.java	(working copy)
@@ -2,11 +2,13 @@
 
 import java.io.Serializable;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.LinkedHashMap;
 import java.util.LinkedHashSet;
+import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
@@ -31,13 +33,13 @@
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.hooks.ReadEntity;
 import org.apache.hadoop.hive.ql.hooks.WriteEntity;
-import org.apache.hadoop.hive.ql.lib.Node;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.FilterDesc.sampleDesc;
+import org.apache.hadoop.hive.ql.plan.GroupByDesc;
 import org.apache.hadoop.hive.ql.plan.HiveOperation;
 import org.apache.hadoop.hive.ql.plan.LoadFileDesc;
 import org.apache.hadoop.hive.ql.plan.LoadTableDesc;
@@ -52,34 +54,48 @@
   //For incremental optimization
   private final ASTNode originalTree;
   private Map<String,String> colNameToType;
-  private final incCtx incCtx;
+  private final incCtx incCtx ;
   private String crtCacheQL;
   private ParseContext cacheTbPctx;
   // use for finding cachetable column;s type while processing TOK_SELECT ,it's got by genplan:genExprNodeDesc()
-  private final Map<ASTNode, ExprNodeDesc> allExprs;
+  private final Map<ASTNode, ExprNodeDesc> allExprs= new HashMap<ASTNode, ExprNodeDesc>();;
   // use for storing cachetable's column info
-  private final ArrayList<CacheColumn> collist ;
+  //private final ArrayList<CacheColumn> collist ;
   //the TOK_SELECT to extract cache table,use to identify sub query
-  private ASTNode tok_select ;
+  //private ASTNode tok_select ;
   //the TOK_FROM to extract cache table,use to identify sub query
-  private ASTNode tok_from;
+  //private ASTNode tok_from;
   //use to identify sub query
-  private ASTNode tok_query;
+  //private ASTNode tok_query;
 
-  private int incquerycount = 0;
-  private int colcount = 0;
-  private final Map<ASTNode,CacheColumn> ASTtoCol ;
+  private final Map<ASTNode,SimpleQB> splitSQBs = new HashMap<ASTNode,SimpleQB>();//TOK_QUERY -> SimpleQB
+  private final ArrayList<SimpleQB> splitJoins = new ArrayList<SimpleQB>();
+
+  private int gbySplitCount = 0;
+  //private final int colcount = 0;
+  //private final Map<ASTNode,CacheColumn> ASTtoCol ;
   //ASTNode:TOK_SELEXPR's child:FUNCTION ,use to store the ASTNode generated by the rules
   //ArrayList's size should be 2,one form incPhase1's function ,one from incPhase2's function
   //for example:on count(col) ,
   //  funcmap.get(key).get(0) should give "count" function
   //  funcmap.get(key).get(1) should give "sum" function
-  private final Map<ASTNode,ArrayList<ExprFunction>> funcmap ;
+  //private final Map<ASTNode,ArrayList<ExprFunction>> funcmap ;
   //inc optimization cache table name
   //TODO :generate automatically
-  private final String cachetable = "inc_cache_q3";
+  //private final String cachetable = "inc_cache_q3";
   //store the two QBs,one for inerst cache table ,one for select final result from cache table
-  private final ArrayList<QB> qblist ;
+  private final ArrayList<QB> gbyCacheQBlist = new ArrayList<QB>();;
+  private final ArrayList<QB> joinCacheQBlist = new ArrayList<QB>();
+  private QB finalQB = null;
+  private int joinInputsCount = 0;
+  private final ArrayList<String> scanAllIncAlias = new ArrayList<String>();//to store the inc table which should be scaned wholely, is the join source
+  /*
+   * Record distinct colName in "convertBasedonRules",
+   * add the col as TOK_GROUPBY's child in "doPhase1Inc" if not null
+   */
+  private String distinctColName = null;
+  private final Map<QBJoinTree,ASTNode> joinTreeToAST = new HashMap<QBJoinTree,ASTNode>();
+  private final Map<ASTNode,Map<String, ExprNodeDesc> > ASTtoColExprMap =  new HashMap<ASTNode,Map<String, ExprNodeDesc> >();
 
   //class for cache table column info
   public static class CacheColumn {
@@ -121,27 +137,61 @@
     ExprFunction next = null;
   }
 
+//class for ASTNode:FUNCTION generated by the rules
+  //since it will generate more functions, so we create it as a LinkList with next pointer
+  public static class SimpleQB{
+    //functionname .such as "sum" "avg"
+    String subqalias ;
+    //it's generated by the rules,  use to create a TOK_SELEXPR in the future
+    ASTNode TOK_Select = null;
+    ASTNode TOK_Query = null;
+    ASTNode TOK_From = null;
+    //function return type
+    boolean groupby = false;
+    // which phase(incphase1,incphase2) to use
+    boolean orderby = false;
+    //since it will generate more functions, so we create it as a LinkList with next pointer
+    SimpleQB subqb = null;
+    //boolean haveInctoken = false;
+    boolean distinct = false;
 
-  /*
-   * Record distinct colName in "convertBasedonRules",
-   * add the col as TOK_GROUPBY's child in "doPhase1Inc" if not null
-   */
-  private String distinctColName = null;
+    private String cachetable = null;
+    private final ArrayList<CacheColumn> collist = new ArrayList<CacheColumn>();
+    private final Map<ASTNode,CacheColumn> ASTtoCol = new HashMap<ASTNode,CacheColumn>();
+    //ASTNode:TOK_SELEXPR's child:FUNCTION ,use to store the ASTNode generated by the rules
+    //ArrayList's size should be 2,one form incPhase1's function ,one from incPhase2's function
+    //for example:on count(col) ,
+    //  funcmap.get(key).get(0) should give "count" function
+    //  funcmap.get(key).get(1) should give "sum" function
+    private final Map<ASTNode,ArrayList<ExprFunction>> funcmap = new HashMap<ASTNode,ArrayList<ExprFunction>>();
+    private int colcount = 0;
+    private Phase1Ctx ctx = null;
+    private QB qb = null;
+    private final String cacheTable = null;
+    public boolean aggregation = false;
+    public String id = null;
+    private final boolean isJoinCache = false;
+    public boolean mustOptimize(){
+      //if(haveInctoken == true && (this.groupby == true || this.orderby == true || this.distinct == true /*|| this.aggregation == true*/)){
+      if(this.groupby == true || this.orderby == true || this.distinct == true || this.aggregation == true){
+        return true;
+      }
+      return false;
+    }
+  }
 
   public IncSplitSemanticAnalyzer(HiveConf conf, ASTNode tree, incCtx ctx) throws SemanticException {
     super(conf);
     originalTree = tree;
     incCtx = ctx;
+  }
 
-    // initialize the variables
-    allExprs = new HashMap<ASTNode, ExprNodeDesc>();
-    collist =new ArrayList<CacheColumn>();
-    funcmap =new HashMap<ASTNode,ArrayList<ExprFunction>>();
-    qblist = new ArrayList<QB>();
-    ASTtoCol =new HashMap<ASTNode, CacheColumn>();
+  public IncSplitSemanticAnalyzer(HiveConf conf) throws SemanticException {
+    super(conf);
+    originalTree = null;
+    incCtx = null;
   }
 
-
   private MultiParseContext mergePctx( HashMap<Integer, ParseContext>  multiPctx ) {
     MultiParseContext multipCtx=new MultiParseContext(
         new HiveConf(),//conf
@@ -322,42 +372,55 @@
     //analyze the cache table position,
     //TODO analyze the sub query method
     SimpleQB sqb = new SimpleQB();
-    analyzeCachePosition(tree,sqb);
-    if(sqb.haveInctoken == true){
-      this.tok_query = sqb.TOK_Query;
-      this.tok_select = sqb.TOK_Select;
-      this.tok_from = sqb.TOK_From;
-      this.incquerycount++;
+    int state = analyzeCachePosition(tree,sqb,11,null);
+    if(00==state||01==state){
+      sqb.cachetable = "inc_gby_cache" + this.gbySplitCount;
+      sqb.id = null;
+      splitSQBs.put(sqb.TOK_Query,sqb);
+      this.gbySplitCount++;
+      LOG.info("Find GBY split point for outest QB");
     }
-    if(incquerycount >1){
-      throw new SemanticException("ERROR:can't support >1 inc table! ");
-    }
 
-    if(tok_select == null && tok_select.getToken().getType() != HiveParser.TOK_SELECT){
-      throw new SemanticException("ERROR:TOKEN is not a TOK_SELECT,we can't extract the cache table!");
+    LOG.info("Find " + gbySplitCount + " qbs with incremental table + gby/orderby/distinct." + "They are : " );
+    for(Map.Entry<ASTNode,SimpleQB> entry:splitSQBs.entrySet()){
+      LOG.info(entry.getKey().dump());
     }
 
     //analyze TOK_SELECT to get the CrtCacheTable QL
     //fill the collist which stores the cache table's column info
-    analyzeTokenSelect();
+    for(SimpleQB cursqb:splitSQBs.values()){
+      if(cursqb.TOK_Select == null && cursqb.TOK_Select.getToken().getType() != HiveParser.TOK_SELECT){
+        throw new SemanticException("ERROR:TOKEN is not a TOK_SELECT,we can't extract the cache table!");
+      }
+      analyzeTokenSelect(cursqb);
+      creatCacheTable(cursqb);
+    }
+    for(SimpleQB cursqb:splitJoins){
+      creatCacheTable(cursqb);
+    }
 
-    //generate the CrtCacheQuery based on the collist
-    StringBuilder CrtCacheQueryBuilder = new StringBuilder("create table "+this.cachetable+" ( ");
+  }
+
+
+  private void creatCacheTable(SimpleQB cursqb) throws SemanticException {
+
+  //generate the CrtCacheQuery based on the collist
+    String tableName =  cursqb.cachetable;
+    StringBuilder CrtCacheQueryBuilder = new StringBuilder("create table "+tableName+" ( ");
     String CrtcacheQuery;
-    for (int i = 0; i < collist.size(); i++) {
+    for (int i = 0; i < cursqb.collist.size(); i++) {
       if (i > 0) {
         CrtCacheQueryBuilder.append(" , ");
       }
-      CrtCacheQueryBuilder.append(collist.get(i).columnname);
+      CrtCacheQueryBuilder.append(cursqb.collist.get(i).columnname);
       CrtCacheQueryBuilder.append(" ");
-      CrtCacheQueryBuilder.append(collist.get(i).type);
+      CrtCacheQueryBuilder.append(cursqb.collist.get(i).type);
 
     }
     CrtCacheQueryBuilder.append(")");
     CrtcacheQuery = CrtCacheQueryBuilder.toString();
 
-    String DropcacheQuery = "drop table if exists "+this.cachetable ;
-    LOG.info("===============wanglei inc test:create cache table query=========================");
+    String DropcacheQuery = "Drop table if exists "+tableName ;
     LOG.info(DropcacheQuery);
     LOG.info(CrtcacheQuery);
 
@@ -374,11 +437,9 @@
     if (ret != 0) {
       throw new SemanticException("ERROR: can't create cache table!");
     }
+  }
 
-    //CrttmpQuery = new VariableSubstitution().substitute(conf, CrttmpQuery);
 
-  }
-
   /**
    * analyze TOK_SELECT to get the CrtCacheTable QL
    * use collist to store the column info
@@ -400,11 +461,11 @@
    * TODO: complex expression
    * @throws SemanticException
    */
-  private void analyzeTokenSelect() throws SemanticException {
+  private void analyzeTokenSelect(SimpleQB sqb) throws SemanticException {
 
-    int child_count = this.tok_select.getChildCount();
+    int child_count = sqb.TOK_Select.getChildCount();
     for(int child_pos = 0;child_pos < child_count ;++child_pos){
-      ASTNode tok_selexpr = (ASTNode) this.tok_select.getChild(child_pos);
+      ASTNode tok_selexpr = (ASTNode) sqb.TOK_Select.getChild(child_pos);
       if(tok_selexpr.getToken().getType() == HiveParser.TOK_SELEXPR){
         //EXPRESSION:tok_selexpr.getChild(0)
         //ALIAS     :tok_selexpr.getChild(1)
@@ -413,7 +474,7 @@
         if(tok_selexpr.getChildCount()==2){
           alias = ((ASTNode) tok_selexpr.getChild(1)).getToken().getText();
         }
-        analyzeTokenSelectExpr(selexprchild,alias);
+        analyzeTokenSelectExpr(sqb,selexprchild,alias);
 
       }
 
@@ -421,7 +482,7 @@
 
   }
 
-  private void analyzeTokenSelectExpr(ASTNode selexprchild,String alias ) throws SemanticException{
+  private void analyzeTokenSelectExpr(SimpleQB sqb,ASTNode selexprchild,String alias ) throws SemanticException{
 
     ExprNodeDesc exprnode = allExprs.get(selexprchild);
     if(exprnode == null){
@@ -435,8 +496,8 @@
         ||selexprchild.getToken().getType() == HiveParser.MINUS
         ||selexprchild.getToken().getType() == HiveParser.PLUS){
 
-      analyzeTokenSelectExpr((ASTNode)selexprchild.getChild(0),"") ;
-      analyzeTokenSelectExpr((ASTNode)selexprchild.getChild(1),"") ;
+      analyzeTokenSelectExpr(sqb,(ASTNode)selexprchild.getChild(0),"") ;
+      analyzeTokenSelectExpr(sqb,(ASTNode)selexprchild.getChild(1),"") ;
 
     }else if(selexprchild.getToken().getType() == HiveParser.TOK_TABLE_OR_COL){
       ASTNode taborcolname =  (ASTNode) selexprchild.getChild(0);
@@ -446,27 +507,27 @@
       col.alias = alias;
       col.type = exprnode.getTypeString();
       col.origcolumnAST = selexprchild;
-      collist.add(col);
-      ASTtoCol.put(selexprchild, col);
+      sqb.collist.add(col);
+      sqb.ASTtoCol.put(selexprchild, col);
 
     }else if(selexprchild.getToken().getType() == HiveParser.TOK_FUNCTION
         ||selexprchild.getToken().getType() == HiveParser.TOK_FUNCTIONDI/*kangynli added*/){
       ASTNode funcnameast =  (ASTNode) selexprchild.getChild(0);
       String funcname = funcnameast.getToken().getText();
-      int tmp = colcount;
+      int tmp = sqb.colcount;
       /**
        * Based on the Rules, generate the function's ASTNode for IncPhase1 and IncPhase2
        * use ArrayList<ExprFunction> funclist to store the result of  ASTNode.
        * use  funclist(0):ExprFunction1  to store the function of IncPhase1
        * use  funclist(1):ExprFunction2  to store the function of IncPhase2
        */
-      colcount = convertBasedonRules(selexprchild,funcname,colcount);
-      ArrayList<ExprFunction> funclist = funcmap.get(selexprchild);
+      sqb.colcount = convertBasedonRules(sqb,selexprchild,funcname,sqb.colcount);
+      ArrayList<ExprFunction> funclist = sqb.funcmap.get(selexprchild);
 
       ExprFunction expr1 = funclist.get(0);
 
 
-      for(int i=0;i<colcount-tmp;i++){
+      for(int i=0;i<sqb.colcount-tmp;i++){
         if(expr1 != null){
 
           CacheColumn col = new CacheColumn();
@@ -482,7 +543,7 @@
           if (i != 0) {
             col.isredundant = true;
           }
-          collist.add(col);
+          sqb.collist.add(col);
           expr1 = expr1.next;
         } else {
           throw new SemanticException("ERROR:we should not reach here, the ASTNode is "
@@ -490,8 +551,8 @@
         }
       }
     }else if(selexprchild.getToken().getType() == HiveParser.DOT){
-      String colunm = "col"+ colcount;
-      colcount++;
+      String colunm = "col"+ sqb.colcount;
+      sqb.colcount++;
       String dotcol = ((ASTNode)selexprchild.getChild(1)).getToken().getText();
       if(exprnode != null){
 
@@ -501,8 +562,8 @@
         col.dotcol = dotcol;
         col.type = exprnode.getTypeString();
         col.origcolumnAST = selexprchild ;
-        collist.add(col);
-        ASTtoCol.put(selexprchild, col);
+        sqb.collist.add(col);
+        sqb.ASTtoCol.put(selexprchild, col);
       }
     }
 
@@ -541,39 +602,9 @@
 
   }
 
-  //class for ASTNode:FUNCTION generated by the rules
-  //since it will generate more functions, so we create it as a LinkList with next pointer
-  public static class SimpleQB{
-    //functionname .such as "sum" "avg"
-    String subqalias ;
-    //it's generated by the rules,  use to create a TOK_SELEXPR in the future
-    ASTNode TOK_Select = null;
-    ASTNode TOK_Query = null;
-    ASTNode TOK_From = null;
-    //function return type
-    boolean groupby = false;
-    // which phase(incphase1,incphase2) to use
-    boolean orderby = false;
-    //since it will generate more functions, so we create it as a LinkList with next pointer
-    SimpleQB subqb = null;
-    boolean haveInctoken = false;
-
-    //kangyanli added begin
-    boolean distinct = false;
-    //kangyanli added end
-
-    public boolean mustOptimize(){
-      if(haveInctoken == true && (this.groupby == true || this.orderby == true || this.distinct == true)){
-        return true;
-      }
-      return false;
-    }
-  }
-
-
-
-  private void analyzeCachePosition(ASTNode ast,SimpleQB sqb) throws SemanticException {
+  private int analyzeCachePosition(ASTNode ast, SimpleQB sqb, int inputstate, String outerAlias) throws SemanticException {
     // find the cache table position in the all query trees
+    int state = inputstate;
     boolean skipRecursion = false;
     if (ast.getToken() != null) {
       skipRecursion = true;
@@ -582,13 +613,20 @@
         sqb.TOK_Query = ast;
         skipRecursion = false;
         break;
+      case HiveParser.TOK_FUNCTIONDI:
+        sqb.distinct = true;
+        break;
       case HiveParser.TOK_SELECTDI:
-        //kangyanli added begin
         sqb.distinct = true;
-        //kangyanli added end
       case HiveParser.TOK_SELECT:
         sqb.TOK_Select = ast;
         skipRecursion = true;
+        for (int i = 0; i < ast.getChildCount(); ++i) {
+          ASTNode selExpr = (ASTNode) ast.getChild(i);
+          if (selExpr.getToken().getType() == HiveParser.TOK_SELEXPR && hasAggregation(selExpr)) {
+            sqb.aggregation  = true;
+          }
+        }
         break;
 
       case HiveParser.TOK_WHERE:
@@ -602,38 +640,8 @@
 
       case HiveParser.TOK_FROM:
         sqb.TOK_From = ast;
-
-        // Check if this is a subquery / lateral view
         ASTNode frm = (ASTNode) ast.getChild(0);
-        if (frm.getToken().getType() == HiveParser.TOK_TABREF) {
-          for(Node child: frm.getChildren()){
-            if(((ASTNode)child).getToken().getType() == HiveParser.TOK_INCRE){
-              sqb.haveInctoken = true;
-            }
-          }
-
-        } else if (frm.getToken().getType() == HiveParser.TOK_SUBQUERY) {
-          SimpleQB ssqb = new SimpleQB();
-          analyzeCachePosition((ASTNode)frm.getChild(0),ssqb);
-          String alias = unescapeIdentifier(frm.getChild(1).getText());
-          ssqb.subqalias = alias;
-          if(ssqb.mustOptimize() == true){
-            this.tok_query = ssqb.TOK_Query;
-            this.tok_select = ssqb.TOK_Select;
-            this.tok_from = ssqb.TOK_From;
-            this.incquerycount++;
-          }else if(ssqb.haveInctoken){
-            sqb.haveInctoken = true;
-          }
-        } else if (frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW ||
-            frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW_OUTER) {
-
-        } else if (isJoinToken(frm)) {
-          analyzeJoinPosition(frm,sqb);
-        }else if(frm.getToken().getType() == HiveParser.TOK_PTBLFUNCTION){
-        }
-
-
+        state = analyzeFromCachePosition(frm,sqb,state,outerAlias);
         skipRecursion = true;
         break;
 
@@ -659,11 +667,6 @@
         sqb.groupby = true;
 
         break;
-      //kangyanli added begin
-      case HiveParser.TOK_FUNCTIONDI:
-        sqb.distinct = true;
-        break;
-      //kangyanli added end
 
       case HiveParser.TOK_HAVING:
         break;
@@ -701,53 +704,144 @@
       for (int child_pos = 0; child_pos < child_count ; ++child_pos) {
         // Recurse
         //phase1Result = phase1Result && doPhase1Inc((ASTNode) ast.getChild(child_pos), qb, ctx_1,qb2,ctx_2);
-        analyzeCachePosition((ASTNode) ast.getChild(child_pos),sqb);
+        state = analyzeCachePosition((ASTNode) ast.getChild(child_pos),sqb,state,outerAlias);
 
       }
     }
+    return state;
   }
 
-  private void analyzeJoinPosition(ASTNode join,SimpleQB sqb) throws SemanticException {
-    int numChildren = join.getChildCount();
-    if ((numChildren != 2) && (numChildren != 3)
-        && join.getToken().getType() != HiveParser.TOK_UNIQUEJOIN) {
-      throw new SemanticException(generateErrorMessage(join,
-          "Join with multiple children"));
+  private boolean hasAggregation(ASTNode selExpr) {
+
+    ASTNode selexprchild = (ASTNode) selExpr.getChild(0);
+
+    //analyze EXPRESSION's three type :TOK_TABLE_OR_COL ,FUNCTION,DOT
+    if(selexprchild.getToken().getType() == HiveParser.DIVIDE
+        ||selexprchild.getToken().getType() == HiveParser.STAR
+        ||selexprchild.getToken().getType() == HiveParser.MINUS
+        ||selexprchild.getToken().getType() == HiveParser.PLUS){
+
+      if(hasAggregation((ASTNode)selexprchild.getChild(0)) || hasAggregation((ASTNode)selexprchild.getChild(1)) ){
+        return true;
+      }
+
+    }else if(selexprchild.getToken().getType() == HiveParser.TOK_FUNCTION
+        ||selexprchild.getToken().getType() == HiveParser.TOK_FUNCTIONDI){
+      ASTNode funcnameast =  (ASTNode) selexprchild.getChild(0);
+      String funcname = funcnameast.getToken().getText();
+      if(funcname.toLowerCase().equals("sum") || funcname.toLowerCase().equals("count") || funcname.toLowerCase().equals("avg")
+          || funcname.toLowerCase().equals("min") || funcname.toLowerCase().equals("max")){
+        return true;
+      }
+
     }
 
-    for (int num = 0; num < numChildren; num++) {
-      ASTNode child = (ASTNode) join.getChild(num);
-      if (child.getToken().getType() == HiveParser.TOK_TABREF) {
-        for(Node c: child.getChildren()){
-          if(((ASTNode)c).getToken().getType() == HiveParser.TOK_INCRE){
-            sqb.haveInctoken = true;
-          }
-        }
+    return false;
+  }
 
-      } else if (child.getToken().getType() == HiveParser.TOK_SUBQUERY) {
-        SimpleQB ssqb = new SimpleQB();
-        analyzeCachePosition((ASTNode)child.getChild(0),ssqb);
-        String alias = unescapeIdentifier(child.getChild(1).getText());
-        ssqb.subqalias = alias;
+  private int analyzeFromCachePosition(ASTNode frm ,SimpleQB sqb, int inputstate, String outerAlias) throws SemanticException {
+    // Check if this is a subquery / lateral view
+    int state = inputstate;
+    if (frm.getToken().getType() == HiveParser.TOK_TABREF) {
+//      for(Node child: frm.getChildren()){
+//        if(((ASTNode)child).getToken().getType() == HiveParser.TOK_INCRE){
+//          sqb.haveInctoken = true;
+//        }
+//      }
+      if(ASTNodeUtils.isIncTabRef(frm)){
+        state = new Integer(01);
+      }else{
+        state = new Integer(11);
+      }
+      outerAlias =  QB.getAppendedAliasFromId(outerAlias, ASTNodeUtils.getAliasId(frm));
+      LOG.info("Find Cache position | To table " + outerAlias+ "; state is " + state);
+
+    } else if (frm.getToken().getType() == HiveParser.TOK_SUBQUERY) {
+      String subQalias = unescapeIdentifier(frm.getChild(1).getText());
+      String newOuterAlias = QB.getAppendedAliasFromId(outerAlias,subQalias);
+      SimpleQB ssqb = new SimpleQB();
+      ssqb.subqalias = subQalias;
+      state = analyzeCachePosition((ASTNode)frm.getChild(0),ssqb,state,newOuterAlias);
+      LOG.info("Find Cache position | Get subQ "+ subQalias + " state: " + state);
+      if(01 == state || 00 ==state){
         if(ssqb.mustOptimize() == true){
-          this.tok_query = ssqb.TOK_Query;
-          this.tok_select = ssqb.TOK_Select;
-          this.tok_from = ssqb.TOK_From;
-          this.incquerycount++;
-        }else if(ssqb.haveInctoken){
-          sqb.haveInctoken = true;
+          ssqb.cachetable = "inc_gby_cache" + this.gbySplitCount;
+          ssqb.id = newOuterAlias;
+          splitSQBs.put(ssqb.TOK_Query,ssqb);
+          this.gbySplitCount++;
+          state = 10;
+          LOG.info("Find GBY split point. stop broadcast - split here " + newOuterAlias + "; Change state to " + state);
         }
-      } else if (child.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) {
-
-      } else if (child.getToken().getType() == HiveParser.TOK_LATERAL_VIEW ||
-          child.getToken().getType() == HiveParser.TOK_LATERAL_VIEW_OUTER){
-      } else if (isJoinToken(child)) {
-        analyzeJoinPosition(child,sqb);
+//        else if(ssqb.haveInctoken){
+//          sqb.haveInctoken = true;
+//          LOG.info("continue broadcast - don't split here " + newOuterAlias);
+//        }
       }
+    } else if (frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW ||
+        frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW_OUTER) {
+      //TODO
+    } else if (isJoinToken(frm)) {
+      state = analyzeJoinCachePosition(frm,sqb,state,outerAlias);
+      LOG.info("Find Cache position | Get Join state: " + state);
+    }else if(frm.getToken().getType() == HiveParser.TOK_PTBLFUNCTION){
+      //TODO
     }
+    return state;
+
   }
 
+  private int analyzeJoinCachePosition(ASTNode join,SimpleQB sqb, int inputstate, String outerAlias) throws SemanticException {
+    int numChildren = join.getChildCount();
+    int state = inputstate;
+    if((numChildren != 2) && (numChildren != 3)
+      && join.getToken().getType() != HiveParser.TOK_UNIQUEJOIN){
+      throw new SemanticException(generateErrorMessage(join,
+        "analyzeJoinCachePosition don't support Join with more than 2 child"));
+    }
+    ASTNode child0 = (ASTNode) join.getChild(0),child1 =(ASTNode) join.getChild(1);
+    int state0 = analyzeFromCachePosition(child0,sqb,state,outerAlias);
+    int state1 = analyzeFromCachePosition(child1,sqb,state,outerAlias);
+    LOG.info("Find Cache position | To join; state is " + state0 + " " + state1);
+    if(state0 == 10 && state1 == 01){
+      storeJoinChild(child1,outerAlias);
+      state = 10;
+    }else if(state0 == 01 && state1 == 10){
+      storeJoinChild(child0,outerAlias);
+      state = 10;
+    }else if(state0 == 01 && state1 == 01){
+      storeJoinChild(child1,outerAlias);
+      storeJoinChild(child0,outerAlias);
+      state = 11;
+    }else if(state0 == 10 && state1 == 10){
+      state = 10;
+    }else if(state0 == 11){
+      state = state1;
+    }else if(state1 == 11){
+      state = state0;
+    }
+    return state;
+  }
 
+  private void storeJoinChild(ASTNode child,String outerAlias) throws SemanticException {
+    if(child.getToken().getType() == HiveParser.TOK_TABREF){
+      String tabAlias = ASTNodeUtils.getAliasId(child);
+      scanAllIncAlias.add(QB.getAppendedAliasFromId(outerAlias,tabAlias));
+      LOG.info("Change to scan whole table " + QB.getAppendedAliasFromId(outerAlias, tabAlias));
+    }else if(child.getToken().getType() == HiveParser.TOK_SUBQUERY){
+      ASTNode join = (ASTNode) child.getParent();
+      String dirName = "joinInputCacheDir"+joinInputsCount;
+      joinInputsCount ++;
+      join.getStoreChild().put(join.getChildren().indexOf(child),dirName);
+      LOG.info("Find join input cache point in AST. input is subQ " + outerAlias);
+    }else if(isJoinToken(child)){
+      ASTNode join = (ASTNode) child.getParent();
+      String dirName = "joinInputCacheDir"+joinInputsCount;
+      joinInputsCount ++;
+      join.getStoreChild().put(join.getChildren().indexOf(child),dirName);
+      LOG.info("Find join input cache point in AST. input is join");
+    }
+  }
+
   /**
    * Based on the Rules, generate the function's ASTNode for IncPhase1 and IncPhase2
    * use ArrayList<ExprFunction> funclist to store the result of  ASTNode.
@@ -756,7 +850,7 @@
    * then store <funcAST,funclist> in the  funcmap
    * @return colcount, to generate the name of the column
    */
-  private int convertBasedonRules(ASTNode funcAST,String funcname,int colcount) throws SemanticException {
+  private int convertBasedonRules(SimpleQB sqb,ASTNode funcAST,String funcname,int colcount) throws SemanticException {
 
     ArrayList<ExprFunction> ExprList =new ArrayList<ExprFunction>();
     if(funcname.toLowerCase().equals("sum")){
@@ -922,7 +1016,7 @@
       throw new SemanticException("Inc ERROR: should have two expr func,one for insert  cache ,one for select cache");
     }
 
-    funcmap.put(funcAST, ExprList);
+    sqb.funcmap.put(funcAST, ExprList);
     return colcount;
 
   }
@@ -958,15 +1052,417 @@
     return cached;
   }
 
-  public ArrayList<QB> getQBList(){
+  @Override
+  protected QBJoinTree genJoinTree(QB qb, ASTNode joinParseTree,
+      Map<String, Operator> aliasToOpInfo)
+      throws SemanticException {
+    QBJoinTree joinTree = new QBJoinTree();
+    JoinCond[] condn = new JoinCond[1];
 
-    return qblist;
+    switch (joinParseTree.getToken().getType()) {
+    case HiveParser.TOK_LEFTOUTERJOIN:
+      joinTree.setNoOuterJoin(false);
+      condn[0] = new JoinCond(0, 1, JoinType.LEFTOUTER);
+      break;
+    case HiveParser.TOK_RIGHTOUTERJOIN:
+      joinTree.setNoOuterJoin(false);
+      condn[0] = new JoinCond(0, 1, JoinType.RIGHTOUTER);
+      break;
+    case HiveParser.TOK_FULLOUTERJOIN:
+      joinTree.setNoOuterJoin(false);
+      condn[0] = new JoinCond(0, 1, JoinType.FULLOUTER);
+      break;
+    case HiveParser.TOK_LEFTSEMIJOIN:
+      joinTree.setNoSemiJoin(false);
+      condn[0] = new JoinCond(0, 1, JoinType.LEFTSEMI);
+      break;
+    default:
+      condn[0] = new JoinCond(0, 1, JoinType.INNER);
+      joinTree.setNoOuterJoin(true);
+      break;
+    }
+
+    joinTree.setJoinCond(condn);
+
+    ASTNode left = (ASTNode) joinParseTree.getChild(0);
+    ASTNode right = (ASTNode) joinParseTree.getChild(1);
+
+    if ((left.getToken().getType() == HiveParser.TOK_TABREF)
+        || (left.getToken().getType() == HiveParser.TOK_SUBQUERY)
+        || (left.getToken().getType() == HiveParser.TOK_PTBLFUNCTION)) {
+      String tableName = getUnescapedUnqualifiedTableName((ASTNode) left.getChild(0))
+          .toLowerCase();
+      String alias = left.getChildCount() == 1 ? tableName
+          : unescapeIdentifier(left.getChild(left.getChildCount() - 1)
+          .getText().toLowerCase());
+      // ptf node form is: ^(TOK_PTBLFUNCTION $name $alias? partitionTableFunctionSource partitioningSpec? expression*)
+      // guranteed to have an lias here: check done in processJoin
+      alias = (left.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) ?
+          unescapeIdentifier(left.getChild(1).getText().toLowerCase()) :
+            alias;
+      joinTree.setLeftAlias(alias);
+      String[] leftAliases = new String[1];
+      leftAliases[0] = alias;
+      joinTree.setLeftAliases(leftAliases);
+      String[] children = new String[2];
+      children[0] = alias;
+      joinTree.setBaseSrc(children);
+      joinTree.setId(qb.getId());
+      joinTree.getAliasToOpInfo().put(
+          getModifiedAlias(qb, alias), aliasToOpInfo.get(alias));
+    } else if (isJoinToken(left)) {
+      QBJoinTree leftTree = genJoinTree(qb, left, aliasToOpInfo);
+      joinTree.setJoinSrc(leftTree);
+      String[] leftChildAliases = leftTree.getLeftAliases();
+      String leftAliases[] = new String[leftChildAliases.length + 1];
+      for (int i = 0; i < leftChildAliases.length; i++) {
+        leftAliases[i] = leftChildAliases[i];
+      }
+      leftAliases[leftChildAliases.length] = leftTree.getRightAliases()[0];
+      joinTree.setLeftAliases(leftAliases);
+    } else {
+      assert (false);
+    }
+
+    if ((right.getToken().getType() == HiveParser.TOK_TABREF)
+        || (right.getToken().getType() == HiveParser.TOK_SUBQUERY)
+        || (right.getToken().getType() == HiveParser.TOK_PTBLFUNCTION)) {
+      String tableName = getUnescapedUnqualifiedTableName((ASTNode) right.getChild(0))
+          .toLowerCase();
+      String alias = right.getChildCount() == 1 ? tableName
+          : unescapeIdentifier(right.getChild(right.getChildCount() - 1)
+          .getText().toLowerCase());
+      // ptf node form is: ^(TOK_PTBLFUNCTION $name $alias? partitionTableFunctionSource partitioningSpec? expression*)
+      // guranteed to have an lias here: check done in processJoin
+      alias = (right.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) ?
+          unescapeIdentifier(right.getChild(1).getText().toLowerCase()) :
+            alias;
+      String[] rightAliases = new String[1];
+      rightAliases[0] = alias;
+      joinTree.setRightAliases(rightAliases);
+      String[] children = joinTree.getBaseSrc();
+      if (children == null) {
+        children = new String[2];
+      }
+      children[1] = alias;
+      joinTree.setBaseSrc(children);
+      joinTree.setId(qb.getId());
+      joinTree.getAliasToOpInfo().put(
+          getModifiedAlias(qb, alias), aliasToOpInfo.get(alias));
+      // remember rhs table for semijoin
+      if (joinTree.getNoSemiJoin() == false) {
+        joinTree.addRHSSemijoin(alias);
+      }
+    } else {
+      assert false;
+    }
+
+    ArrayList<ArrayList<ASTNode>> expressions = new ArrayList<ArrayList<ASTNode>>();
+    expressions.add(new ArrayList<ASTNode>());
+    expressions.add(new ArrayList<ASTNode>());
+    joinTree.setExpressions(expressions);
+
+    ArrayList<Boolean> nullsafes = new ArrayList<Boolean>();
+    joinTree.setNullSafes(nullsafes);
+
+    ArrayList<ArrayList<ASTNode>> filters = new ArrayList<ArrayList<ASTNode>>();
+    filters.add(new ArrayList<ASTNode>());
+    filters.add(new ArrayList<ASTNode>());
+    joinTree.setFilters(filters);
+    joinTree.setFilterMap(new int[2][]);
+
+    ArrayList<ArrayList<ASTNode>> filtersForPushing =
+        new ArrayList<ArrayList<ASTNode>>();
+    filtersForPushing.add(new ArrayList<ASTNode>());
+    filtersForPushing.add(new ArrayList<ASTNode>());
+    joinTree.setFiltersForPushing(filtersForPushing);
+
+    ASTNode joinCond = (ASTNode) joinParseTree.getChild(2);
+    ArrayList<String> leftSrc = new ArrayList<String>();
+    parseJoinCondition(joinTree, joinCond, leftSrc);
+    if (leftSrc.size() == 1) {
+      joinTree.setLeftAlias(leftSrc.get(0));
+    }
+
+    // check the hints to see if the user has specified a map-side join. This
+    // will be removed later on, once the cost-based
+    // infrastructure is in place
+    if (qb.getParseInfo().getHints() != null) {
+      List<String> mapSideTables = getMapSideJoinTables(qb);
+      List<String> mapAliases = joinTree.getMapAliases();
+
+      for (String mapTbl : mapSideTables) {
+        boolean mapTable = false;
+        for (String leftAlias : joinTree.getLeftAliases()) {
+          if (mapTbl.equalsIgnoreCase(leftAlias)) {
+            mapTable = true;
+          }
+        }
+        for (String rightAlias : joinTree.getRightAliases()) {
+          if (mapTbl.equalsIgnoreCase(rightAlias)) {
+            mapTable = true;
+          }
+        }
+
+        if (mapTable) {
+          if (mapAliases == null) {
+            mapAliases = new ArrayList<String>();
+          }
+          mapAliases.add(mapTbl);
+          joinTree.setMapSideJoin(true);
+        }
+      }
+
+      joinTree.setMapAliases(mapAliases);
+
+      parseStreamTables(joinTree, qb);
+    }
+    //joinTreeToAST.put(joinTree, joinParseTree);
+    LOG.info("In incSplitSem for genJoinTree");
+    if(joinParseTree.getStoreChild().size() != 0){
+      LOG.info("Find join input cache point for QBJoinTree");
+      joinTree.setStoreChild(joinParseTree.getStoreChild());
+    }
+    return joinTree;
   }
 
+  @Override
+  protected Operator genJoinOperator(QB qb, QBJoinTree joinTree,
+      Map<String, Operator> map) throws SemanticException {
+    QBJoinTree leftChild = joinTree.getJoinSrc();
+    Operator joinSrcOp = null;
+    if (leftChild != null) {
+      LOG.info("Call next genJoinOperator");
+      Operator joinOp = genJoinOperator(qb, leftChild, map);
+      ArrayList<ASTNode> filter = joinTree.getFiltersForPushing().get(0);
+      for (ASTNode cond : filter) {
+        joinOp = genFilterPlan(qb, cond, joinOp);
+      }
 
+      joinSrcOp = genJoinReduceSinkChild(qb, joinTree, joinOp, null, 0);
+    }
 
+    Operator[] srcOps = new Operator[joinTree.getBaseSrc().length];
 
+    HashSet<Integer> omitOpts = null; // set of input to the join that should be
+    // omitted by the output
+    int pos = 0;
+    for (String src : joinTree.getBaseSrc()) {
+      if (src != null) {
+        Operator srcOp = map.get(src.toLowerCase());
+
+        // for left-semi join, generate an additional selection & group-by
+        // operator before ReduceSink
+        ArrayList<ASTNode> fields = joinTree.getRHSSemijoinColumns(src);
+        if (fields != null) {
+          // the RHS table columns should be not be output from the join
+          if (omitOpts == null) {
+            omitOpts = new HashSet<Integer>();
+          }
+          omitOpts.add(pos);
+
+          // generate a selection operator for group-by keys only
+          srcOp = insertSelectForSemijoin(fields, srcOp);
+
+          // generate a groupby operator (HASH mode) for a map-side partial
+          // aggregation for semijoin
+          srcOp = genMapGroupByForSemijoin(qb, fields, srcOp,
+              GroupByDesc.Mode.HASH);
+        }
+
+        // generate a ReduceSink operator for the join
+        srcOps[pos] = genJoinReduceSinkChild(qb, joinTree, srcOp, src, pos);
+        pos++;
+      } else {
+        assert pos == 0;
+        srcOps[pos++] = null;
+      }
+    }
+
+    // Type checking and implicit type conversion for join keys
+    genJoinOperatorTypeCheck(joinSrcOp, srcOps);
+
+    JoinOperator joinOp = (JoinOperator) genJoinOperatorChildren(joinTree,
+      joinSrcOp, srcOps, omitOpts);
+    joinContext.put(joinOp, joinTree);
+
+//    ASTNode astJoin = joinTreeToAST.get(joinTree);
+//    ASTtoColExprMap.put(astJoin,joinOp.getColumnExprMap());
+    LOG.info("In incSplitSem for genJoinOperator");
+    for(Map.Entry<Integer,String> entry:joinTree.getStoreChild().entrySet()){
+      int storePos = entry.getKey().intValue();
+      String dirName = entry.getValue();
+      ReduceSinkOperator storeRSOp = (ReduceSinkOperator) joinOp.getParentOperators().get(storePos);
+      storeRSOp.setStoreDirName(dirName);
+      LOG.info("Find join input cache point in OpTree "  + storeRSOp.toString() + " aliases are " +
+          storeRSOp.getInputAlias() + " store Dir is " + dirName);
+    }
+    return joinOp;
+  }
+
   /**
+   * Merges node to target
+   */
+  @Override
+  protected void mergeJoins(QB qb, QBJoinTree node, QBJoinTree target, int pos) {
+    String[] nodeRightAliases = node.getRightAliases();
+    String[] trgtRightAliases = target.getRightAliases();
+    String[] rightAliases = new String[nodeRightAliases.length
+        + trgtRightAliases.length];
+
+    for (int i = 0; i < trgtRightAliases.length; i++) {
+      rightAliases[i] = trgtRightAliases[i];
+    }
+    for (int i = 0; i < nodeRightAliases.length; i++) {
+      rightAliases[i + trgtRightAliases.length] = nodeRightAliases[i];
+    }
+    target.setRightAliases(rightAliases);
+    target.getAliasToOpInfo().putAll(node.getAliasToOpInfo());
+
+    String[] nodeBaseSrc = node.getBaseSrc();
+    String[] trgtBaseSrc = target.getBaseSrc();
+    LOG.info("[rightAliases ] node's are " + Arrays.asList(nodeRightAliases).toString() + " target's are " + Arrays.asList(trgtRightAliases).toString());
+    LOG.info("[baseSrc ] node's are " + Arrays.asList(nodeBaseSrc).toString() + " target's are " + Arrays.asList(trgtBaseSrc).toString());
+    LOG.info("merge pos is " + pos);
+    String[] baseSrc = new String[nodeBaseSrc.length + trgtBaseSrc.length - 1];
+
+    for (int i = 0; i < trgtBaseSrc.length; i++) {
+      baseSrc[i] = trgtBaseSrc[i];
+    }
+    for (int i = 1; i < nodeBaseSrc.length; i++) {
+      baseSrc[i + trgtBaseSrc.length - 1] = nodeBaseSrc[i];
+    }
+    target.setBaseSrc(baseSrc);
+
+    //for incremental store join input
+    if(node.getStoreChild().keySet().contains(0)){
+
+      if(trgtBaseSrc[pos] != null){
+        incCtx.getParserRes().getScanAllIncAliases().add(trgtBaseSrc[pos]);
+        LOG.info("Change join store child; Add to scanAllIncAlias: " + trgtBaseSrc[pos]);
+      }else{
+        target.getStoreChild().put(pos, node.getStoreChild().get(0));
+        LOG.info("Change join store child from " + 0 + " to " + pos );
+      }
+    }
+    for (int i = 1; i < nodeBaseSrc.length; i++) {
+      if(node.getStoreChild().keySet().contains(i)){
+        target.getStoreChild().put(i + trgtBaseSrc.length - 1,node.getStoreChild().get(i) );
+        LOG.info("change join store child from " + i + " to " + new Integer(i + trgtBaseSrc.length - 1));
+      }
+    }
+
+    ArrayList<ArrayList<ASTNode>> expr = target.getExpressions();
+    for (int i = 0; i < nodeRightAliases.length; i++) {
+      expr.add(node.getExpressions().get(i + 1));
+    }
+
+    ArrayList<Boolean> nns = node.getNullSafes();
+    ArrayList<Boolean> tns = target.getNullSafes();
+    for (int i = 0; i < tns.size(); i++) {
+      tns.set(i, tns.get(i) & nns.get(i)); // any of condition contains non-NS, non-NS
+    }
+
+    ArrayList<ArrayList<ASTNode>> filters = target.getFilters();
+    for (int i = 0; i < nodeRightAliases.length; i++) {
+      filters.add(node.getFilters().get(i + 1));
+    }
+
+    if (node.getFilters().get(0).size() != 0) {
+      ArrayList<ASTNode> filterPos = filters.get(pos);
+      filterPos.addAll(node.getFilters().get(0));
+    }
+
+    int[][] nmap = node.getFilterMap();
+    int[][] tmap = target.getFilterMap();
+    int[][] newmap = new int[tmap.length + nmap.length - 1][];
+
+    for (int[] mapping : nmap) {
+      if (mapping != null) {
+        for (int i = 0; i < mapping.length; i += 2) {
+          if (pos > 0 || mapping[i] > 0) {
+            mapping[i] += trgtRightAliases.length;
+          }
+        }
+      }
+    }
+    if (nmap[0] != null) {
+      if (tmap[pos] == null) {
+        tmap[pos] = nmap[0];
+      } else {
+        int[] appended = new int[tmap[pos].length + nmap[0].length];
+        System.arraycopy(tmap[pos], 0, appended, 0, tmap[pos].length);
+        System.arraycopy(nmap[0], 0, appended, tmap[pos].length, nmap[0].length);
+        tmap[pos] = appended;
+      }
+    }
+    System.arraycopy(tmap, 0, newmap, 0, tmap.length);
+    System.arraycopy(nmap, 1, newmap, tmap.length, nmap.length - 1);
+    target.setFilterMap(newmap);
+
+    ArrayList<ArrayList<ASTNode>> filter = target.getFiltersForPushing();
+    for (int i = 0; i < nodeRightAliases.length; i++) {
+      filter.add(node.getFiltersForPushing().get(i + 1));
+    }
+
+    if (node.getFiltersForPushing().get(0).size() != 0) {
+      ArrayList<ASTNode> filterPos = filter.get(pos);
+      filterPos.addAll(node.getFiltersForPushing().get(0));
+    }
+
+    if (node.getNoOuterJoin() && target.getNoOuterJoin()) {
+      target.setNoOuterJoin(true);
+    } else {
+      target.setNoOuterJoin(false);
+    }
+
+    if (node.getNoSemiJoin() && target.getNoSemiJoin()) {
+      target.setNoSemiJoin(true);
+    } else {
+      target.setNoSemiJoin(false);
+    }
+
+    target.mergeRHSSemijoin(node);
+
+    JoinCond[] nodeCondns = node.getJoinCond();
+    int nodeCondnsSize = nodeCondns.length;
+    JoinCond[] targetCondns = target.getJoinCond();
+    int targetCondnsSize = targetCondns.length;
+    JoinCond[] newCondns = new JoinCond[nodeCondnsSize + targetCondnsSize];
+    for (int i = 0; i < targetCondnsSize; i++) {
+      newCondns[i] = targetCondns[i];
+    }
+
+    for (int i = 0; i < nodeCondnsSize; i++) {
+      JoinCond nodeCondn = nodeCondns[i];
+      if (nodeCondn.getLeft() == 0) {
+        nodeCondn.setLeft(pos);
+      } else {
+        nodeCondn.setLeft(nodeCondn.getLeft() + targetCondnsSize);
+      }
+      nodeCondn.setRight(nodeCondn.getRight() + targetCondnsSize);
+      newCondns[targetCondnsSize + i] = nodeCondn;
+    }
+
+    target.setJoinCond(newCondns);
+    if (target.isMapSideJoin()) {
+      assert node.isMapSideJoin();
+      List<String> mapAliases = target.getMapAliases();
+      for (String mapTbl : node.getMapAliases()) {
+        if (!mapAliases.contains(mapTbl)) {
+          mapAliases.add(mapTbl);
+        }
+      }
+      target.setMapAliases(mapAliases);
+    }
+  }
+
+  public ArrayList<QB> getGbyCacheQBList(){
+
+    return gbyCacheQBlist;
+  }
+
+  /**
    *   Spiting origin query , to generate insert ql and query ql
    *   of cache .then generate optree for each ql,
    * @throws SemanticException
@@ -975,18 +1471,12 @@
 
     LOG.info("Incremental Query Semantic Analysis DoPhase1 Start!");
 
-    // overwrite dophase1 to split original query into two QB
-    // fill the two QBs into the qblist
+    // overwrite dophase1 to split original query into three kinds of QB
+    // gbyCacheQB, joinCahceQB, finalQB
     analyzeIncPhase1(this.originalTree);
 
   }
 
-  public void genLogicalPlan() throws SemanticException{
-    //compile and generate MR task based on IncPhase1's QB : getQBList().get(0)
-    analyzeQB(this.originalTree, getQBList().get(0));
-  }
-
-
   /**
    * overwrite dophase1 to split original query into two QB
    * fill the two QBs into the qblist
@@ -1015,30 +1505,57 @@
       // prevent view from referencing itself
       viewsExpanded.add(SessionState.get().getCurrentDatabase() + "." + createVwDesc.getViewName());
     }
+    //ArrayList<Phase1Ctx> ctxs = new ArrayList<Phase1Ctx>();
+    //ArrayList<QB> qbs = new ArrayList<QB>();
+    for(Map.Entry<ASTNode,SimpleQB> entry: splitSQBs.entrySet()){
+      Phase1Ctx ctx_1 = initPhase1Ctx();
+      ctx_1.isinc = true;
+      QB qb1 = new QB(null, null, false);
+      entry.getValue().ctx = ctx_1;
+      entry.getValue().qb = qb1;
 
-    Phase1Ctx ctx_1 = initPhase1Ctx();
-    Phase1Ctx ctx_2 = initPhase1Ctx();
-    ctx_1.isinc = true;
+    }
 
-
     //create two QB
     //qb1 is IncPhase1's QB
     //qb2 is IncPhase2's QB
-    QB qb1 = new QB(null, null, false);
-    QB qb2 = new QB(null, null, false);
+    finalQB = new QB(null, null, false);
+    Phase1Ctx ctx_final = initPhase1Ctx();
 
     //generate the two QBs
-    if (!doPhase1Inc(child, qb1, ctx_1,qb2,ctx_2)) {
+    if (!doPhase1Inc(child,null,null, finalQB, ctx_final,null)) {
       // if phase1Result false return
       return ;
     }
 
-    qblist.add(qb1);
-    qblist.add(qb2);
+//    if(finalQB.getParseInfo().getJoinExpr() != null){
+//      //handle point of intersection of inc path
+//      Integer state = new Integer(10);
+//      LOG.info("=====Begin join cache analyze!=====");
+//      incJoinAnalyze(finalQB.getParseInfo().getJoinExpr(),state,finalQB.getId());
+//    }
 
+    //qblist.addAll(qbs);
+    for(Map.Entry<ASTNode,SimpleQB> entry: splitSQBs.entrySet()){
+      gbyCacheQBlist.add(entry.getValue().qb);
+    }
+    //qblist.add(qb2);
+    LOG.info("GbyCacheQBlist.size is " + gbyCacheQBlist.size() + "; They are: ");
+//    for(QB cur:gbyCacheQBlist){
+//      cur.print("");
+//      LOG.info("=========");
+//    }
+    LOG.info("JoinCacheQBlist.size is " + joinCacheQBlist.size() + "; Ther are: ");
+//    for(QB cur:joinCacheQBlist){
+//      cur.print("");
+//      LOG.info("=========");
+//    }
+//    LOG.info("Final QB is: " );
+//    finalQB.print("");
+    /*qblist.add(qb1);
+    qblist.add(qb2);*/
   }
 
-
   /**
    * generate two QBs
    * one for insert cache table
@@ -1050,13 +1567,19 @@
    */
 
   @Override
-  public boolean doPhase1Inc(ASTNode ast, QB qb, Phase1Ctx ctx_1, QB qb2, Phase1Ctx ctx_2)
+  public boolean doPhase1Inc(ASTNode ast, QB qb, Phase1Ctx ctx_1, QB qb2, Phase1Ctx ctx_2,SimpleQB curSQB)
       throws SemanticException {
 
     boolean phase1Result = true;
-    QBParseInfo qbp = qb.getParseInfo();
+    QBParseInfo qbp = null;
+    if(qb != null) {
+      qbp = qb.getParseInfo();
+    }
     QBParseInfo qbp2 = qb2.getParseInfo();
     boolean skipRecursion = false;
+    //Phase1Ctx ctx_1 = new Phase1Ctx();
+    //SimpleQB curSQB = null;
+    //QB qb = null;
 
     if (ast.getToken() != null) {
       skipRecursion = true;
@@ -1064,10 +1587,21 @@
       case HiveParser.TOK_QUERY:
         skipRecursion = false;
         // inc wanglei
-        if (ast.equals(this.tok_query)) {
+        /*if (ast.equals(this.tok_query)) {
           ctx_1.isinc = true;
         } else {
           ctx_1.isinc = false;
+        }*/
+        if(splitSQBs.containsKey(ast)){
+          LOG.info("Find inc TOK_QUERY: ");
+          LOG.info(ast.dump());
+          curSQB = splitSQBs.get(ast);
+          ctx_1 = curSQB.ctx;
+          qb = curSQB.qb;
+          qbp = qb.getParseInfo();
+        }else{
+          ctx_1 = null;
+          //ctx_1.isinc = false;
         }
 
         break;
@@ -1076,7 +1610,7 @@
         qb.countSelDi();
         // fall through
       case HiveParser.TOK_SELECT:
-        if (ctx_1.isinc && this.tok_select.equals(ast)) {
+        if (ctx_1 != null && curSQB.TOK_Select.equals(ast)) {
           //kangyanli added begin
           ASTNode tableselect1 = null,tableselect2 = null;
           if(ast.getToken().getType() == HiveParser.TOK_SELECT){
@@ -1092,9 +1626,9 @@
           ASTNode tokselexpr2 = null;
           // ArrayList<ExprFunction> exprlist = funcmap.get(ast);
 
-          for (int i = 0 ; i < collist.size(); i++) {
+          for (int i = 0 ; i < curSQB.collist.size(); i++) {
             ASTNode origintokselexpr = (ASTNode) ast.getChild(i);
-            ASTNode child = collist.get(i).origcolumnAST;
+            ASTNode child = curSQB.collist.get(i).origcolumnAST;
 
 
             ASTNode tableselexpr = new ASTNode(new CommonToken(HiveParser.TOK_SELEXPR, "TOK_SELEXPR"));
@@ -1108,7 +1642,7 @@
           for(int i = 0;i<ast.getChildCount();i++){
             ASTNode origintokselexpr = (ASTNode) ast.getChild(i);
 
-            ASTNode origintokselexprchild =newSelectExpr2((ASTNode)origintokselexpr.getChild(0));
+            ASTNode origintokselexprchild =newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(0));
 
             ASTNode tableselexpr = new ASTNode(new CommonToken(HiveParser.TOK_SELEXPR, "TOK_SELEXPR"));
             tableselexpr.addChild(origintokselexprchild);
@@ -1162,18 +1696,22 @@
         } else {
 
           qb2.countSel();
-          qbp2.setSelExprForClause(ctx_1.dest, ast);
+          //qbp2.setSelExprForClause(ctx_1.dest, ast);
+          qbp2.setSelExprForClause(ctx_2.dest, ast);
 
 
           if (((ASTNode) ast.getChild(0)).getToken().getType() == HiveParser.TOK_HINTLIST) {
             qbp2.setHints((ASTNode) ast.getChild(0));
           }
 
-          LinkedHashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast,
-              qb2, ctx_1.dest);
-          doPhase1GetColumnAliasesFromSelect(ast, qbp2);
+          /*LinkedHashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast, qb2, ctx_1.dest);
           qbp2.setAggregationExprsForClause(ctx_1.dest, aggregations);
           qbp2.setDistinctFuncExprsForClause(ctx_1.dest,
+              doPhase1GetDistinctFuncExprs(aggregations));*/
+          LinkedHashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast, qb2, ctx_2.dest);
+          doPhase1GetColumnAliasesFromSelect(ast, qbp2);
+          qbp2.setAggregationExprsForClause(ctx_2.dest, aggregations);
+          qbp2.setDistinctFuncExprsForClause(ctx_2.dest,
               doPhase1GetDistinctFuncExprs(aggregations));
 
         }
@@ -1183,10 +1721,11 @@
         break;
 
       case HiveParser.TOK_WHERE:
-        if (ctx_1.isinc) {
+        if (ctx_1 != null) {
           qbp.setWhrExprForClause(ctx_1.dest, ast);
         } else {
-          qbp2.setWhrExprForClause(ctx_1.dest, ast);
+          //qbp2.setWhrExprForClause(ctx_1.dest, ast);
+          qbp2.setWhrExprForClause(ctx_2.dest, ast);
         }
 
         break;
@@ -1194,7 +1733,7 @@
       case HiveParser.TOK_INSERT_INTO:
         String currentDatabase = SessionState.get().getCurrentDatabase();
         String tab_name = getUnescapedName((ASTNode) ast.getChild(0).getChild(0), currentDatabase);
-        if (ctx_1.isinc) {
+        if (ctx_1 != null) {
           qbp2.addInsertIntoTable(tab_name);
         } else {
           qbp2.addInsertIntoTable(tab_name);
@@ -1202,14 +1741,16 @@
 
 
       case HiveParser.TOK_DESTINATION:
-        ctx_1.dest = "insclause-" + ctx_1.nextNum;
-        ctx_1.nextNum++;
+        if(ctx_1!=null){
+          ctx_1.dest = "insclause-" + ctx_1.nextNum;
+          ctx_1.nextNum++;
+        }
 
         ctx_2.dest = "insclause-" + ctx_2.nextNum;
         ctx_2.nextNum++;
 
         // is there a insert in the subquery
-        if (qbp.getIsSubQ()) {
+        if (qbp != null && qbp.getIsSubQ()) {
           ASTNode ch = (ASTNode) ast.getChild(0);
           if ((ch.getToken().getType() != HiveParser.TOK_DIR)
               || (((ASTNode) ch.getChild(0)).getToken().getType() != HiveParser.TOK_TMP_FILE)) {
@@ -1217,10 +1758,10 @@
                 .getMsg(ast));
           }
         }
-        if (ctx_1.isinc) {
+        if (ctx_1 != null) {
           //no insertoverwrite
 
-          ASTNode toktab = ASTNodeUtils.newtokTable(this.cachetable);
+          ASTNode toktab = ASTNodeUtils.newtokTable(curSQB.cachetable);
           qbp.setDestForClause(ctx_1.dest, toktab);
 
           String currentDatabase1 = SessionState.get().getCurrentDatabase();
@@ -1233,7 +1774,7 @@
           LOG.info("INC_AST TOK_DESTINATION.toktab: " + toktab.dump());
           LOG.info("INC_AST TOK_DESTINATION.toktmpfile: " + tokdchild.dump());
         } else {
-          qbp2.setDestForClause(ctx_1.dest, (ASTNode) ast.getChild(0));
+          qbp2.setDestForClause(ctx_2.dest, (ASTNode) ast.getChild(0));
         }
 
 
@@ -1246,12 +1787,12 @@
               "Multiple Children " + child_count));
         }
 
-        if (ctx_1.isinc && ast.equals(this.tok_from)) {
+        if (ctx_1 != null && ast.equals(curSQB.TOK_From)) {
           // --inc code
           // Create a cache Table for qb2;
           // Insert this map into the stats
-          String alias = this.cachetable;
-          String tabIdName = this.cachetable;
+          String alias = curSQB.cachetable;
+          String tabIdName = curSQB.cachetable;
           qb2.setTabAlias(alias, tabIdName);
           qb2.addAlias(alias);
           ASTNode tableref = ASTNodeUtils.newtoktabref(tabIdName);
@@ -1262,6 +1803,8 @@
           // --- inc code
           // Check if this is a subquery / lateral view
           ASTNode frm = (ASTNode) ast.getChild(0);
+          qb.setId(curSQB.id);
+          LOG.info("Set outer alias for split gby point: " + qb.getId());
           if (frm.getToken().getType() == HiveParser.TOK_TABREF) {
             processTable(qb, frm);
           } else if (frm.getToken().getType() == HiveParser.TOK_SUBQUERY) {
@@ -1272,6 +1815,7 @@
           } else if (isJoinToken(frm)) {
             queryProperties.setHasJoin(true);
             processJoin(qb, frm);
+            //processIncJoin(qb,qb2, frm,curSQB);
             qbp.setJoinExpr(frm);
           } else if (frm.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) {
             queryProperties.setHasPTF(true);
@@ -1284,13 +1828,13 @@
           if (frm.getToken().getType() == HiveParser.TOK_TABREF) {
             processTable(qb2, frm);
           } else if (frm.getToken().getType() == HiveParser.TOK_SUBQUERY) {
-            processIncSubQuery(qb,qb2, frm);
+            processIncSubQuery(qb,qb2, frm, curSQB);
           } else if (frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW ||
               frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW_OUTER) {
             //TODO
           } else if (isJoinToken(frm)) {
-
-            processIncJoin(qb,qb2, frm);
+            queryProperties.setHasJoin(true);
+            processIncJoin(qb,qb2, frm,curSQB);
             qbp2.setJoinExpr(frm);
           } else if (frm.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) {
             //TODO
@@ -1342,24 +1886,32 @@
         // select list
         queryProperties.setHasOrderBy(true);
 
-        if (ctx_1.isinc) {
+        if (ctx_1 != null) {
           // --- inc code
           // qbp.setOrderByExprForClause(ctx_1.dest, ast);
-          ASTNode tokOrderBY = createOrderByAST(ast);
+          ASTNode tokOrderBY = createOrderByAST(curSQB,ast);
           qbp2.setOrderByExprForClause(ctx_2.dest, tokOrderBY);
           LOG.info("INC AST: TOK_ORDERBY 1 :" + ast.dump());
           LOG.info("INC AST: TOK_ORDERBY 2 :" + tokOrderBY.dump());
           // --- inc code
-
+          if (qbp.getClusterByForClause(ctx_1.dest) != null) {
+            throw new SemanticException(generateErrorMessage(ast,
+                ErrorMsg.CLUSTERBY_ORDERBY_CONFLICT.getMsg()));
+          }
         } else {
-          qbp2.setOrderByExprForClause(ctx_1.dest, ast);
+          //qbp2.setOrderByExprForClause(ctx_1.dest, ast);
+          qbp2.setOrderByExprForClause(ctx_2.dest, ast);
+          if (qbp2.getClusterByForClause(ctx_2.dest) != null) {
+            throw new SemanticException(generateErrorMessage(ast,
+                ErrorMsg.CLUSTERBY_ORDERBY_CONFLICT.getMsg()));
+          }
         }
 
 
-        if (qbp.getClusterByForClause(ctx_1.dest) != null) {
+        /*if (qbp.getClusterByForClause(ctx_1.dest) != null) {
           throw new SemanticException(generateErrorMessage(ast,
               ErrorMsg.CLUSTERBY_ORDERBY_CONFLICT.getMsg()));
-        }
+        }*/
         break;
 
       case HiveParser.TOK_GROUPBY:
@@ -1371,7 +1923,7 @@
         queryProperties.setHasGroupBy(true);
 
 
-        if (ctx_1.isinc) {
+        if (ctx_1 != null) {
           if (qbp.getJoinExpr() != null) {
             queryProperties.setHasJoinFollowedByGroupBy(true);
           }
@@ -1387,8 +1939,8 @@
           int gbycount = 0;
           for (int index = 0; index < ast.getChildCount(); index++) {
             ASTNode origgrychild = (ASTNode) ast.getChild(index);
-            for (int i = 0; i < collist.size(); i++) {
-              CacheColumn col = collist.get(i);
+            for (int i = 0; i < curSQB.collist.size(); i++) {
+              CacheColumn col = curSQB.collist.get(i);
               if (col.isfunction == false) {
                 if (ASTNodeUtils.isSameAST(origgrychild, col.origcolumnAST)) {
                   tokgrychild = ASTNodeUtils.newtoktableorcol(col.columnname);
@@ -1423,24 +1975,26 @@
           if (qbp2.getJoinExpr() != null) {
             queryProperties.setHasJoinFollowedByGroupBy(true);
           }
-          if (qbp2.getSelForClause(ctx_1.dest).getToken().getType() == HiveParser.TOK_SELECTDI) {
+          //if (qbp2.getSelForClause(ctx_1.dest).getToken().getType() == HiveParser.TOK_SELECTDI) {
+          if (qbp2.getSelForClause(ctx_2.dest).getToken().getType() == HiveParser.TOK_SELECTDI) {
             throw new SemanticException(generateErrorMessage(ast,
                 ErrorMsg.SELECT_DISTINCT_WITH_GROUPBY.getMsg()));
           }
-          qbp2.setGroupByExprForClause(ctx_1.dest, ast);
+          //qbp2.setGroupByExprForClause(ctx_1.dest, ast);
+          qbp2.setGroupByExprForClause(ctx_2.dest, ast);
         }
 
 
         skipRecursion = true;
 
         // Rollup and Cubes are syntactic sugar on top of grouping sets
-        if (ast.getToken().getType() == HiveParser.TOK_ROLLUP_GROUPBY) {
+        /*if (ast.getToken().getType() == HiveParser.TOK_ROLLUP_GROUPBY) {
           qbp.getDestRollups().add(ctx_1.dest);
         } else if (ast.getToken().getType() == HiveParser.TOK_CUBE_GROUPBY) {
           qbp.getDestCubes().add(ctx_1.dest);
         } else if (ast.getToken().getType() == HiveParser.TOK_GROUPING_SETS) {
           qbp.getDestGroupingSets().add(ctx_1.dest);
-        }
+        }*/
         break;
 
       case HiveParser.TOK_HAVING:
@@ -1551,33 +2105,33 @@
       for (int child_pos = 0; child_pos < child_count && phase1Result; ++child_pos) {
         // Recurse
         phase1Result = phase1Result
-            && doPhase1Inc((ASTNode) ast.getChild(child_pos), qb, ctx_1, qb2, ctx_2);
+            && doPhase1Inc((ASTNode) ast.getChild(child_pos), qb, ctx_1, qb2, ctx_2, curSQB);
       }
     }
     return phase1Result;
   }
 
-  private ASTNode newSelectExpr2( ASTNode origintokselexpr) throws SemanticException {
+  private ASTNode newSelectExpr2(SimpleQB curSQB, ASTNode origintokselexpr) throws SemanticException {
     // TODO Auto-generated method stub
     int type = origintokselexpr.getToken().getType();
     switch(type){
     case HiveParser.STAR :
-      return ASTNodeUtils.newStarAST(newSelectExpr2((ASTNode)origintokselexpr.getChild(0)),
-      newSelectExpr2((ASTNode)origintokselexpr.getChild(1)) );
+      return ASTNodeUtils.newStarAST(newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(0)),
+      newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(1)) );
     case HiveParser.MINUS :
-      return ASTNodeUtils.newMinusAST(newSelectExpr2((ASTNode)origintokselexpr.getChild(0)),
-          newSelectExpr2((ASTNode)origintokselexpr.getChild(1)) );
+      return ASTNodeUtils.newMinusAST(newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(0)),
+          newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(1)) );
     case HiveParser.PLUS :
-      return ASTNodeUtils.newPlusAST(newSelectExpr2((ASTNode)origintokselexpr.getChild(0)),
-          newSelectExpr2((ASTNode)origintokselexpr.getChild(1)) );
+      return ASTNodeUtils.newPlusAST(newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(0)),
+          newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(1)) );
     case HiveParser.DIVIDE :
-      return ASTNodeUtils.newDividAST(newSelectExpr2((ASTNode)origintokselexpr.getChild(0)),
-          newSelectExpr2((ASTNode)origintokselexpr.getChild(1)) );
+      return ASTNodeUtils.newDividAST(newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(0)),
+          newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(1)) );
     case HiveParser.TOK_FUNCTION :
-       return ASTNodeUtils.newSelExpr2withFunc(origintokselexpr, funcmap);
+       return ASTNodeUtils.newSelExpr2withFunc(origintokselexpr, curSQB.funcmap);
     case HiveParser.TOK_TABLE_OR_COL :
     case HiveParser.DOT :
-       String colname = ASTtoCol.get(origintokselexpr).columnname;
+       String colname = curSQB.ASTtoCol.get(origintokselexpr).columnname;
       return ASTNodeUtils.newtoktableorcol(colname);
     default:
       return origintokselexpr;
@@ -1586,7 +2140,7 @@
     }
   }
 
-  private void processIncJoin(QB qb1, QB qb2, ASTNode join)throws SemanticException {
+  private void processIncJoin(QB qb1, QB qb2, ASTNode join,SimpleQB curSQB)throws SemanticException {
     int numChildren = join.getChildCount();
     if ((numChildren != 2) && (numChildren != 3)
         && join.getToken().getType() != HiveParser.TOK_UNIQUEJOIN) {
@@ -1599,20 +2153,20 @@
       if (child.getToken().getType() == HiveParser.TOK_TABREF) {
         processTable(qb2, child);
       } else if (child.getToken().getType() == HiveParser.TOK_SUBQUERY) {
-        processIncSubQuery(qb1,qb2, child);
+        processIncSubQuery(qb1,qb2, child,curSQB);
       } else if (child.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) {
 
       } else if (child.getToken().getType() == HiveParser.TOK_LATERAL_VIEW ||
           child.getToken().getType() == HiveParser.TOK_LATERAL_VIEW_OUTER) {
 
       } else if (isJoinToken(child)) {
-        processIncJoin(qb1,qb2, child);
+        processIncJoin(qb1,qb2, child,curSQB);
       }
     }
   }
 
 
-  private String processIncSubQuery(QB qb1, QB qb2, ASTNode subq) throws SemanticException {
+  private String processIncSubQuery(QB qb1, QB qb2, ASTNode subq,SimpleQB curSQB) throws SemanticException {
     /*if(((ASTNode)subq.getChild(0)).equals(this.tok_query)){
 
 
@@ -1629,7 +2183,7 @@
     // Recursively do the first phase of semantic analysis for the subquery
     QBExpr qbexpr = new QBExpr(alias);
 
-    doPhase1IncQBExpr(subqref, qbexpr, qb2.getId(), alias,qb1);
+    doPhase1IncQBExpr(subqref, qbexpr, qb2.getId(), alias,qb1,curSQB);
 
     // If the alias is already there then we have a conflict
     if (qb2.exists(alias)) {
@@ -1649,7 +2203,7 @@
   }
 
 
-  private void doPhase1IncQBExpr(ASTNode subqref, QBExpr qbexpr, String id, String alias,QB qb1) throws SemanticException {
+  private void doPhase1IncQBExpr(ASTNode subqref, QBExpr qbexpr, String id, String alias,QB qb1,SimpleQB curSQB) throws SemanticException {
 
     assert (subqref.getToken() != null);
     switch (subqref.getToken().getType()) {
@@ -1657,7 +2211,8 @@
       QB qb = new QB(id, alias, true);
       Phase1Ctx ctx_1 = initPhase1Ctx();
       Phase1Ctx ctx_2 = initPhase1Ctx();
-      doPhase1Inc(subqref,qb1, ctx_1,qb, ctx_2);
+      //doPhase1Inc(subqref,qb1, ctx_1,qb, ctx_2,curSQB);
+      doPhase1Inc(subqref,null, null,qb, ctx_2,curSQB);
 
       qbexpr.setOpcode(QBExpr.Opcode.NULLOP);
       qbexpr.setQB(qb);
@@ -1684,7 +2239,7 @@
    * @throws SemanticException
    */
 
-  private ASTNode createOrderByAST(ASTNode orderby) throws SemanticException{
+  private ASTNode createOrderByAST(SimpleQB curSQB,ASTNode orderby) throws SemanticException{
     ASTNode dest =new ASTNode(new CommonToken(HiveParser.TOK_ORDERBY, "TOK_ORDERBY"));
     ASTNode tokodychild = null;
     ASTNode expr = null;
@@ -1701,8 +2256,8 @@
       tokodychild.setParent(dest);
       ASTNode origexpr = (ASTNode)origorderchild.getChild(0);
 
-        for(int i=0; i < collist.size(); i++){
-          CacheColumn col = collist.get(i);
+        for(int i=0; i < curSQB.collist.size(); i++){
+          CacheColumn col = curSQB.collist.get(i);
           if(ASTNodeUtils.isSameAST(origexpr, col.origcolumnAST)){
             expr = ASTNodeUtils.newtoktableorcol(col.columnname);
             expr.setParent(tokodychild);
@@ -1737,14 +2292,6 @@
     return dest;
   }
 
-  public HashMap<String, Table> getTabNameToTab() throws HiveException {
-    HashMap<String, Table> tabNameToTab = new HashMap< String,Table > ();
-    String incTabName = incCtx.getParserRes().getIncTableName();
-    Table table = db.getTable(incTabName);
-    tabNameToTab.put(incTabName, table);
-    return tabNameToTab;
-  }
-
   @Override
   public void reset(){
     super.reset();
@@ -1766,4 +2313,16 @@
     prunedPartitions.clear();
 
   }
+
+  public ArrayList<QB> getJoinCacheQBList() {
+    return joinCacheQBlist;
+  }
+
+  public QB getFinalQB() {
+    return finalQB;
+  }
+
+  public ArrayList<String> getScanAllIncAliases(){
+    return scanAllIncAlias;
+  }
 }
Index: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java	(working copy)
@@ -2187,7 +2187,7 @@
     return success;
   }
 
-  static protected void copyFiles(HiveConf conf, Path srcf, Path destf, FileSystem fs)
+  public static void copyFiles(HiveConf conf, Path srcf, Path destf, FileSystem fs)
       throws HiveException {
     boolean inheritPerms = HiveConf.getBoolVar(conf,
         HiveConf.ConfVars.HIVE_WAREHOUSE_SUBDIR_INHERIT_PERMS);
Index: ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java	(working copy)
@@ -275,4 +275,5 @@
     // be difficult to figure out the big table for the mapjoin.
     return false;
   }
+
 }
Index: ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java	(working copy)
@@ -79,6 +79,7 @@
   transient protected int numDistributionKeys;
   transient protected int numDistinctExprs;
   transient String inputAlias;  // input alias of this RS for join (used for PPD)
+  private String storeDirName = null; // for store incremental join input
 
   public void setInputAlias(String inputAlias) {
     this.inputAlias = inputAlias;
@@ -428,4 +429,13 @@
   public boolean opAllowedBeforeMapJoin() {
     return false;
   }
+
+  public void setStoreDirName(String dirName) {
+    storeDirName = dirName;
+  }
+
+  public String getStoreDirName() {
+    return storeDirName ;
+  }
+
 }
Index: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java	(working copy)
@@ -1869,6 +1869,7 @@
     synchronized (INPUT_SUMMARY_LOCK) {
       // For each input path, calculate the total size.
       for (String path : work.getPathToAliases().keySet()) {
+        LOG.info(work.getPathToAliases().get(path).toString());
         Path p = new Path(path);
 
         if (filter != null && !filter.accept(p)) {
@@ -3007,5 +3008,37 @@
       }
     }
   }
+
+  public static String getDir(ReduceWork rWork){
+    if(rWork == null){
+      return null;
+    }
+    List<Operator<? extends OperatorDesc>> ops = new LinkedList<Operator<? extends OperatorDesc>>();
+    ops.add(rWork.getReducer());
+    String ans = null;
+    while(!ops.isEmpty()){
+
+      Operator<? extends OperatorDesc> op = ops.remove(0);
+
+      if (op instanceof FileSinkOperator) {
+
+        FileSinkDesc fdesc = ((FileSinkOperator) op).getConf();
+        ans = fdesc.getDirName();
+
+        /*if (tempDir != null) {
+          Path tempPath = Utilities.toTempPath(new Path(tempDir));
+          FileSystem fs = tempPath.getFileSystem(conf);
+          fs.mkdirs(tempPath);
+        }*/
+        break;
+      }
+
+      if (op.getChildOperators() != null) {
+        ops.addAll(op.getChildOperators());
+      }
+
+    }
+    return ans;
+  }
 }
 
Index: ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java	(working copy)
@@ -49,12 +49,14 @@
 import org.apache.hadoop.hive.ql.lockmgr.HiveLock;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockManager;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockObj;
+import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.BucketCol;
 import org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.SortCol;
 import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;
+import org.apache.hadoop.hive.ql.plan.CopyFileDesc;
 import org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx;
 import org.apache.hadoop.hive.ql.plan.LoadFileDesc;
 import org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc;
@@ -226,6 +228,15 @@
         }
       }
 
+      // CopyFile to hive_inc file
+      CopyFileDesc cfd = work.getCopyFileWork();
+      if(cfd != null){
+        Path srcPath = new Path(cfd.getSourceDir());
+        Path destPath = new Path(cfd.getTargetDir());
+        boolean isDfs = cfd.getIsDfDir();
+        copyFile(srcPath,destPath,isDfs);
+      }
+
       // Next we do this for tables and partitions
       LoadTableDesc tbd = work.getLoadTableWork();
       if (tbd != null) {
@@ -435,6 +446,23 @@
     }
   }
 
+  private void copyFile(Path sourcePath, Path targetPath, boolean isDfs) throws IOException {
+
+    FileSystem fs = sourcePath.getFileSystem(conf);
+    String mesg = "Incremental Copying data to: " + targetPath.toString();
+    String mesg_detail = " from " + sourcePath.toString();
+    console.printInfo(mesg, mesg_detail);
+
+    if (fs.exists(sourcePath)) {
+      try {
+        Hive.copyFiles(conf, sourcePath, targetPath, fs);
+      } catch (HiveException e) {
+        // TODO Auto-generated catch block
+        e.printStackTrace();
+      }
+    }
+  }
+
   private boolean isSkewedStoredAsDirs(LoadTableDesc tbd) {
     return (tbd.getLbCtx() == null) ? false : tbd.getLbCtx()
         .isSkewedStoredAsDir();
Index: ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java	(working copy)
@@ -559,7 +559,8 @@
       conf.set("hive.index.compact.file", mWork.getIndexIntermediateFile());
       conf.set("hive.index.blockfilter.file", mWork.getIndexIntermediateFile());
     }
-
+//    conf.setLong("mapreduce.input.fileinputformat.split.maxsize", 10000000);
+//    LOG.info("set mapreduce.input.fileinputformat.split.maxsize = 10000000");
     // Intentionally overwrites anything the user may have put here
     conf.setBoolean("hive.input.format.sorted", mWork.isInputFormatSorted());
   }
Index: ql/src/java/org/apache/hadoop/hive/ql/Context.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/Context.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/Context.java	(working copy)
@@ -26,6 +26,7 @@
 import java.util.ArrayList;
 import java.util.Date;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
@@ -97,6 +98,8 @@
 
   private boolean needLockMgr;
 
+  private final HashSet<String> createdJoinInputDirs = new HashSet<String>();
+
   // Keep track of the mapping from load table desc to the output and the lock
   private final Map<LoadTableDesc, WriteEntity> loadTableOutputMap =
       new HashMap<LoadTableDesc, WriteEntity>();
@@ -198,6 +201,7 @@
     if (dir == null) {
       Path dirPath = new Path(scheme, authority,
           scratchDir + "-" + TaskRunner.getTaskRunnerID());
+      LOG.info(scheme + " * " + authority + " * " + scratchDir + " * " + TaskRunner.getTaskRunnerID());
       if (mkdir) {
         try {
           FileSystem fs = dirPath.getFileSystem(conf);
@@ -217,6 +221,7 @@
         }
       }
       dir = dirPath.toString();
+      LOG.info(dir);
       fsScratchDirs.put(fileSystem + "-" + TaskRunner.getTaskRunnerID(), dir);
 
     }
@@ -364,6 +369,42 @@
       nextPathId();
   }
 
+  public String getIncPath(String dirName) throws IOException{
+
+//    String wholeDir = "/hive_inc/"+dirName;
+//    LOG.info("create dir " + "/hive_inc/"+dirName);
+//    if(createdJoinInputDirs.contains(wholeDir)){
+//      return wholeDir;
+//    }else{
+
+      Path incPath = new Path("/hive_inc" , dirName);
+      Path dir = FileUtils.makeQualified(incPath, conf);
+      URI uri = dir.toUri();
+      Path dirPath = new Path(uri.getScheme(), uri.getAuthority(),
+          uri.getPath());
+      LOG.info("getIncPath: " + dirPath.toString());
+      if (!explain) {
+        try {
+          FileSystem fs = dirPath.getFileSystem(conf);
+          dirPath = new Path(fs.makeQualified(dirPath).toString());
+          if (!fs.mkdirs(dirPath)) {
+            throw new RuntimeException("Cannot make directory: "
+                + dirPath.toString());
+          } else {
+            FsPermission fsPermission = new FsPermission(Short.parseShort(scratchDirPermission.trim(), 8));
+            fs.setPermission(dirPath, fsPermission);
+          }
+
+        } catch (IOException e) {
+          throw new RuntimeException (e);
+        }
+      }
+      createdJoinInputDirs.add(dirPath.toString());
+      return dirPath.toString();
+
+//    }
+  }
+
   /**
    * @return the resFile
    */
@@ -648,4 +689,8 @@
   public void setTryCount(int tryCount) {
     this.tryCount = tryCount;
   }
+
+  public HashSet<String> getJoinInputCache() {
+    return createdJoinInputDirs;
+  }
 }
Index: ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java	(revision 116)
+++ ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java	(working copy)
@@ -397,7 +397,10 @@
     // Processing directories
     List<InputSplitShim> iss = new ArrayList<InputSplitShim>();
     if (!mrwork.isMapperCannotSpanPartns()) {
+//      combine.setMaxSplitSize(250000000);
+//      LOG.info("setMaxSplitSize = 250000000" );
       iss = Arrays.asList(combine.getSplits(job, 1));
+      LOG.info("get split from " +combine.toString() + "; iss size is: " + iss.size());
     } else {
       for (Path path : inpDirs) {
         processPaths(job, combine, iss, path);
@@ -417,10 +420,11 @@
     }
 
     for (InputSplitShim is : iss) {
+      LOG.info("is path is: " + is.toString());
       CombineHiveInputSplit csplit = new CombineHiveInputSplit(job, is);
       result.add(csplit);
     }
-
+    LOG.info("input paremeter numSplits is " + numSplits);
     LOG.info("number of splits " + result.size());
     perfLogger.PerfLogEnd(LOG, PerfLogger.GET_SPLITS);
     return result.toArray(new CombineHiveInputSplit[result.size()]);
@@ -430,7 +434,9 @@
       List<InputSplitShim> iss, Path... path) throws IOException {
     JobConf currJob = new JobConf(job);
     FileInputFormat.setInputPaths(currJob, path);
+    LOG.info("path is " + path.toString());
     iss.addAll(Arrays.asList(combine.getSplits(currJob, 1)));
+    LOG.info("iss size is " + iss.size());
   }
 
   /**
