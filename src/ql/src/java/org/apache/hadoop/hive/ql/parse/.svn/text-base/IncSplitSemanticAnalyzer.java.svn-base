package org.apache.hadoop.hive.ql.parse;

import java.io.Serializable;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedHashMap;
import java.util.LinkedHashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.antlr.runtime.CommonToken;
import org.antlr.runtime.tree.Tree;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hadoop.hive.ql.CommandNeedRetryException;
import org.apache.hadoop.hive.ql.Driver;
import org.apache.hadoop.hive.ql.ErrorMsg;
import org.apache.hadoop.hive.ql.IncDriver.incCtx;
import org.apache.hadoop.hive.ql.QueryProperties;
import org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator;
import org.apache.hadoop.hive.ql.exec.ColumnInfo;
import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
import org.apache.hadoop.hive.ql.exec.GroupByOperator;
import org.apache.hadoop.hive.ql.exec.JoinOperator;
import org.apache.hadoop.hive.ql.exec.Operator;
import org.apache.hadoop.hive.ql.exec.OperatorFactory;
import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
import org.apache.hadoop.hive.ql.exec.RowSchema;
import org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator;
import org.apache.hadoop.hive.ql.exec.TableScanOperator;
import org.apache.hadoop.hive.ql.exec.Task;
import org.apache.hadoop.hive.ql.exec.Utilities;
import org.apache.hadoop.hive.ql.hooks.ReadEntity;
import org.apache.hadoop.hive.ql.hooks.WriteEntity;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.metadata.Partition;
import org.apache.hadoop.hive.ql.metadata.Table;
import org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext;
import org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec;
import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
import org.apache.hadoop.hive.ql.plan.FilterDesc.sampleDesc;
import org.apache.hadoop.hive.ql.plan.GroupByDesc;
import org.apache.hadoop.hive.ql.plan.HiveOperation;
import org.apache.hadoop.hive.ql.plan.LoadFileDesc;
import org.apache.hadoop.hive.ql.plan.LoadTableDesc;
import org.apache.hadoop.hive.ql.plan.MapJoinDesc;
import org.apache.hadoop.hive.ql.plan.OperatorDesc;
import org.apache.hadoop.hive.ql.plan.PlanUtils;
import org.apache.hadoop.hive.ql.session.SessionState;
import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;

public class IncSplitSemanticAnalyzer extends SemanticAnalyzer {
  private static final Log LOG = LogFactory
      .getLog(IncSplitSemanticAnalyzer.class);

  //For incremental optimization
  private final ASTNode originalTree;
  private Map<String,String> colNameToType;
  private final incCtx incCtx ;
  private String crtCacheQL;
  private ParseContext cacheTbPctx;
  // use for finding cachetable column;s type while processing TOK_SELECT ,it's got by genplan:genExprNodeDesc()
  private final Map<ASTNode, ExprNodeDesc> allExprs= new HashMap<ASTNode, ExprNodeDesc>();;
  // use for storing cachetable's column info
  //private final ArrayList<CacheColumn> collist ;
  //the TOK_SELECT to extract cache table,use to identify sub query
  //private ASTNode tok_select ;
  //the TOK_FROM to extract cache table,use to identify sub query
  //private ASTNode tok_from;
  //use to identify sub query
  //private ASTNode tok_query;

  private final Map<ASTNode,SimpleQB> splitSQBs = new HashMap<ASTNode,SimpleQB>();//TOK_QUERY -> SimpleQB
  private final ArrayList<SimpleQB> splitJoins = new ArrayList<SimpleQB>();

  private int gbySplitCount = 0;
  //private final int colcount = 0;
  //private final Map<ASTNode,CacheColumn> ASTtoCol ;
  //ASTNode:TOK_SELEXPR's child:FUNCTION ,use to store the ASTNode generated by the rules
  //ArrayList's size should be 2,one form incPhase1's function ,one from incPhase2's function
  //for example:on count(col) ,
  //  funcmap.get(key).get(0) should give "count" function
  //  funcmap.get(key).get(1) should give "sum" function
  //private final Map<ASTNode,ArrayList<ExprFunction>> funcmap ;
  //inc optimization cache table name
  //TODO :generate automatically
  //private final String cachetable = "inc_cache_q3";
  //store the two QBs,one for inerst cache table ,one for select final result from cache table
  private final ArrayList<QB> gbyCacheQBlist = new ArrayList<QB>();;
  private final ArrayList<QB> joinCacheQBlist = new ArrayList<QB>();
  private QB finalQB = null;
  private int joinInputsCount = 0;
  private int gbyInputsCount=0;
  private final ArrayList<String> scanAllIncAlias = new ArrayList<String>();//to store the inc table which should be scaned wholely, is the join source
  /*
   * Record distinct colName in "convertBasedonRules",
   * add the col as TOK_GROUPBY's child in "doPhase1Inc" if not null
   */
  private String distinctColName = null;
  private final Map<QBJoinTree,ASTNode> joinTreeToAST = new HashMap<QBJoinTree,ASTNode>();
  private final Map<ASTNode,Map<String, ExprNodeDesc> > ASTtoColExprMap =  new HashMap<ASTNode,Map<String, ExprNodeDesc> >();

  //class for cache table column info
  public static class CacheColumn {
    //original query's ASTNode
    //it's used for groupby/orderby's column to match the original column
    ASTNode origcolumnAST;
    // cache table 's columnname
    String  columnname;
    //cache table 's column alias
    //it should be the same as the original query column alias
    String alias;
    //cache table 's columnname
    String type;
    // DOT
    //   table
    //   col
    // dotdotcol is for col
    String dotcol;
    //whether origin colunm is a function(TOK_SELEXPR's child is FUNCTION)
    boolean isfunction = false;
    //the colunm is a function,but it's not the first column generated by the rules
    //for example, expr: avg(col) generate two column1: sum(col1) and column2 : count(col2)
    //    and column2 is a redundant column
    boolean isredundant = false;
  }

  //class for ASTNode:FUNCTION generated by the rules
  //since it will generate more functions, so we create it as a LinkList with next pointer
  public static class ExprFunction{
    //functionname .such as "sum" "avg"
    String fname ;
    //it's generated by the rules,  use to create a TOK_SELEXPR in the future
    ASTNode FuncAST;
    //function return type
    String type = "";
    // which phase(incphase1,incphase2) to use
    int phase;
    //since it will generate more functions, so we create it as a LinkList with next pointer
    ExprFunction next = null;
  }

//class for ASTNode:FUNCTION generated by the rules
  //since it will generate more functions, so we create it as a LinkList with next pointer
  public static class SimpleQB{
    //functionname .such as "sum" "avg"
    String subqalias ;
    //it's generated by the rules,  use to create a TOK_SELEXPR in the future
    ASTNode TOK_Select = null;
    ASTNode TOK_Query = null;
    ASTNode TOK_From = null;
    //function return type
    boolean groupby = false;
    // which phase(incphase1,incphase2) to use
    boolean orderby = false;
    //since it will generate more functions, so we create it as a LinkList with next pointer
    SimpleQB subqb = null;
    //boolean haveInctoken = false;
    boolean distinct = false;

    private String cachetable = null;
    private final ArrayList<CacheColumn> collist = new ArrayList<CacheColumn>();
    private final Map<ASTNode,CacheColumn> ASTtoCol = new HashMap<ASTNode,CacheColumn>();
    //ASTNode:TOK_SELEXPR's child:FUNCTION ,use to store the ASTNode generated by the rules
    //ArrayList's size should be 2,one form incPhase1's function ,one from incPhase2's function
    //for example:on count(col) ,
    //  funcmap.get(key).get(0) should give "count" function
    //  funcmap.get(key).get(1) should give "sum" function
    private final Map<ASTNode,ArrayList<ExprFunction>> funcmap = new HashMap<ASTNode,ArrayList<ExprFunction>>();
    private int colcount = 0;
    private Phase1Ctx ctx = null;
    private QB qb = null;
    private final String cacheTable = null;
    public boolean aggregation = false;
    public String id = null;
    private final boolean isJoinCache = false;
    public boolean mustOptimize(){
      //if(haveInctoken == true && (this.groupby == true || this.orderby == true || this.distinct == true /*|| this.aggregation == true*/)){
      if(this.groupby == true || this.orderby == true || this.distinct == true || this.aggregation == true){
        return true;
      }
      return false;
    }
  }

  public IncSplitSemanticAnalyzer(HiveConf conf, ASTNode tree, incCtx ctx) throws SemanticException {
    super(conf);
    originalTree = tree;
    incCtx = ctx;
  }

  public IncSplitSemanticAnalyzer(HiveConf conf) throws SemanticException {
    super(conf);
    originalTree = null;
    incCtx = null;
  }

  private MultiParseContext mergePctx( HashMap<Integer, ParseContext>  multiPctx ) {
    MultiParseContext multipCtx=new MultiParseContext(
        new HiveConf(),//conf
        new QB() ,//qb
        new ASTNode() ,//ast
        new HashMap<TableScanOperator, ExprNodeDesc>(),//opToPartPruner
        new HashMap<TableScanOperator, PrunedPartitionList>(),//opToPartList
        new HashMap<String, Operator<? extends OperatorDesc>>(),//topOps
        new HashMap<String, Operator<? extends OperatorDesc>>(),//topSelOps
        new LinkedHashMap<Operator<? extends OperatorDesc>, OpParseContext>(),//opParseCtx
        new HashMap<JoinOperator, QBJoinTree>(),//joinContext
        new HashMap<SMBMapJoinOperator, QBJoinTree>(),//smbMapJoinContext
        new HashMap<TableScanOperator, Table>(),//topToTable
        new HashMap<TableScanOperator, Map<String, String>>(),//topToProps
        new HashMap<FileSinkOperator, Table>() ,//fsopToTable
        new ArrayList<LoadTableDesc>(),//loadTableWork
        new ArrayList<LoadFileDesc>() ,//loadFileWork
        null,//ctx
        new HashMap<String, String>() ,//idToTableNameMap
        0,//destTableId
        new UnionProcContext(),//uCtx
        new ArrayList<AbstractMapJoinOperator<? extends MapJoinDesc>>(),//listMapJoinOpsNoReducer
        new HashMap<GroupByOperator, Set<String>>(),//groupOpToInputTables
        new HashMap<String, PrunedPartitionList>() ,//prunedPartitions
        new HashMap<TableScanOperator, sampleDesc>() ,//opToSamplePruner
        new GlobalLimitCtx() ,//globalLimitCtx
        new HashMap<String, SplitSample>() ,//nameToSplitSample
        new HashSet<ReadEntity>(),//semanticInputs
        new ArrayList<Task<? extends Serializable>>() ,//rootTasks
        new HashMap<TableScanOperator, Map<String, ExprNodeDesc>>(),//opToPartToSkewedPruner
        new HashMap<String, ReadEntity>(),//viewAliasToInput
        new ArrayList<ReduceSinkOperator>(),//reduceSinkOperatorsAddedByEnforceBucketingSorting
        new QueryProperties(),//queryProperties
        multiPctx   //multipctx
        );

    for(int i=0;i<multiPctx.size();i++){
      ParseContext pCtx=multiPctx.get(i) ;
      int queryid=i;

      //1 conf
      // set the first query conf to the multipctx.hiveconf
      if(i==multiPctx.size()-1){
        multipCtx.setConf(pCtx.getConf());
      }

      //2 qb
      // it's useless for qb field
      if(i==0){
        multipCtx.setQB(pCtx.getQB());
      }

      //3 ast
      // TODO

      //4 opToPartPruner
      multipCtx.getOpToPartPruner().putAll(pCtx.getOpToPartPruner());

      //5 opToPartList
      multipCtx.getOpToPartList().putAll(pCtx.getOpToPartList());

      //6  topOps
      // add "query id" string in front of the hashmap key string
      Iterator iter=pCtx.getTopOps().entrySet().iterator();
      while(iter.hasNext()){
        Map.Entry<String, Operator<? extends OperatorDesc>> entry = (Map.Entry<String, Operator<? extends OperatorDesc>> )iter.next();
        Object key=entry.getKey();
        Operator<? extends OperatorDesc> value=entry.getValue();
        multipCtx.getTopOps().put("query"+queryid+":"+key,value);
      }

      //7 topSelOps
      iter=pCtx.getTopSelOps().entrySet().iterator();
      while(iter.hasNext()){
        Map.Entry<String, Operator<? extends OperatorDesc>> entry = (Map.Entry<String, Operator<? extends OperatorDesc>> )iter.next();
        Object key=entry.getKey();
        Operator<? extends OperatorDesc> value=entry.getValue();
        multipCtx.getTopSelOps().put("query"+queryid+":"+key,value);
      }

      //8 opParseCtx
      //private LinkedHashMap<Operator<? extends OperatorDesc>, OpParseContext> opParseCtx;
      multipCtx.getOpParseCtx().putAll(pCtx.getOpParseCtx());

      //9 joinContext
      multipCtx.getJoinContext().putAll(pCtx.getJoinContext());

      // TODO
      //10 smbMapJoinContext
      //multipCtx.getSmbMapJoinContext().putAll(pCtx.getSmbMapJoinContext());

      //11  topToTable
      multipCtx.getTopToTable().putAll(pCtx.getTopToTable());

      //12 topToProps
      multipCtx.getTopToProps().putAll(pCtx.getTopToProps());


      //13 fsopToTable
      multipCtx.getFsopToTable().putAll(pCtx.getFsopToTable());
      //14 loadTableWork
      //15 loadFileWork

      //16 ctx
      if(i==multiPctx.size()-1){
      multipCtx.setContext(pCtx.getContext());
      }

      //TODO
      //17 idToTableNameMap
      //18 destTableId
      //19 uCtx
      //20 listMapJoinOpsNoReducer
      //21 groupOpToInputTables

      //22 prunedPartitions
      iter=pCtx.getPrunedPartitions().entrySet().iterator();
      while(iter.hasNext()){
        Map.Entry<String, PrunedPartitionList> entry = (Map.Entry<String, PrunedPartitionList> )iter.next();
        Object key=entry.getKey();
        PrunedPartitionList value=entry.getValue();
        multipCtx.getPrunedPartitions().put("query"+queryid+":"+key,value);
      }

      //23 opToSamplePruner
      multipCtx.getOpToSamplePruner().putAll(pCtx.getOpToSamplePruner());

      //24 globalLimitCtx
      // set globallimitctx disable
      GlobalLimitCtx globalLimitCtx =new GlobalLimitCtx();
      globalLimitCtx.disableOpt();
      multipCtx.setGlobalLimitCtx(globalLimitCtx);

      //TODO
      //25 nameToSplitSample
      //26 semanticInputs
      //27 rootTasks

      //28 opToPartToSkewedPruner
      multipCtx.getOpToPartToSkewedPruner().putAll(pCtx.getOpToPartToSkewedPruner());

      //29 viewAliasToInput
      iter=pCtx.getViewAliasToInput().entrySet().iterator();
      while(iter.hasNext()){
        Map.Entry<String, ReadEntity> entry = (Map.Entry<String, ReadEntity> )iter.next();
        Object key=entry.getKey();
        ReadEntity value=entry.getValue();
        multipCtx.getViewAliasToInput().put("query"+queryid+":"+key,value);
      }

      //30 reduceSinkOperatorsAddedByEnforceBucketingSorting
      multipCtx.getReduceSinkOperatorsAddedByEnforceBucketingSorting().addAll(multipCtx.getReduceSinkOperatorsAddedByEnforceBucketingSorting());

      //31 queryProperties

    }
    return multipCtx;

  }


   /**
   * generate QL of creating cache table ,and run it by  a driver
   * 1 doPreAnalysis          :get the  ExprNodeDesc of each expression,store it in allExprs
   * 2 analyzeCachePosition   :analyze the cache table position,return a TOK_SELECT
   * 3 analyzeTokenSelect     :analyze the TOK_SELECT,generate collist to store cache table's columns
   * 4 genCrtCacheQuery       :use collist to generate the String
   * 5 run(QL)                :run drop/create cache table
   */
  public void genAndrunCrtCacheQuery() throws SemanticException {
    ASTNode tree = this.originalTree;

    //doPreAnalysis to get the type of the cache table
    //get the  ExprNodeDesc of each expression in the query
    //use allExprs to store the info
    doPreAnalysis(tree);

    //analyze the cache table position,
    //TODO analyze the sub query method
    SimpleQB sqb = new SimpleQB();
    int state = analyzeCachePosition(tree,sqb,11,null);
    if(00==state||01==state){
      sqb.cachetable = "inc_gby_cache" + this.gbySplitCount;
      sqb.id = null;
      splitSQBs.put(sqb.TOK_Query,sqb);
      this.gbySplitCount++;
      LOG.info("Find GBY split point for outest QB");
    }

    LOG.info("Find " + gbySplitCount + " qbs with incremental table + gby/orderby/distinct." + "They are : " );
    for(Map.Entry<ASTNode,SimpleQB> entry:splitSQBs.entrySet()){
      LOG.info(entry.getKey().dump());
    }

    //analyze TOK_SELECT to get the CrtCacheTable QL
    //fill the collist which stores the cache table's column info
    for(SimpleQB cursqb:splitSQBs.values()){
      if(cursqb.TOK_Select == null && cursqb.TOK_Select.getToken().getType() != HiveParser.TOK_SELECT){
        throw new SemanticException("ERROR:TOKEN is not a TOK_SELECT,we can't extract the cache table!");
      }
      analyzeTokenSelect(cursqb);
      creatCacheTable(cursqb);
    }
    for(SimpleQB cursqb:splitJoins){
      creatCacheTable(cursqb);
    }

  }


  private void creatCacheTable(SimpleQB cursqb) throws SemanticException {

  //generate the CrtCacheQuery based on the collist
    String tableName =  cursqb.cachetable;
    StringBuilder CrtCacheQueryBuilder = new StringBuilder("create table "+tableName+" ( ");
    String CrtcacheQuery;
    for (int i = 0; i < cursqb.collist.size(); i++) {
      if (i > 0) {
        CrtCacheQueryBuilder.append(" , ");
      }
      CrtCacheQueryBuilder.append(cursqb.collist.get(i).columnname);
      CrtCacheQueryBuilder.append(" ");
      CrtCacheQueryBuilder.append(cursqb.collist.get(i).type);

    }
    CrtCacheQueryBuilder.append(")");
    CrtcacheQuery = CrtCacheQueryBuilder.toString();

    String DropcacheQuery = "Drop table if exists "+tableName ;
    LOG.info(DropcacheQuery);
    LOG.info(CrtcacheQuery);

    //create a Driver to run the CrtcacheQuery String
    int ret = 0;
    Driver qp = new Driver();
    try {
      ret = qp.run(DropcacheQuery).getResponseCode();
      ret = qp.run(CrtcacheQuery).getResponseCode();

    } catch (CommandNeedRetryException e) {
      e.printStackTrace();
    }
    if (ret != 0) {
      throw new SemanticException("ERROR: can't create cache table!");
    }
  }


  /**
   * analyze TOK_SELECT to get the CrtCacheTable QL
   * use collist to store the column info
   *
   * TOK_SELECT
   *    TOK_SELEXPR
   *    ......
   *    TOK_SELEXPR
   *
   *TOK_SELEXPR
   *   EXPRESSION
   *   ALIAS
   *
   * EXPRESSION's type :
   *    TOK_TABLE_OR_COL
   *    FUNCTION
   *    DOT
   *
   * TODO: complex expression
   * @throws SemanticException
   */
  private void analyzeTokenSelect(SimpleQB sqb) throws SemanticException {

    int child_count = sqb.TOK_Select.getChildCount();
    for(int child_pos = 0;child_pos < child_count ;++child_pos){
      ASTNode tok_selexpr = (ASTNode) sqb.TOK_Select.getChild(child_pos);
      if(tok_selexpr.getToken().getType() == HiveParser.TOK_SELEXPR){
        //EXPRESSION:tok_selexpr.getChild(0)
        //ALIAS     :tok_selexpr.getChild(1)
        ASTNode selexprchild =  (ASTNode) tok_selexpr.getChild(0);
        String alias = "";
        if(tok_selexpr.getChildCount()==2){
          alias = ((ASTNode) tok_selexpr.getChild(1)).getToken().getText();
        }
        analyzeTokenSelectExpr(sqb,selexprchild,alias);

      }

    }

  }

  private void analyzeTokenSelectExpr(SimpleQB sqb,ASTNode selexprchild,String alias ) throws SemanticException{

    ExprNodeDesc exprnode = allExprs.get(selexprchild);
    if(exprnode == null){
      throw new SemanticException("ERROR:we should not reach here, the ASTNode is "+ selexprchild.dump());
    }


    //analyze EXPRESSION's three type :TOK_TABLE_OR_COL ,FUNCTION,DOT
    if(selexprchild.getToken().getType() == HiveParser.DIVIDE
        ||selexprchild.getToken().getType() == HiveParser.STAR
        ||selexprchild.getToken().getType() == HiveParser.MINUS
        ||selexprchild.getToken().getType() == HiveParser.PLUS){

      analyzeTokenSelectExpr(sqb,(ASTNode)selexprchild.getChild(0),"") ;
      analyzeTokenSelectExpr(sqb,(ASTNode)selexprchild.getChild(1),"") ;

    }else if(selexprchild.getToken().getType() == HiveParser.TOK_TABLE_OR_COL){
      ASTNode taborcolname =  (ASTNode) selexprchild.getChild(0);

      CacheColumn col = new CacheColumn();
      col.columnname = taborcolname.getToken().getText();
      col.alias = alias;
      col.type = exprnode.getTypeString();
      col.origcolumnAST = selexprchild;
      sqb.collist.add(col);
      sqb.ASTtoCol.put(selexprchild, col);

    }else if(selexprchild.getToken().getType() == HiveParser.TOK_FUNCTION
        ||selexprchild.getToken().getType() == HiveParser.TOK_FUNCTIONDI/*kangynli added*/){
      ASTNode funcnameast =  (ASTNode) selexprchild.getChild(0);
      String funcname = funcnameast.getToken().getText();
      int tmp = sqb.colcount;
      /**
       * Based on the Rules, generate the function's ASTNode for IncPhase1 and IncPhase2
       * use ArrayList<ExprFunction> funclist to store the result of  ASTNode.
       * use  funclist(0):ExprFunction1  to store the function of IncPhase1
       * use  funclist(1):ExprFunction2  to store the function of IncPhase2
       */
      sqb.colcount = convertBasedonRules(sqb,selexprchild,funcname,sqb.colcount);
      ArrayList<ExprFunction> funclist = sqb.funcmap.get(selexprchild);

      ExprFunction expr1 = funclist.get(0);


      for(int i=0;i<sqb.colcount-tmp;i++){
        if(expr1 != null){

          CacheColumn col = new CacheColumn();
          col.columnname = expr1.fname;
          col.alias = alias;
          if(expr1.type.equals("")){
            col.type = exprnode.getTypeString();
          }else{
            col.type = expr1.type;
          }
          col.origcolumnAST = expr1.FuncAST ;
          col.isfunction = true;
          if (i != 0) {
            col.isredundant = true;
          }
          sqb.collist.add(col);
          expr1 = expr1.next;
        } else {
          throw new SemanticException("ERROR:we should not reach here, the ASTNode is "
              + selexprchild.dump());
        }
      }
    }else if(selexprchild.getToken().getType() == HiveParser.DOT){
      String colunm = "col"+ sqb.colcount;
      sqb.colcount++;
      String dotcol = ((ASTNode)selexprchild.getChild(1)).getToken().getText();
      if(exprnode != null){

        CacheColumn col = new CacheColumn();
        col.columnname = colunm;
        col.alias = alias;
        col.dotcol = dotcol;
        col.type = exprnode.getTypeString();
        col.origcolumnAST = selexprchild ;
        sqb.collist.add(col);
        sqb.ASTtoCol.put(selexprchild, col);
      }
    }

  }



  /**
   * get the type of the cache table ,is's be stored in allExprs
   * get the  ExprNodeDesc of each expression in the query
   * add some code to store the ExprNodeDesc of expr  in genplan
   */
  private void doPreAnalysis(ASTNode tree) throws SemanticException {
    SessionState.get().setCommandType(HiveOperation.QUERY);
    // continue analyzing from the child ASTNode.
    Phase1Ctx ctx_1 = initPhase1Ctx();
    QB qb = new QB(null, null, false);
    if (!doPhase1(tree, qb, ctx_1)) {
      // if phase1Result false return
      return ;
    }

    LOG.info("Completed phase 1 of Semantic Analysis");

    getMetaData(qb);
    LOG.info("Completed getting MetaData in Semantic Analysis");

    // Save the result schema derived from the sink operator produced
    // by genPlan. This has the correct column names, which clients
    // such as JDBC would prefer instead of the c0, c1 we'll end
    // up with later.
    genPlan(qb);

    //reset it to clear the variables
    reset();

  }

  private int analyzeCachePosition(ASTNode ast, SimpleQB sqb, int inputstate, String outerAlias) throws SemanticException {
    // find the cache table position in the all query trees
    int state = inputstate;
    boolean skipRecursion = false;
    if (ast.getToken() != null) {
      skipRecursion = true;
      switch (ast.getToken().getType()) {
      case HiveParser.TOK_QUERY:
        sqb.TOK_Query = ast;
        skipRecursion = false;
        break;
      case HiveParser.TOK_FUNCTIONDI:
        sqb.distinct = true;
        break;
      case HiveParser.TOK_SELECTDI:
        sqb.distinct = true;
      case HiveParser.TOK_SELECT:
        sqb.TOK_Select = ast;
        skipRecursion = true;
        for (int i = 0; i < ast.getChildCount(); ++i) {
          ASTNode selExpr = (ASTNode) ast.getChild(i);
          if (selExpr.getToken().getType() == HiveParser.TOK_SELEXPR && hasAggregation((ASTNode) selExpr.getChild(0))) {
            sqb.aggregation  = true;
          }
        }
        break;

      case HiveParser.TOK_WHERE:
        break;

      case HiveParser.TOK_INSERT_INTO:

      case HiveParser.TOK_DESTINATION:
        skipRecursion = true;
        break;

      case HiveParser.TOK_FROM:
        sqb.TOK_From = ast;
        ASTNode frm = (ASTNode) ast.getChild(0);
        state = analyzeFromCachePosition(frm,sqb,state,outerAlias);
        skipRecursion = true;
        break;

      case HiveParser.TOK_CLUSTERBY:
        break;

      case HiveParser.TOK_DISTRIBUTEBY:

        break;

      case HiveParser.TOK_SORTBY:
        break;

      case HiveParser.TOK_ORDERBY:
        sqb.orderby = true;

        break;

      case HiveParser.TOK_GROUPBY:
      case HiveParser.TOK_ROLLUP_GROUPBY:
      case HiveParser.TOK_CUBE_GROUPBY:
      case HiveParser.TOK_GROUPING_SETS:
        sqb.groupby = true;

        break;

      case HiveParser.TOK_HAVING:
        break;

      case HiveParser.KW_WINDOW:
        break;

      case HiveParser.TOK_LIMIT:
        break;

      case HiveParser.TOK_ANALYZE:

        break;

      case HiveParser.TOK_UNION:


      case HiveParser.TOK_INSERT:
        skipRecursion = false;

        break;
      case HiveParser.TOK_LATERAL_VIEW:
      case HiveParser.TOK_LATERAL_VIEW_OUTER:

        break;
      default:
        skipRecursion = false;
        break;
      }
    }

    if (!skipRecursion) {
      // Iterate over the rest of the children
      int child_count = ast.getChildCount();
      for (int child_pos = 0; child_pos < child_count ; ++child_pos) {
        // Recurse
        //phase1Result = phase1Result && doPhase1Inc((ASTNode) ast.getChild(child_pos), qb, ctx_1,qb2,ctx_2);
        state = analyzeCachePosition((ASTNode) ast.getChild(child_pos),sqb,state,outerAlias);

      }
    }
    return state;
  }

  private boolean hasAggregation(ASTNode selexprchild) {

    //analyze EXPRESSION's three type :TOK_TABLE_OR_COL ,FUNCTION,DOT
    if(selexprchild.getToken().getType() == HiveParser.DIVIDE
        ||selexprchild.getToken().getType() == HiveParser.STAR
        ||selexprchild.getToken().getType() == HiveParser.MINUS
        ||selexprchild.getToken().getType() == HiveParser.PLUS){

      if(hasAggregation((ASTNode)selexprchild.getChild(0)) || hasAggregation((ASTNode)selexprchild.getChild(1)) ){
        return true;
      }

    }else if(selexprchild.getToken().getType() == HiveParser.TOK_FUNCTION
        ||selexprchild.getToken().getType() == HiveParser.TOK_FUNCTIONDI){
      ASTNode funcnameast =  (ASTNode) selexprchild.getChild(0);
      String funcname = funcnameast.getToken().getText();
      if(funcname.toLowerCase().equals("sum") || funcname.toLowerCase().equals("count") || funcname.toLowerCase().equals("avg")
          || funcname.toLowerCase().equals("min") || funcname.toLowerCase().equals("max")){
        return true;
      }

    }

    return false;
  }

  private int analyzeFromCachePosition(ASTNode frm ,SimpleQB sqb, int inputstate, String outerAlias) throws SemanticException {
    // Check if this is a subquery / lateral view
    int state = inputstate;
    if (frm.getToken().getType() == HiveParser.TOK_TABREF) {
//      for(Node child: frm.getChildren()){
//        if(((ASTNode)child).getToken().getType() == HiveParser.TOK_INCRE){
//          sqb.haveInctoken = true;
//        }
//      }
      if(ASTNodeUtils.isIncTabRef(frm)){
        state = new Integer(01);
      }else{
        state = new Integer(11);
      }
      outerAlias =  QB.getAppendedAliasFromId(outerAlias, ASTNodeUtils.getAliasId(frm));
      LOG.info("Find Cache position | To table " + outerAlias+ "; state is " + state);

    } else if (frm.getToken().getType() == HiveParser.TOK_SUBQUERY) {
      String subQalias = unescapeIdentifier(frm.getChild(1).getText());
      String newOuterAlias = QB.getAppendedAliasFromId(outerAlias,subQalias);
      SimpleQB ssqb = new SimpleQB();
      ssqb.subqalias = subQalias;
      state = analyzeCachePosition((ASTNode)frm.getChild(0),ssqb,state,newOuterAlias);
      LOG.info("Find Cache position | Get subQ "+ subQalias + " state: " + state);
      if(01 == state || 00 ==state){
        if(ssqb.mustOptimize() == true){
          ssqb.cachetable = "inc_gby_cache" + this.gbySplitCount;
          ssqb.id = newOuterAlias;
          splitSQBs.put(ssqb.TOK_Query,ssqb);
          this.gbySplitCount++;
          state = 10;
          LOG.info("Find GBY split point. stop broadcast - split here " + newOuterAlias + "; Change state to " + state);
        }
//        else if(ssqb.haveInctoken){
//          sqb.haveInctoken = true;
//          LOG.info("continue broadcast - don't split here " + newOuterAlias);
//        }
      }
    } else if (frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW ||
        frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW_OUTER) {
      //TODO
    } else if (isJoinToken(frm)) {
      state = analyzeJoinCachePosition(frm,sqb,state,outerAlias);
      LOG.info("Find Cache position | Get Join state: " + state);
    }else if(frm.getToken().getType() == HiveParser.TOK_PTBLFUNCTION){
      //TODO
    }
    return state;

  }

  private int analyzeJoinCachePosition(ASTNode join,SimpleQB sqb, int inputstate, String outerAlias) throws SemanticException {
    int numChildren = join.getChildCount();
    int state = inputstate;
    if((numChildren != 2) && (numChildren != 3)
      && join.getToken().getType() != HiveParser.TOK_UNIQUEJOIN){
      throw new SemanticException(generateErrorMessage(join,
        "analyzeJoinCachePosition don't support Join with more than 2 child"));
    }
    ASTNode child0 = (ASTNode) join.getChild(0),child1 =(ASTNode) join.getChild(1);
    int state0 = analyzeFromCachePosition(child0,sqb,state,outerAlias);
    int state1 = analyzeFromCachePosition(child1,sqb,state,outerAlias);
    LOG.info("Find Cache position | To join; state is " + state0 + " " + state1);
    if(state0 == 10 && state1 == 01){
      storeJoinChild(child1,outerAlias);
      state = 10;
    }else if(state0 == 01 && state1 == 10){
      storeJoinChild(child0,outerAlias);
      state = 10;
    }else if(state0 == 01 && state1 == 01){
      storeJoinChild(child1,outerAlias);
      storeJoinChild(child0,outerAlias);
      state = 11;
    }else if(state0 == 10 && state1 == 10){
      state = 10;
    }else if(state0 == 11){
      state = state1;
    }else if(state1 == 11){
      state = state0;
    }
    return state;
  }

  private void storeJoinChild(ASTNode child,String outerAlias) throws SemanticException {
    if(child.getToken().getType() == HiveParser.TOK_TABREF){
      String tabAlias = ASTNodeUtils.getAliasId(child);
      scanAllIncAlias.add(QB.getAppendedAliasFromId(outerAlias,tabAlias));
      LOG.info("Change to scan whole table " + QB.getAppendedAliasFromId(outerAlias, tabAlias));
    }else if(child.getToken().getType() == HiveParser.TOK_SUBQUERY){
      ASTNode join = (ASTNode) child.getParent();
      String dirName = "joinInputCacheDir"+joinInputsCount;
      joinInputsCount ++;
      join.getStoreChild().put(join.getChildren().indexOf(child),dirName);
      LOG.info("Find join input cache point in AST. input is subQ " + outerAlias);
    }else if(isJoinToken(child)){
      ASTNode join = (ASTNode) child.getParent();
      String dirName = "joinInputCacheDir"+joinInputsCount;
      joinInputsCount ++;
      join.getStoreChild().put(join.getChildren().indexOf(child),dirName);
      LOG.info("Find join input cache point in AST. input is join");
    }
  }

  /**
   * Based on the Rules, generate the function's ASTNode for IncPhase1 and IncPhase2
   * use ArrayList<ExprFunction> funclist to store the result of  ASTNode.
   * use  funclist(0):ExprFunction1  to store the function of IncPhase1
   * use  funclist(1):ExprFunction2  to store the function of IncPhase2
   * then store <funcAST,funclist> in the  funcmap
   * @return colcount, to generate the name of the column
   */
  private int convertBasedonRules(SimpleQB sqb,ASTNode funcAST,String funcname,int colcount) throws SemanticException {

    ArrayList<ExprFunction> ExprList =new ArrayList<ExprFunction>();
    if(funcname.toLowerCase().equals("sum")){
      if(funcAST.getToken().getType() == HiveParser.TOK_FUNCTIONDI){//kangyanli added
        //sum(distinct col)
        //phase1 :select col GroupBy col
        //phase2 :sum(distinct col)
        distinctColName = funcAST.getChild(1).getChild(0).toString();
        colcount++;
        ExprFunction ef1 = new ExprFunction();
        ef1.fname = distinctColName;
        //ef1.type = ;
        ef1.phase = 1;
        ef1.FuncAST = (ASTNode) funcAST.getChild(1);

        ExprFunction ef2 = new ExprFunction();
        ef2.phase = 2;
        ef2.FuncAST = funcAST;
        LOG.info("INC:func count p1 AST:  "+ef1.FuncAST.dump());
        LOG.info("INC:func count p2 AST:  "+ef2.FuncAST.dump());

        ExprList.add(ef1);
        ExprList.add(ef2);
      }else if(funcAST.getToken().getType() == HiveParser.TOK_FUNCTION){
        //sum(col)
        //phase1 :sum(col) as col1
        //phase2 :sum(col1)
        String col = "col" + colcount ;
        colcount++;
        ExprFunction ef1 = new ExprFunction();
        ef1.fname = col;
        //ef1.type = ;
        ef1.phase = 1;
        ef1.FuncAST = funcAST;

        ExprFunction ef2 = new ExprFunction();
        ef2.phase = 2;
        ef2.FuncAST = ASTNodeUtils.newfunctinAST("sum", col);
        LOG.info("INC:func sum p1 AST:  "+ef1.FuncAST.dump());
        LOG.info("INC:func sum p2 AST:  "+ef2.FuncAST.dump());

        ExprList.add(ef1);
        ExprList.add(ef2);
      }


    }else if(funcname.toLowerCase().equals("count")){
      if(funcAST.getToken().getType() == HiveParser.TOK_FUNCTIONDI){//kangyanli added
        //count(distinct col)
        //phase1 :select col GroupBy col
        //phase2 :count(distinct col)
        distinctColName = funcAST.getChild(1).getChild(0).toString();
        colcount++;
        ExprFunction ef1 = new ExprFunction();
        ef1.fname = distinctColName;
        //ef1.type = ;
        ef1.phase = 1;
        ef1.FuncAST = (ASTNode) funcAST.getChild(1);

        ExprFunction ef2 = new ExprFunction();
        ef2.phase = 2;
        ef2.FuncAST = funcAST;
        LOG.info("INC:func count p1 AST:  "+ef1.FuncAST.dump());
        LOG.info("INC:func count p2 AST:  "+ef2.FuncAST.dump());

        ExprList.add(ef1);
        ExprList.add(ef2);
      }else if(funcAST.getToken().getType() == HiveParser.TOK_FUNCTION){
        //count(col)
        //phase1 :count(col) as col1
        //phase2 :sum(col1)
        String col = "col" + colcount ;
        colcount++;
        ExprFunction ef1 = new ExprFunction();
        ef1.fname = col;
        //ef1.type = ;
        ef1.phase = 1;
        ef1.FuncAST = funcAST;

        ExprFunction ef2 = new ExprFunction();
        ef2.phase = 2;
        ef2.FuncAST = ASTNodeUtils.newfunctinAST("sum", col);
        LOG.info("INC:func count p1 AST:  "+ef1.FuncAST.dump());
        LOG.info("INC:func count p2 AST:  "+ef2.FuncAST.dump());

        ExprList.add(ef1);
        ExprList.add(ef2);
      }

    }else if(funcname.toLowerCase().equals("min")){
      //min(col) = min(col) as col1 + min(col1)
      String col = "col" + colcount ;
      colcount++;
      ExprFunction ef1 = new ExprFunction();
      ef1.fname = col;
      //ef1.type = ;
      ef1.phase = 1;
      ef1.FuncAST = funcAST;

      ExprFunction ef2 = new ExprFunction();
      ef2.phase = 2;
      ef2.FuncAST = ASTNodeUtils.newfunctinAST("min", col);
      LOG.info("INC:func min p1 AST:  "+ef1.FuncAST.dump());
      LOG.info("INC:func min p2 AST:  "+ef2.FuncAST.dump());

      ExprList.add(ef1);
      ExprList.add(ef2);

    }else if(funcname.toLowerCase().equals("max")){
      //max(col) = max(col) as col1 + max(col1)
      String col = "col" + colcount ;
      colcount++;
      ExprFunction ef1 = new ExprFunction();
      ef1.fname = col;
      //ef1.type = ;
      ef1.phase = 1;
      ef1.FuncAST = funcAST;

      ExprFunction ef2 = new ExprFunction();
      ef2.phase = 2;
      ef2.FuncAST = ASTNodeUtils.newfunctinAST("max", col);
      LOG.info("INC:func max p1 AST:  "+ef1.FuncAST.dump());
      LOG.info("INC:func max p2 AST:  "+ef2.FuncAST.dump());

      ExprList.add(ef1);
      ExprList.add(ef2);

    }else if(funcname.toLowerCase().equals("avg")){
      //avg(col) = sum(col)as col1,count(col) as col2  + sum(col1) /sum(col2)
      String col1 = "col" + colcount ;
      colcount++;
      ExprFunction ef11 = new ExprFunction();
      ef11.fname = col1;
      //ef1.type = ;
      ef11.phase = 1;
      ef11.FuncAST = ASTNodeUtils.newfunctinAST("sum", (ASTNode)funcAST.getChild(1));

      String col2 = "col" + colcount ;
      colcount++;
      ExprFunction ef12 = new ExprFunction();
      ef12.fname = col2;
      //ef1.type = ;
      ef12.phase = 1;
      ef12.FuncAST = ASTNodeUtils.newfunctinAST("count", (ASTNode)funcAST.getChild(1));

      ef11.next = ef12;

      ExprFunction ef2 = new ExprFunction();
      ef2.phase = 2;
      ef2.FuncAST =ASTNodeUtils.newDividAST(
          ASTNodeUtils.newfunctinAST("sum", ASTNodeUtils.newtoktableorcol(col1)),
          ASTNodeUtils.newfunctinAST("sum",ASTNodeUtils.newtoktableorcol(col2)));
      LOG.info("INC:func count p11 AST:  "+ef11.FuncAST.dump());
      LOG.info("INC:func count p12 AST:  "+ef12.FuncAST.dump());
      LOG.info("INC:func count p2 AST:  "+ef2.FuncAST.dump());

      ExprList.add(ef11);
      ExprList.add(ef2);


    }
    if(ExprList.size()!=2){
      throw new SemanticException("Inc ERROR: should have two expr func,one for insert  cache ,one for select cache");
    }

    sqb.funcmap.put(funcAST, ExprList);
    return colcount;

  }

  @Override
  public ExprNodeDesc genExprNodeDesc(ASTNode expr, RowResolver input,
      TypeCheckCtx tcCtx) throws SemanticException {
    // We recursively create the exprNodeDesc. Base cases: when we encounter
    // a column ref, we convert that into an exprNodeColumnDesc; when we
    // encounter
    // a constant, we convert that into an exprNodeConstantDesc. For others we
    // just
    // build the exprNodeFuncDesc with recursively built children.

    // If the current subExpression is pre-calculated, as in Group-By etc.
    ExprNodeDesc cached = getExprNodeDescCached(expr, input);
    if (cached == null) {
      Map<ASTNode, ExprNodeDesc> allExprs = genAllExprNodeDesc(expr, input, tcCtx);

      //inc opt add to find the type of cache table column
      Iterator iter = allExprs.entrySet().iterator();
      while(iter.hasNext()){
        Map.Entry<ASTNode, ExprNodeDesc>  entry = (Map.Entry<ASTNode, ExprNodeDesc>  )iter.next();
        ASTNode key=entry.getKey();
        ExprNodeDesc value=entry.getValue();
        this.allExprs.put(key,value);
      }

      return allExprs.get(expr);
    }
    //inc opt add to find the type of cache table column
    this.allExprs.put(expr,cached);
    return cached;
  }

  @Override
  protected QBJoinTree genJoinTree(QB qb, ASTNode joinParseTree,
      Map<String, Operator> aliasToOpInfo)
      throws SemanticException {
    QBJoinTree joinTree = new QBJoinTree();
    JoinCond[] condn = new JoinCond[1];

    switch (joinParseTree.getToken().getType()) {
    case HiveParser.TOK_LEFTOUTERJOIN:
      joinTree.setNoOuterJoin(false);
      condn[0] = new JoinCond(0, 1, JoinType.LEFTOUTER);
      break;
    case HiveParser.TOK_RIGHTOUTERJOIN:
      joinTree.setNoOuterJoin(false);
      condn[0] = new JoinCond(0, 1, JoinType.RIGHTOUTER);
      break;
    case HiveParser.TOK_FULLOUTERJOIN:
      joinTree.setNoOuterJoin(false);
      condn[0] = new JoinCond(0, 1, JoinType.FULLOUTER);
      break;
    case HiveParser.TOK_LEFTSEMIJOIN:
      joinTree.setNoSemiJoin(false);
      condn[0] = new JoinCond(0, 1, JoinType.LEFTSEMI);
      break;
    default:
      condn[0] = new JoinCond(0, 1, JoinType.INNER);
      joinTree.setNoOuterJoin(true);
      break;
    }

    joinTree.setJoinCond(condn);

    ASTNode left = (ASTNode) joinParseTree.getChild(0);
    ASTNode right = (ASTNode) joinParseTree.getChild(1);

    if ((left.getToken().getType() == HiveParser.TOK_TABREF)
        || (left.getToken().getType() == HiveParser.TOK_SUBQUERY)
        || (left.getToken().getType() == HiveParser.TOK_PTBLFUNCTION)) {
      String tableName = getUnescapedUnqualifiedTableName((ASTNode) left.getChild(0))
          .toLowerCase();
      String alias = left.getChildCount() == 1 ? tableName
          : unescapeIdentifier(left.getChild(left.getChildCount() - 1)
          .getText().toLowerCase());
      // ptf node form is: ^(TOK_PTBLFUNCTION $name $alias? partitionTableFunctionSource partitioningSpec? expression*)
      // guranteed to have an lias here: check done in processJoin
      alias = (left.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) ?
          unescapeIdentifier(left.getChild(1).getText().toLowerCase()) :
            alias;
      joinTree.setLeftAlias(alias);
      String[] leftAliases = new String[1];
      leftAliases[0] = alias;
      joinTree.setLeftAliases(leftAliases);
      String[] children = new String[2];
      children[0] = alias;
      joinTree.setBaseSrc(children);
      joinTree.setId(qb.getId());
      joinTree.getAliasToOpInfo().put(
          getModifiedAlias(qb, alias), aliasToOpInfo.get(alias));
    } else if (isJoinToken(left)) {
      QBJoinTree leftTree = genJoinTree(qb, left, aliasToOpInfo);
      joinTree.setJoinSrc(leftTree);
      String[] leftChildAliases = leftTree.getLeftAliases();
      String leftAliases[] = new String[leftChildAliases.length + 1];
      for (int i = 0; i < leftChildAliases.length; i++) {
        leftAliases[i] = leftChildAliases[i];
      }
      leftAliases[leftChildAliases.length] = leftTree.getRightAliases()[0];
      joinTree.setLeftAliases(leftAliases);
    } else {
      assert (false);
    }

    if ((right.getToken().getType() == HiveParser.TOK_TABREF)
        || (right.getToken().getType() == HiveParser.TOK_SUBQUERY)
        || (right.getToken().getType() == HiveParser.TOK_PTBLFUNCTION)) {
      String tableName = getUnescapedUnqualifiedTableName((ASTNode) right.getChild(0))
          .toLowerCase();
      String alias = right.getChildCount() == 1 ? tableName
          : unescapeIdentifier(right.getChild(right.getChildCount() - 1)
          .getText().toLowerCase());
      // ptf node form is: ^(TOK_PTBLFUNCTION $name $alias? partitionTableFunctionSource partitioningSpec? expression*)
      // guranteed to have an lias here: check done in processJoin
      alias = (right.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) ?
          unescapeIdentifier(right.getChild(1).getText().toLowerCase()) :
            alias;
      String[] rightAliases = new String[1];
      rightAliases[0] = alias;
      joinTree.setRightAliases(rightAliases);
      String[] children = joinTree.getBaseSrc();
      if (children == null) {
        children = new String[2];
      }
      children[1] = alias;
      joinTree.setBaseSrc(children);
      joinTree.setId(qb.getId());
      joinTree.getAliasToOpInfo().put(
          getModifiedAlias(qb, alias), aliasToOpInfo.get(alias));
      // remember rhs table for semijoin
      if (joinTree.getNoSemiJoin() == false) {
        joinTree.addRHSSemijoin(alias);
      }
    } else {
      assert false;
    }

    ArrayList<ArrayList<ASTNode>> expressions = new ArrayList<ArrayList<ASTNode>>();
    expressions.add(new ArrayList<ASTNode>());
    expressions.add(new ArrayList<ASTNode>());
    joinTree.setExpressions(expressions);

    ArrayList<Boolean> nullsafes = new ArrayList<Boolean>();
    joinTree.setNullSafes(nullsafes);

    ArrayList<ArrayList<ASTNode>> filters = new ArrayList<ArrayList<ASTNode>>();
    filters.add(new ArrayList<ASTNode>());
    filters.add(new ArrayList<ASTNode>());
    joinTree.setFilters(filters);
    joinTree.setFilterMap(new int[2][]);

    ArrayList<ArrayList<ASTNode>> filtersForPushing =
        new ArrayList<ArrayList<ASTNode>>();
    filtersForPushing.add(new ArrayList<ASTNode>());
    filtersForPushing.add(new ArrayList<ASTNode>());
    joinTree.setFiltersForPushing(filtersForPushing);

    ASTNode joinCond = (ASTNode) joinParseTree.getChild(2);
    ArrayList<String> leftSrc = new ArrayList<String>();
    parseJoinCondition(joinTree, joinCond, leftSrc);
    if (leftSrc.size() == 1) {
      joinTree.setLeftAlias(leftSrc.get(0));
    }

    // check the hints to see if the user has specified a map-side join. This
    // will be removed later on, once the cost-based
    // infrastructure is in place
    if (qb.getParseInfo().getHints() != null) {
      List<String> mapSideTables = getMapSideJoinTables(qb);
      List<String> mapAliases = joinTree.getMapAliases();

      for (String mapTbl : mapSideTables) {
        boolean mapTable = false;
        for (String leftAlias : joinTree.getLeftAliases()) {
          if (mapTbl.equalsIgnoreCase(leftAlias)) {
            mapTable = true;
          }
        }
        for (String rightAlias : joinTree.getRightAliases()) {
          if (mapTbl.equalsIgnoreCase(rightAlias)) {
            mapTable = true;
          }
        }

        if (mapTable) {
          if (mapAliases == null) {
            mapAliases = new ArrayList<String>();
          }
          mapAliases.add(mapTbl);
          joinTree.setMapSideJoin(true);
        }
      }

      joinTree.setMapAliases(mapAliases);

      parseStreamTables(joinTree, qb);
    }
    //joinTreeToAST.put(joinTree, joinParseTree);
    LOG.info("In incSplitSem for genJoinTree");
    if(joinParseTree.getStoreChild().size() != 0){
      LOG.info("Find join input cache point for QBJoinTree");
      joinTree.setStoreChild(joinParseTree.getStoreChild());
    }
    return joinTree;
  }

  @Override
  protected Operator genJoinOperator(QB qb, QBJoinTree joinTree,
      Map<String, Operator> map) throws SemanticException {
    QBJoinTree leftChild = joinTree.getJoinSrc();
    Operator joinSrcOp = null;
    if (leftChild != null) {
      LOG.info("Call next genJoinOperator");
      Operator joinOp = genJoinOperator(qb, leftChild, map);
      ArrayList<ASTNode> filter = joinTree.getFiltersForPushing().get(0);
      for (ASTNode cond : filter) {
        joinOp = genFilterPlan(qb, cond, joinOp);
      }

      joinSrcOp = genJoinReduceSinkChild(qb, joinTree, joinOp, null, 0);
    }

    Operator[] srcOps = new Operator[joinTree.getBaseSrc().length];

    HashSet<Integer> omitOpts = null; // set of input to the join that should be
    // omitted by the output
    int pos = 0;
    for (String src : joinTree.getBaseSrc()) {
      if (src != null) {
        Operator srcOp = map.get(src.toLowerCase());

        // for left-semi join, generate an additional selection & group-by
        // operator before ReduceSink
        ArrayList<ASTNode> fields = joinTree.getRHSSemijoinColumns(src);
        if (fields != null) {
          // the RHS table columns should be not be output from the join
          if (omitOpts == null) {
            omitOpts = new HashSet<Integer>();
          }
          omitOpts.add(pos);

          // generate a selection operator for group-by keys only
          srcOp = insertSelectForSemijoin(fields, srcOp);

          // generate a groupby operator (HASH mode) for a map-side partial
          // aggregation for semijoin
          srcOp = genMapGroupByForSemijoin(qb, fields, srcOp,
              GroupByDesc.Mode.HASH);
        }

        // generate a ReduceSink operator for the join
        srcOps[pos] = genJoinReduceSinkChild(qb, joinTree, srcOp, src, pos);
        pos++;
      } else {
        assert pos == 0;
        srcOps[pos++] = null;
      }
    }

    // Type checking and implicit type conversion for join keys
    genJoinOperatorTypeCheck(joinSrcOp, srcOps);

    JoinOperator joinOp = (JoinOperator) genJoinOperatorChildren(joinTree,
      joinSrcOp, srcOps, omitOpts);
    joinContext.put(joinOp, joinTree);

//    ASTNode astJoin = joinTreeToAST.get(joinTree);
//    ASTtoColExprMap.put(astJoin,joinOp.getColumnExprMap());
    LOG.info("In incSplitSem for genJoinOperator");
    for(Map.Entry<Integer,String> entry:joinTree.getStoreChild().entrySet()){
      int storePos = entry.getKey().intValue();
      String dirName = entry.getValue();
      ReduceSinkOperator storeRSOp = (ReduceSinkOperator) joinOp.getParentOperators().get(storePos);
      storeRSOp.setStoreDirName(dirName);
      LOG.info("Find join input cache point in OpTree "  + storeRSOp.toString() + " aliases are " +
          storeRSOp.getInputAlias() + " store Dir is " + dirName);
    }
    return joinOp;
  }

  /**
   * Generate the ReduceSinkOperator for the Group By Query Block
   * (qb.getPartInfo().getXXX(dest)). The new ReduceSinkOperator will be a child
   * of inputOperatorInfo.
   *
   * It will put all Group By keys and the distinct field (if any) in the
   * map-reduce sort key, and all other fields in the map-reduce value.
   *
   * @param numPartitionFields
   *          the number of fields for map-reduce partitioning. This is usually
   *          the number of fields in the Group By keys.
   * @return the new ReduceSinkOperator.
   * @throws SemanticException
   */
  @Override
  @SuppressWarnings("nls")
  protected ReduceSinkOperator genGroupByPlanReduceSinkOperator(QB qb,
      String dest,
      Operator inputOperatorInfo,
      List<ASTNode> grpByExprs,
      int numPartitionFields,
      boolean changeNumPartitionFields,
      int numReducers,
      boolean mapAggrDone,
      boolean groupingSetsPresent) throws SemanticException {

    RowResolver reduceSinkInputRowResolver = opParseCtx.get(inputOperatorInfo)
        .getRowResolver();
    QBParseInfo parseInfo = qb.getParseInfo();
    RowResolver reduceSinkOutputRowResolver = new RowResolver();
    reduceSinkOutputRowResolver.setIsExprResolver(true);
    Map<String, ExprNodeDesc> colExprMap = new HashMap<String, ExprNodeDesc>();
    // Pre-compute group-by keys and store in reduceKeys

    List<String> outputKeyColumnNames = new ArrayList<String>();
    List<String> outputValueColumnNames = new ArrayList<String>();

    ArrayList<ExprNodeDesc> reduceKeys = getReduceKeysForReduceSink(grpByExprs, dest,
        reduceSinkInputRowResolver, reduceSinkOutputRowResolver, outputKeyColumnNames,
        colExprMap);

    // add a key for reduce sink
    if (groupingSetsPresent) {
      // Process grouping set for the reduce sink operator
      processGroupingSetReduceSinkOperator(
          reduceSinkInputRowResolver,
          reduceSinkOutputRowResolver,
          reduceKeys,
          outputKeyColumnNames,
          colExprMap);

      if (changeNumPartitionFields) {
        numPartitionFields++;
      }
    }

    List<List<Integer>> distinctColIndices = getDistinctColIndicesForReduceSink(parseInfo, dest,
        reduceKeys, reduceSinkInputRowResolver, reduceSinkOutputRowResolver, outputKeyColumnNames,
        colExprMap);

    ArrayList<ExprNodeDesc> reduceValues = new ArrayList<ExprNodeDesc>();
    HashMap<String, ASTNode> aggregationTrees = parseInfo
        .getAggregationExprsForClause(dest);

    if (!mapAggrDone) {
      getReduceValuesForReduceSinkNoMapAgg(parseInfo, dest, reduceSinkInputRowResolver,
          reduceSinkOutputRowResolver, outputValueColumnNames, reduceValues, colExprMap);
    } else {
      // Put partial aggregation results in reduceValues
      int inputField = reduceKeys.size();

      for (Map.Entry<String, ASTNode> entry : aggregationTrees.entrySet()) {

        TypeInfo type = reduceSinkInputRowResolver.getColumnInfos().get(
            inputField).getType();
        ExprNodeColumnDesc exprDesc = new ExprNodeColumnDesc(type,
            getColumnInternalName(inputField), "", false);
        reduceValues.add(exprDesc);
        inputField++;
        outputValueColumnNames.add(getColumnInternalName(reduceValues.size() - 1));
        String field = Utilities.ReduceField.VALUE.toString() + "."
            + getColumnInternalName(reduceValues.size() - 1);
        reduceSinkOutputRowResolver.putExpression(entry.getValue(),
            new ColumnInfo(field, type, null, false));
        colExprMap.put(field, exprDesc);
      }
    }

    ReduceSinkOperator rsOp = (ReduceSinkOperator) putOpInsertMap(
        OperatorFactory.getAndMakeChild(
            PlanUtils.getReduceSinkDesc(reduceKeys,
                groupingSetsPresent ? grpByExprs.size() + 1 : grpByExprs.size(),
                reduceValues, distinctColIndices,
                outputKeyColumnNames, outputValueColumnNames, true, -1, numPartitionFields,
                numReducers),
            new RowSchema(reduceSinkOutputRowResolver.getColumnInfos()), inputOperatorInfo),
        reduceSinkOutputRowResolver);
    rsOp.setColumnExprMap(colExprMap);
    LOG.info("In incSplitSem for genGBYRSOperator");
    if(parseInfo.getStoreIncInput()){
      String dirName = "gbyInputCacheDir"+gbyInputsCount;
      gbyInputsCount ++;
      rsOp.setStoreDirName(dirName);
      LOG.info("Store gby input to " + dirName);
    }
    return rsOp;
  }

  /**
   * Merges node to target
   */
  @Override
  protected void mergeJoins(QB qb, QBJoinTree node, QBJoinTree target, int pos) {
    String[] nodeRightAliases = node.getRightAliases();
    String[] trgtRightAliases = target.getRightAliases();
    String[] rightAliases = new String[nodeRightAliases.length
        + trgtRightAliases.length];

    for (int i = 0; i < trgtRightAliases.length; i++) {
      rightAliases[i] = trgtRightAliases[i];
    }
    for (int i = 0; i < nodeRightAliases.length; i++) {
      rightAliases[i + trgtRightAliases.length] = nodeRightAliases[i];
    }
    target.setRightAliases(rightAliases);
    target.getAliasToOpInfo().putAll(node.getAliasToOpInfo());

    String[] nodeBaseSrc = node.getBaseSrc();
    String[] trgtBaseSrc = target.getBaseSrc();
    LOG.info("[rightAliases ] node's are " + Arrays.asList(nodeRightAliases).toString() + " target's are " + Arrays.asList(trgtRightAliases).toString());
    LOG.info("[baseSrc ] node's are " + Arrays.asList(nodeBaseSrc).toString() + " target's are " + Arrays.asList(trgtBaseSrc).toString());
    LOG.info("merge pos is " + pos);
    String[] baseSrc = new String[nodeBaseSrc.length + trgtBaseSrc.length - 1];

    for (int i = 0; i < trgtBaseSrc.length; i++) {
      baseSrc[i] = trgtBaseSrc[i];
    }
    for (int i = 1; i < nodeBaseSrc.length; i++) {
      baseSrc[i + trgtBaseSrc.length - 1] = nodeBaseSrc[i];
    }
    target.setBaseSrc(baseSrc);

    //for incremental store join input
    if(node.getStoreChild().keySet().contains(0)){

      if(trgtBaseSrc[pos] != null){
        incCtx.getParserRes().getScanAllIncAliases().add(trgtBaseSrc[pos]);
        LOG.info("Change join store child; Add to scanAllIncAlias: " + trgtBaseSrc[pos]);
      }else{
        target.getStoreChild().put(pos, node.getStoreChild().get(0));
        LOG.info("Change join store child from " + 0 + " to " + pos );
      }
    }
    for (int i = 1; i < nodeBaseSrc.length; i++) {
      if(node.getStoreChild().keySet().contains(i)){
        target.getStoreChild().put(i + trgtBaseSrc.length - 1,node.getStoreChild().get(i) );
        LOG.info("change join store child from " + i + " to " + new Integer(i + trgtBaseSrc.length - 1));
      }
    }

    ArrayList<ArrayList<ASTNode>> expr = target.getExpressions();
    for (int i = 0; i < nodeRightAliases.length; i++) {
      expr.add(node.getExpressions().get(i + 1));
    }

    ArrayList<Boolean> nns = node.getNullSafes();
    ArrayList<Boolean> tns = target.getNullSafes();
    for (int i = 0; i < tns.size(); i++) {
      tns.set(i, tns.get(i) & nns.get(i)); // any of condition contains non-NS, non-NS
    }

    ArrayList<ArrayList<ASTNode>> filters = target.getFilters();
    for (int i = 0; i < nodeRightAliases.length; i++) {
      filters.add(node.getFilters().get(i + 1));
    }

    if (node.getFilters().get(0).size() != 0) {
      ArrayList<ASTNode> filterPos = filters.get(pos);
      filterPos.addAll(node.getFilters().get(0));
    }

    int[][] nmap = node.getFilterMap();
    int[][] tmap = target.getFilterMap();
    int[][] newmap = new int[tmap.length + nmap.length - 1][];

    for (int[] mapping : nmap) {
      if (mapping != null) {
        for (int i = 0; i < mapping.length; i += 2) {
          if (pos > 0 || mapping[i] > 0) {
            mapping[i] += trgtRightAliases.length;
          }
        }
      }
    }
    if (nmap[0] != null) {
      if (tmap[pos] == null) {
        tmap[pos] = nmap[0];
      } else {
        int[] appended = new int[tmap[pos].length + nmap[0].length];
        System.arraycopy(tmap[pos], 0, appended, 0, tmap[pos].length);
        System.arraycopy(nmap[0], 0, appended, tmap[pos].length, nmap[0].length);
        tmap[pos] = appended;
      }
    }
    System.arraycopy(tmap, 0, newmap, 0, tmap.length);
    System.arraycopy(nmap, 1, newmap, tmap.length, nmap.length - 1);
    target.setFilterMap(newmap);

    ArrayList<ArrayList<ASTNode>> filter = target.getFiltersForPushing();
    for (int i = 0; i < nodeRightAliases.length; i++) {
      filter.add(node.getFiltersForPushing().get(i + 1));
    }

    if (node.getFiltersForPushing().get(0).size() != 0) {
      ArrayList<ASTNode> filterPos = filter.get(pos);
      filterPos.addAll(node.getFiltersForPushing().get(0));
    }

    if (node.getNoOuterJoin() && target.getNoOuterJoin()) {
      target.setNoOuterJoin(true);
    } else {
      target.setNoOuterJoin(false);
    }

    if (node.getNoSemiJoin() && target.getNoSemiJoin()) {
      target.setNoSemiJoin(true);
    } else {
      target.setNoSemiJoin(false);
    }

    target.mergeRHSSemijoin(node);

    JoinCond[] nodeCondns = node.getJoinCond();
    int nodeCondnsSize = nodeCondns.length;
    JoinCond[] targetCondns = target.getJoinCond();
    int targetCondnsSize = targetCondns.length;
    JoinCond[] newCondns = new JoinCond[nodeCondnsSize + targetCondnsSize];
    for (int i = 0; i < targetCondnsSize; i++) {
      newCondns[i] = targetCondns[i];
    }

    for (int i = 0; i < nodeCondnsSize; i++) {
      JoinCond nodeCondn = nodeCondns[i];
      if (nodeCondn.getLeft() == 0) {
        nodeCondn.setLeft(pos);
      } else {
        nodeCondn.setLeft(nodeCondn.getLeft() + targetCondnsSize);
      }
      nodeCondn.setRight(nodeCondn.getRight() + targetCondnsSize);
      newCondns[targetCondnsSize + i] = nodeCondn;
    }

    target.setJoinCond(newCondns);
    if (target.isMapSideJoin()) {
      assert node.isMapSideJoin();
      List<String> mapAliases = target.getMapAliases();
      for (String mapTbl : node.getMapAliases()) {
        if (!mapAliases.contains(mapTbl)) {
          mapAliases.add(mapTbl);
        }
      }
      target.setMapAliases(mapAliases);
    }
  }

  public ArrayList<QB> getGbyCacheQBList(){

    return gbyCacheQBlist;
  }

  /**
   *   Spiting origin query , to generate insert ql and query ql
   *   of cache .then generate optree for each ql,
   * @throws SemanticException
   */
  public  void doIncSplitCompile() throws SemanticException{

    LOG.info("Incremental Query Semantic Analysis DoPhase1 Start!");

    // overwrite dophase1 to split original query into three kinds of QB
    // gbyCacheQB, joinCahceQB, finalQB
    analyzeIncPhase1(this.originalTree);

  }

  /**
   * overwrite dophase1 to split original query into two QB
   * fill the two QBs into the qblist
   * @throws SemanticException
   */
  public  void analyzeIncPhase1(ASTNode ast) throws SemanticException {
    ASTNode child = ast;
    this.ast = ast;
    viewsExpanded = new ArrayList<String>();

    LOG.info("Starting Semantic Analysis");

    // analyze and process the position alias
    processPositionAlias(ast);
    SessionState.get().setCommandType(HiveOperation.QUERY);

    // analyze create view command
    if (ast.getToken().getType() == HiveParser.TOK_CREATEVIEW ||
        ast.getToken().getType() == HiveParser.TOK_ALTERVIEW_AS) {
      child = analyzeCreateView(ast, qb);
      SessionState.get().setCommandType(HiveOperation.CREATEVIEW);
      if (child == null) {
        return ;
      }
      viewSelect = child;
      // prevent view from referencing itself
      viewsExpanded.add(SessionState.get().getCurrentDatabase() + "." + createVwDesc.getViewName());
    }
    //ArrayList<Phase1Ctx> ctxs = new ArrayList<Phase1Ctx>();
    //ArrayList<QB> qbs = new ArrayList<QB>();
    for(Map.Entry<ASTNode,SimpleQB> entry: splitSQBs.entrySet()){
      Phase1Ctx ctx_1 = initPhase1Ctx();
      ctx_1.isinc = true;
      QB qb1 = new QB(null, null, false);
      entry.getValue().ctx = ctx_1;
      entry.getValue().qb = qb1;

    }

    //create two QB
    //qb1 is IncPhase1's QB
    //qb2 is IncPhase2's QB
    finalQB = new QB(null, null, false);
    Phase1Ctx ctx_final = initPhase1Ctx();
    if(HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVEINCSPLITGBY)){
      //generate the two QBs
      if (!doPhase1Inc(child,null,null, finalQB, ctx_final,null)) {
        // if phase1Result false return
        return ;
      }
      for(Map.Entry<ASTNode,SimpleQB> entry: splitSQBs.entrySet()){
        gbyCacheQBlist.add(entry.getValue().qb);
      }
    }else{
      if (!doPhase1IncWithoutSplitGBY(child,finalQB, ctx_final)) {
        // if phase1Result false return
        return ;
      }
    }
    LOG.info("GbyCacheQBlist.size is " + gbyCacheQBlist.size() + "; They are: ");
//    for(QB cur:gbyCacheQBlist){
//      cur.print("");
//      LOG.info("=========");
//    }
    //LOG.info("JoinCacheQBlist.size is " + joinCacheQBlist.size() + "; Ther are: ");
//    for(QB cur:joinCacheQBlist){
//      cur.print("");
//      LOG.info("=========");
//    }
//    LOG.info("Final QB is: " );
//    finalQB.print("");
    /*qblist.add(qb1);
    qblist.add(qb2);*/
  }

  /**
   * generate two QBs
   * one for insert cache table
   * one for select final result from cache table
   * @param ast
   * @param QB1 insert operator
   * @param QB2 query operator
   * @return
   */

  @Override
  public boolean doPhase1Inc(ASTNode ast, QB qb, Phase1Ctx ctx_1, QB qb2, Phase1Ctx ctx_2,SimpleQB curSQB)
      throws SemanticException {

    boolean phase1Result = true;
    QBParseInfo qbp = null;
    if(qb != null) {
      qbp = qb.getParseInfo();
    }
    QBParseInfo qbp2 = qb2.getParseInfo();
    boolean skipRecursion = false;
    //Phase1Ctx ctx_1 = new Phase1Ctx();
    //SimpleQB curSQB = null;
    //QB qb = null;

    if (ast.getToken() != null) {
      skipRecursion = true;
      switch (ast.getToken().getType()) {
      case HiveParser.TOK_QUERY:
        skipRecursion = false;
        // inc wanglei
        /*if (ast.equals(this.tok_query)) {
          ctx_1.isinc = true;
        } else {
          ctx_1.isinc = false;
        }*/
        if(splitSQBs.containsKey(ast)){
          LOG.info("Find inc TOK_QUERY: ");
          LOG.info(ast.dump());
          curSQB = splitSQBs.get(ast);
          ctx_1 = curSQB.ctx;
          qb = curSQB.qb;
          qbp = qb.getParseInfo();
        }else{
          ctx_1 = null;
          //ctx_1.isinc = false;
        }

        break;

      case HiveParser.TOK_SELECTDI:
        if(ctx_1!=null){
	  qb.countSelDi();
	}else{
          qb2.countSelDi();
	}
        // fall through
      case HiveParser.TOK_SELECT:
        if (ctx_1 != null && curSQB.TOK_Select.equals(ast)) {
          //kangyanli added begin
          ASTNode tableselect1 = null,tableselect2 = null;
          if(ast.getToken().getType() == HiveParser.TOK_SELECT){
            tableselect1 = new ASTNode(new CommonToken(HiveParser.TOK_SELECT, "TOK_SELECT"));
            tableselect2 = new ASTNode(new CommonToken(HiveParser.TOK_SELECT, "TOK_SELECT"));
          }else if(ast.getToken().getType() == HiveParser.TOK_SELECTDI){
            tableselect1 = new ASTNode(new CommonToken(HiveParser.TOK_SELECTDI, "TOK_SELECTDI"));
            tableselect2 = new ASTNode(new CommonToken(HiveParser.TOK_SELECTDI, "TOK_SELECTDI"));
          }
          //kangyanli added end

          ASTNode tokselexpr1 = null;
          ASTNode tokselexpr2 = null;
          // ArrayList<ExprFunction> exprlist = funcmap.get(ast);

          for (int i = 0 ; i < curSQB.collist.size(); i++) {
            ASTNode origintokselexpr = (ASTNode) ast.getChild(i);
            ASTNode child = curSQB.collist.get(i).origcolumnAST;


            ASTNode tableselexpr = new ASTNode(new CommonToken(HiveParser.TOK_SELEXPR, "TOK_SELEXPR"));
            tableselexpr.addChild(child);
           // child.setParent(tableselexpr);
            tableselect1.addChild(tableselexpr);

            // tokselexpr1.setParent(tableselect2);
          }

          for(int i = 0;i<ast.getChildCount();i++){
            ASTNode origintokselexpr = (ASTNode) ast.getChild(i);

            ASTNode origintokselexprchild =newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(0));

            ASTNode tableselexpr = new ASTNode(new CommonToken(HiveParser.TOK_SELEXPR, "TOK_SELEXPR"));
            tableselexpr.addChild(origintokselexprchild);
            origintokselexprchild.setParent(tableselexpr);

            ASTNode expralias = null;
            if(origintokselexpr.getChildCount() ==2){
              ASTNode originalias = (ASTNode)origintokselexpr.getChild(1);
              expralias = ASTNodeUtils.newalias(originalias);
              tableselexpr.addChild(expralias);
              expralias.setParent(tableselexpr);

            }
            tableselect2.addChild(tableselexpr);
            tableselexpr.setParent(tableselect2);


          }

          LOG.info("INC : AST : select 1 :" + tableselect1.dump());
          LOG.info("INC : AST : select 2 :" + tableselect2.dump());
          qb.countSel();
          qbp.setSelExprForClause(ctx_1.dest, tableselect1);

          /*
           * if (((ASTNode) ast.getChild(0)).getToken().getType() == HiveParser.TOK_HINTLIST) {
           * qbp.setHints((ASTNode) ast.getChild(0));
           * }
           */

          LinkedHashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(
              tableselect1,
              qb, ctx_1.dest);
          doPhase1GetColumnAliasesFromSelect(tableselect1, qbp);
          qbp.setAggregationExprsForClause(ctx_1.dest, aggregations);
          qbp.setDistinctFuncExprsForClause(ctx_1.dest,
              doPhase1GetDistinctFuncExprs(aggregations));


          qb2.countSel();
          qbp2.setSelExprForClause(ctx_2.dest, tableselect2);

          LinkedHashMap<String, ASTNode> aggregations2 = doPhase1GetAggregationsFromSelect(
              tableselect2,
              qb2, ctx_2.dest);
          doPhase1GetColumnAliasesFromSelect(tableselect2, qbp2);
          qbp2.setAggregationExprsForClause(ctx_2.dest, aggregations2);
          qbp2.setDistinctFuncExprsForClause(ctx_2.dest,
              doPhase1GetDistinctFuncExprs(aggregations2));

        } else {

          qb2.countSel();
          //qbp2.setSelExprForClause(ctx_1.dest, ast);
          qbp2.setSelExprForClause(ctx_2.dest, ast);


          if (((ASTNode) ast.getChild(0)).getToken().getType() == HiveParser.TOK_HINTLIST) {
            qbp2.setHints((ASTNode) ast.getChild(0));
          }

          /*LinkedHashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast, qb2, ctx_1.dest);
          qbp2.setAggregationExprsForClause(ctx_1.dest, aggregations);
          qbp2.setDistinctFuncExprsForClause(ctx_1.dest,
              doPhase1GetDistinctFuncExprs(aggregations));*/
          LinkedHashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast, qb2, ctx_2.dest);
          doPhase1GetColumnAliasesFromSelect(ast, qbp2);
          qbp2.setAggregationExprsForClause(ctx_2.dest, aggregations);
          qbp2.setDistinctFuncExprsForClause(ctx_2.dest,
              doPhase1GetDistinctFuncExprs(aggregations));

        }



        break;

      case HiveParser.TOK_WHERE:
        if (ctx_1 != null) {
          qbp.setWhrExprForClause(ctx_1.dest, ast);
          //qbp2.setWhrExprForClause(ctx_2.dest, ast);
        } else {
          //qbp2.setWhrExprForClause(ctx_1.dest, ast);
          qbp2.setWhrExprForClause(ctx_2.dest, ast);
        }

        break;

      case HiveParser.TOK_INSERT_INTO:
        String currentDatabase = SessionState.get().getCurrentDatabase();
        String tab_name = getUnescapedName((ASTNode) ast.getChild(0).getChild(0), currentDatabase);
        if (ctx_1 != null) {
          qbp2.addInsertIntoTable(tab_name);
        } else {
          qbp2.addInsertIntoTable(tab_name);
        }


      case HiveParser.TOK_DESTINATION:
        if(ctx_1!=null){
          ctx_1.dest = "insclause-" + ctx_1.nextNum;
          ctx_1.nextNum++;
        }

        ctx_2.dest = "insclause-" + ctx_2.nextNum;
        ctx_2.nextNum++;

        // is there a insert in the subquery
        if (qbp != null && qbp.getIsSubQ()) {
          ASTNode ch = (ASTNode) ast.getChild(0);
          if ((ch.getToken().getType() != HiveParser.TOK_DIR)
              || (((ASTNode) ch.getChild(0)).getToken().getType() != HiveParser.TOK_TMP_FILE)) {
            throw new SemanticException(ErrorMsg.NO_INSERT_INSUBQUERY
                .getMsg(ast));
          }
        }
        if (ctx_1 != null) {
          //no insertoverwrite

          ASTNode toktab = ASTNodeUtils.newtokTable(curSQB.cachetable);
          qbp.setDestForClause(ctx_1.dest, toktab);

          String currentDatabase1 = SessionState.get().getCurrentDatabase();
          String tab_name1 = getUnescapedName((ASTNode) toktab.getChild(0), currentDatabase1);
          qbp.addInsertIntoTable(tab_name1);

          //ASTNode toktmpfile = ASTNodeUtils.newtoktmpfile();
          ASTNode tokdchild = (ASTNode) ast.getChild(0);
          qbp2.setDestForClause(ctx_2.dest, tokdchild);
          LOG.info("INC_AST TOK_DESTINATION.toktab: " + toktab.dump());
          LOG.info("INC_AST TOK_DESTINATION.toktmpfile: " + tokdchild.dump());
        } else {
          qbp2.setDestForClause(ctx_2.dest, (ASTNode) ast.getChild(0));
        }


        break;

      case HiveParser.TOK_FROM:
        int child_count = ast.getChildCount();
        if (child_count != 1) {
          throw new SemanticException(generateErrorMessage(ast,
              "Multiple Children " + child_count));
        }

        if (ctx_1 != null && ast.equals(curSQB.TOK_From)) {
          // --inc code
          // Create a cache Table for qb2;
          // Insert this map into the stats
          String alias = curSQB.cachetable;
          String tabIdName = curSQB.cachetable;
          qb2.setTabAlias(alias, tabIdName);
          qb2.addAlias(alias);
          ASTNode tableref = ASTNodeUtils.newtoktabref(tabIdName);
          qb2.getParseInfo().setSrcForAlias(alias, tableref);
          LOG.info("INC_AST TOK_TABREF: " + tableref.dump());

          // qb2.getParseInfo().setSrcForAlias(alias, tableTree);
          // --- inc code
          // Check if this is a subquery / lateral view
          ASTNode frm = (ASTNode) ast.getChild(0);
          qb.setId(curSQB.id);
          LOG.info("Set outer alias for split gby point: " + qb.getId());
          if (frm.getToken().getType() == HiveParser.TOK_TABREF) {
            processTable(qb, frm);
          } else if (frm.getToken().getType() == HiveParser.TOK_SUBQUERY) {
            processSubQuery(qb, frm);
          } else if (frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW ||
              frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW_OUTER) {
            processLateralView(qb, frm);
          } else if (isJoinToken(frm)) {
            queryProperties.setHasJoin(true);
            processJoin(qb, frm);
            //processIncJoin(qb,qb2, frm,curSQB);
            qbp.setJoinExpr(frm);
          } else if (frm.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) {
            queryProperties.setHasPTF(true);
            processPTF(qb, frm);
          }
        }else{

          // Check if this is a subquery / lateral view
          ASTNode frm = (ASTNode) ast.getChild(0);
          if (frm.getToken().getType() == HiveParser.TOK_TABREF) {
            processTable(qb2, frm);
          } else if (frm.getToken().getType() == HiveParser.TOK_SUBQUERY) {
            processIncSubQuery(qb,qb2, frm, curSQB);
          } else if (frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW ||
              frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW_OUTER) {
            //TODO
          } else if (isJoinToken(frm)) {
            queryProperties.setHasJoin(true);
            processIncJoin(qb,qb2, frm,curSQB);
            qbp2.setJoinExpr(frm);
          } else if (frm.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) {
            //TODO
          }

        }

        break;

      case HiveParser.TOK_CLUSTERBY:
        // Get the clusterby aliases - these are aliased to the entries in the
        // select list
        queryProperties.setHasClusterBy(true);
        qbp.setClusterByExprForClause(ctx_1.dest, ast);
        break;

      case HiveParser.TOK_DISTRIBUTEBY:
        // Get the distribute by aliases - these are aliased to the entries in
        // the
        // select list
        queryProperties.setHasDistributeBy(true);
        qbp.setDistributeByExprForClause(ctx_1.dest, ast);
        if (qbp.getClusterByForClause(ctx_1.dest) != null) {
          throw new SemanticException(generateErrorMessage(ast,
              ErrorMsg.CLUSTERBY_DISTRIBUTEBY_CONFLICT.getMsg()));
        } else if (qbp.getOrderByForClause(ctx_1.dest) != null) {
          throw new SemanticException(generateErrorMessage(ast,
              ErrorMsg.ORDERBY_DISTRIBUTEBY_CONFLICT.getMsg()));
        }
        break;

      case HiveParser.TOK_SORTBY:
        // Get the sort by aliases - these are aliased to the entries in the
        // select list
        queryProperties.setHasSortBy(true);
        qbp.setSortByExprForClause(ctx_1.dest, ast);
        if (qbp.getClusterByForClause(ctx_1.dest) != null) {
          throw new SemanticException(generateErrorMessage(ast,
              ErrorMsg.CLUSTERBY_SORTBY_CONFLICT.getMsg()));
        } else if (qbp.getOrderByForClause(ctx_1.dest) != null) {
          throw new SemanticException(generateErrorMessage(ast,
              ErrorMsg.ORDERBY_SORTBY_CONFLICT.getMsg()));
        }

        break;

      case HiveParser.TOK_ORDERBY:
        // Get the order by aliases - these are aliased to the entries in the
        // select list
        queryProperties.setHasOrderBy(true);

        if (ctx_1 != null) {
          // --- inc code
          // qbp.setOrderByExprForClause(ctx_1.dest, ast);
          ASTNode tokOrderBY = createOrderByAST(curSQB,ast);
          qbp2.setOrderByExprForClause(ctx_2.dest, tokOrderBY);
          LOG.info("INC AST: TOK_ORDERBY 1 :" + ast.dump());
          LOG.info("INC AST: TOK_ORDERBY 2 :" + tokOrderBY.dump());
          // --- inc code
          if (qbp.getClusterByForClause(ctx_1.dest) != null) {
            throw new SemanticException(generateErrorMessage(ast,
                ErrorMsg.CLUSTERBY_ORDERBY_CONFLICT.getMsg()));
          }
        } else {
          //qbp2.setOrderByExprForClause(ctx_1.dest, ast);
          qbp2.setOrderByExprForClause(ctx_2.dest, ast);
          if (qbp2.getClusterByForClause(ctx_2.dest) != null) {
            throw new SemanticException(generateErrorMessage(ast,
                ErrorMsg.CLUSTERBY_ORDERBY_CONFLICT.getMsg()));
          }
        }


        /*if (qbp.getClusterByForClause(ctx_1.dest) != null) {
          throw new SemanticException(generateErrorMessage(ast,
              ErrorMsg.CLUSTERBY_ORDERBY_CONFLICT.getMsg()));
        }*/
        break;

      case HiveParser.TOK_GROUPBY:
      case HiveParser.TOK_ROLLUP_GROUPBY:
      case HiveParser.TOK_CUBE_GROUPBY:
      case HiveParser.TOK_GROUPING_SETS:
        // Get the groupby aliases - these are aliased to the entries in the
        // select list
        queryProperties.setHasGroupBy(true);


        if (ctx_1 != null) {
          if (qbp.getJoinExpr() != null) {
            queryProperties.setHasJoinFollowedByGroupBy(true);
          }
          if (qbp.getSelForClause(ctx_1.dest).getToken().getType() == HiveParser.TOK_SELECTDI) {
            throw new SemanticException(generateErrorMessage(ast,
                ErrorMsg.SELECT_DISTINCT_WITH_GROUPBY.getMsg()));
          }

          qbp.setGroupByExprForClause(ctx_1.dest, ast);
          // --- inc code
          ASTNode tokGROUPBY = new ASTNode(new CommonToken(HiveParser.TOK_GROUPBY, "TOK_GROUPBY"));
          ASTNode tokgrychild = null;
          int gbycount = 0;
          for (int index = 0; index < ast.getChildCount(); index++) {
            ASTNode origgrychild = (ASTNode) ast.getChild(index);
            for (int i = 0; i < curSQB.collist.size(); i++) {
              CacheColumn col = curSQB.collist.get(i);
              if (col.isfunction == false) {
                if (ASTNodeUtils.isSameAST(origgrychild, col.origcolumnAST)) {
                  tokgrychild = ASTNodeUtils.newtoktableorcol(col.columnname);
                  tokgrychild.setParent(tokGROUPBY);
                  tokGROUPBY.addChild(tokgrychild);
                  gbycount++;
                  break;
                }

              }

            }

          }
          if (gbycount != ast.getChildCount()) {
            throw new SemanticException("INC ERROR: can't generate GROUPBY's Child!");
          }

          qbp2.setGroupByExprForClause(ctx_2.dest, tokGROUPBY);
          /*add distinct col to GBY in qb1*/
          if(distinctColName != null){
            ASTNode distinctCol = ASTNodeUtils.newtoktableorcol(distinctColName);
            ast.addChild(distinctCol);
            distinctCol.setParent(ast);
          }

          LOG.info("INC AST: GROUPBY 1 :" + ast.dump());
          LOG.info("INC AST: GROUPBY 2 :" + tokGROUPBY.dump());
          // --- inc code

        }else{
          if (qbp2.getJoinExpr() != null) {
            queryProperties.setHasJoinFollowedByGroupBy(true);
          }
          //if (qbp2.getSelForClause(ctx_1.dest).getToken().getType() == HiveParser.TOK_SELECTDI) {
          if (qbp2.getSelForClause(ctx_2.dest).getToken().getType() == HiveParser.TOK_SELECTDI) {
            throw new SemanticException(generateErrorMessage(ast,
                ErrorMsg.SELECT_DISTINCT_WITH_GROUPBY.getMsg()));
          }
          //qbp2.setGroupByExprForClause(ctx_1.dest, ast);
          qbp2.setGroupByExprForClause(ctx_2.dest, ast);
        }


        skipRecursion = true;

        // Rollup and Cubes are syntactic sugar on top of grouping sets
        /*if (ast.getToken().getType() == HiveParser.TOK_ROLLUP_GROUPBY) {
          qbp.getDestRollups().add(ctx_1.dest);
        } else if (ast.getToken().getType() == HiveParser.TOK_CUBE_GROUPBY) {
          qbp.getDestCubes().add(ctx_1.dest);
        } else if (ast.getToken().getType() == HiveParser.TOK_GROUPING_SETS) {
          qbp.getDestGroupingSets().add(ctx_1.dest);
        }*/
        break;

      case HiveParser.TOK_HAVING:
        qbp.setHavingExprForClause(ctx_1.dest, ast);
        qbp.addAggregationExprsForClause(ctx_1.dest,
            doPhase1GetAggregationsFromSelect(ast, qb, ctx_1.dest));
        break;

      case HiveParser.KW_WINDOW:
        if (!qb.hasWindowingSpec(ctx_1.dest)) {
          throw new SemanticException(generateErrorMessage(ast,
              "Query has no Cluster/Distribute By; but has a Window definition"));
        }
        handleQueryWindowClauses(qb, ctx_1, ast);
        break;

      case HiveParser.TOK_LIMIT:
        if (ctx_1 != null) {
          qbp.setDestLimit(ctx_1.dest, new Integer(ast.getChild(0).getText()));
        }
        qbp2.setDestLimit(ctx_2.dest, new Integer(ast.getChild(0).getText()));
        break;

      case HiveParser.TOK_ANALYZE:
        // Case of analyze command

        String table_name = getUnescapedName((ASTNode) ast.getChild(0).getChild(0));


        qb.setTabAlias(table_name, table_name);
        qb.addAlias(table_name);
        qb.getParseInfo().setIsAnalyzeCommand(true);
        qb.getParseInfo().setNoScanAnalyzeCommand(this.noscan);
        qb.getParseInfo().setPartialScanAnalyzeCommand(this.partialscan);
        // Allow analyze the whole table and dynamic partitions
        HiveConf.setVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
        HiveConf.setVar(conf, HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");

        break;

      case HiveParser.TOK_UNION:
        // currently, we dont support subq1 union subq2 - the user has to
        // explicitly say:
        // select * from (subq1 union subq2) subqalias
        if (!qbp.getIsSubQ()) {
          throw new SemanticException(generateErrorMessage(ast,
              ErrorMsg.UNION_NOTIN_SUBQ.getMsg()));
        }

      case HiveParser.TOK_INSERT:
        ASTNode destination = (ASTNode) ast.getChild(0);
        Tree tab = destination.getChild(0);

        // Proceed if AST contains partition & If Not Exists
        if (destination.getChildCount() == 2 &&
            tab.getChildCount() == 2 &&
            destination.getChild(1).getType() == HiveParser.TOK_IFNOTEXISTS) {
          String tableName = tab.getChild(0).getChild(0).getText();

          Tree partitions = tab.getChild(1);
          int childCount = partitions.getChildCount();
          HashMap<String, String> partition = new HashMap<String, String>();
          for (int i = 0; i < childCount; i++) {
            String partitionName = partitions.getChild(i).getChild(0).getText();
            Tree pvalue = partitions.getChild(i).getChild(1);
            if (pvalue == null) {
              break;
            }
            String partitionVal = stripQuotes(pvalue.getText());
            partition.put(partitionName, partitionVal);
          }
          // if it is a dynamic partition throw the exception
          if (childCount == partition.size()) {
            try {
              Table table = db.getTable(tableName);
              Partition parMetaData = db.getPartition(table, partition, false);
              // Check partition exists if it exists skip the overwrite
              if (parMetaData != null) {
                phase1Result = false;
                skipRecursion = true;
                LOG.info("Partition already exists so insert into overwrite " +
                    "skipped for partition : " + parMetaData.toString());
                break;
              }
            } catch (HiveException e) {
              LOG.info("Error while getting metadata : ", e);
            }
          } else {
            throw new SemanticException(ErrorMsg.INSERT_INTO_DYNAMICPARTITION_IFNOTEXISTS
                .getMsg(partition.toString()));
          }
        }
        skipRecursion = false;
        break;
      case HiveParser.TOK_LATERAL_VIEW:
      case HiveParser.TOK_LATERAL_VIEW_OUTER:
        // todo: nested LV
        assert ast.getChildCount() == 1;
        qb.getParseInfo().getDestToLateralView().put(ctx_1.dest, ast);
        break;
      default:
        skipRecursion = false;
        break;
      }
    }

    if (!skipRecursion) {
      // Iterate over the rest of the children
      int child_count = ast.getChildCount();
      for (int child_pos = 0; child_pos < child_count && phase1Result; ++child_pos) {
        // Recurse
        phase1Result = phase1Result
            && doPhase1Inc((ASTNode) ast.getChild(child_pos), qb, ctx_1, qb2, ctx_2, curSQB);
      }
    }
    return phase1Result;
  }

  public boolean doPhase1IncWithoutSplitGBY(ASTNode ast, QB qb2, Phase1Ctx ctx_2)
    throws SemanticException {

    boolean phase1Result = true;
    QBParseInfo qbp = null;
    if(qb != null) {
      qbp = qb.getParseInfo();
    }
    QBParseInfo qbp2 = qb2.getParseInfo();
    boolean skipRecursion = false;
    //Phase1Ctx ctx_1 = new Phase1Ctx();
    //SimpleQB curSQB = null;
    //QB qb = null;

  if (ast.getToken() != null) {
    skipRecursion = true;
    switch (ast.getToken().getType()) {
    case HiveParser.TOK_QUERY:
      skipRecursion = false;
      if(splitSQBs.containsKey(ast)){
         qbp2.setStoreIncInput(true);
      }

      break;

    case HiveParser.TOK_SELECTDI:
      qb2.countSelDi();
      // fall through
    case HiveParser.TOK_SELECT:

        qb2.countSel();
        //qbp2.setSelExprForClause(ctx_1.dest, ast);
        qbp2.setSelExprForClause(ctx_2.dest, ast);


        if (((ASTNode) ast.getChild(0)).getToken().getType() == HiveParser.TOK_HINTLIST) {
          qbp2.setHints((ASTNode) ast.getChild(0));
        }

        /*LinkedHashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast, qb2, ctx_1.dest);
        qbp2.setAggregationExprsForClause(ctx_1.dest, aggregations);
        qbp2.setDistinctFuncExprsForClause(ctx_1.dest,
            doPhase1GetDistinctFuncExprs(aggregations));*/
        LinkedHashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast, qb2, ctx_2.dest);
        doPhase1GetColumnAliasesFromSelect(ast, qbp2);
        qbp2.setAggregationExprsForClause(ctx_2.dest, aggregations);
        qbp2.setDistinctFuncExprsForClause(ctx_2.dest,
            doPhase1GetDistinctFuncExprs(aggregations));




      break;

    case HiveParser.TOK_WHERE:
        //qbp2.setWhrExprForClause(ctx_1.dest, ast);
        qbp2.setWhrExprForClause(ctx_2.dest, ast);

      break;

    case HiveParser.TOK_INSERT_INTO:
      String currentDatabase = SessionState.get().getCurrentDatabase();
      String tab_name = getUnescapedName((ASTNode) ast.getChild(0).getChild(0), currentDatabase);
      qbp2.addInsertIntoTable(tab_name);


    case HiveParser.TOK_DESTINATION:

      ctx_2.dest = "insclause-" + ctx_2.nextNum;
      ctx_2.nextNum++;

      // is there a insert in the subquery
      if (qbp != null && qbp.getIsSubQ()) {
        ASTNode ch = (ASTNode) ast.getChild(0);
        if ((ch.getToken().getType() != HiveParser.TOK_DIR)
            || (((ASTNode) ch.getChild(0)).getToken().getType() != HiveParser.TOK_TMP_FILE)) {
          throw new SemanticException(ErrorMsg.NO_INSERT_INSUBQUERY
              .getMsg(ast));
        }
      }
        qbp2.setDestForClause(ctx_2.dest, (ASTNode) ast.getChild(0));


      break;

    case HiveParser.TOK_FROM:
      int child_count = ast.getChildCount();
      if (child_count != 1) {
        throw new SemanticException(generateErrorMessage(ast,
            "Multiple Children " + child_count));
      }


        // Check if this is a subquery / lateral view
        ASTNode frm = (ASTNode) ast.getChild(0);
        if (frm.getToken().getType() == HiveParser.TOK_TABREF) {
          processTable(qb2, frm);
          qb2.getParseInfo().setStoreIncInput(false);
          scanAllIncAlias.add(QB.getAppendedAliasFromId(qb2.getId(), ASTNodeUtils.getAliasId(frm)));
          LOG.info("Add " + QB.getAppendedAliasFromId(qb2.getId(), ASTNodeUtils.getAliasId(frm)) + " to scanAllIncAlias");
        } else if (frm.getToken().getType() == HiveParser.TOK_SUBQUERY) {
          processIncSubQueryWithoutSplitGBY(qb2, frm);
        } else if (frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW ||
            frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW_OUTER) {
          //TODO
        } else if (isJoinToken(frm)) {
          queryProperties.setHasJoin(true);
          processIncJoinWithoutSplitGBY(qb2, frm);
          qbp2.setJoinExpr(frm);
        } else if (frm.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) {
          //TODO
        }


      break;

    case HiveParser.TOK_CLUSTERBY:
      // Get the clusterby aliases - these are aliased to the entries in the
      // select list
      queryProperties.setHasClusterBy(true);
      qbp.setClusterByExprForClause(ctx_2.dest, ast);
      break;

    case HiveParser.TOK_DISTRIBUTEBY:
      // Get the distribute by aliases - these are aliased to the entries in
      // the
      // select list
      queryProperties.setHasDistributeBy(true);
      qbp.setDistributeByExprForClause(ctx_2.dest, ast);
      if (qbp.getClusterByForClause(ctx_2.dest) != null) {
        throw new SemanticException(generateErrorMessage(ast,
            ErrorMsg.CLUSTERBY_SORTBY_CONFLICT.getMsg()));
      } else if (qbp.getOrderByForClause(ctx_2.dest) != null) {
        throw new SemanticException(generateErrorMessage(ast,
            ErrorMsg.ORDERBY_SORTBY_CONFLICT.getMsg()));
      }
      break;

    case HiveParser.TOK_SORTBY:
      // Get the sort by aliases - these are aliased to the entries in the
      // select list
      queryProperties.setHasSortBy(true);
      qbp.setSortByExprForClause(ctx_2.dest, ast);
      if (qbp.getClusterByForClause(ctx_2.dest) != null) {
        throw new SemanticException(generateErrorMessage(ast,
            ErrorMsg.CLUSTERBY_SORTBY_CONFLICT.getMsg()));
      } else if (qbp.getOrderByForClause(ctx_2.dest) != null) {
        throw new SemanticException(generateErrorMessage(ast,
            ErrorMsg.ORDERBY_SORTBY_CONFLICT.getMsg()));
      }

      break;

    case HiveParser.TOK_ORDERBY:
      // Get the order by aliases - these are aliased to the entries in the
      // select list
      queryProperties.setHasOrderBy(true);

        //qbp2.setOrderByExprForClause(ctx_1.dest, ast);
        qbp2.setOrderByExprForClause(ctx_2.dest, ast);
        if (qbp2.getClusterByForClause(ctx_2.dest) != null) {
          throw new SemanticException(generateErrorMessage(ast,
              ErrorMsg.CLUSTERBY_ORDERBY_CONFLICT.getMsg()));
        }


      /*if (qbp.getClusterByForClause(ctx_1.dest) != null) {
        throw new SemanticException(generateErrorMessage(ast,
            ErrorMsg.CLUSTERBY_ORDERBY_CONFLICT.getMsg()));
      }*/
      break;

    case HiveParser.TOK_GROUPBY:
    case HiveParser.TOK_ROLLUP_GROUPBY:
    case HiveParser.TOK_CUBE_GROUPBY:
    case HiveParser.TOK_GROUPING_SETS:
      // Get the groupby aliases - these are aliased to the entries in the
      // select list
      queryProperties.setHasGroupBy(true);


        if (qbp2.getJoinExpr() != null) {
          queryProperties.setHasJoinFollowedByGroupBy(true);
        }
        //if (qbp2.getSelForClause(ctx_1.dest).getToken().getType() == HiveParser.TOK_SELECTDI) {
        if (qbp2.getSelForClause(ctx_2.dest).getToken().getType() == HiveParser.TOK_SELECTDI) {
          throw new SemanticException(generateErrorMessage(ast,
              ErrorMsg.SELECT_DISTINCT_WITH_GROUPBY.getMsg()));
        }
        //qbp2.setGroupByExprForClause(ctx_1.dest, ast);
        qbp2.setGroupByExprForClause(ctx_2.dest, ast);


      skipRecursion = true;

      // Rollup and Cubes are syntactic sugar on top of grouping sets
      /*if (ast.getToken().getType() == HiveParser.TOK_ROLLUP_GROUPBY) {
        qbp.getDestRollups().add(ctx_1.dest);
      } else if (ast.getToken().getType() == HiveParser.TOK_CUBE_GROUPBY) {
        qbp.getDestCubes().add(ctx_1.dest);
      } else if (ast.getToken().getType() == HiveParser.TOK_GROUPING_SETS) {
        qbp.getDestGroupingSets().add(ctx_1.dest);
      }*/
      break;

    case HiveParser.TOK_HAVING:
      qbp.setHavingExprForClause(ctx_2.dest, ast);
      qbp.addAggregationExprsForClause(ctx_2.dest,
      doPhase1GetAggregationsFromSelect(ast, qb, ctx_2.dest));
      break;

    case HiveParser.KW_WINDOW:
      if (!qb.hasWindowingSpec(ctx_2.dest)) {
        throw new SemanticException(generateErrorMessage(ast,
            "Query has no Cluster/Distribute By; but has a Window definition"));
      }
      handleQueryWindowClauses(qb, ctx_2, ast);
      break;

    case HiveParser.TOK_LIMIT:
      //qbp.setDestLimit(ctx_1.dest, new Integer(ast.getChild(0).getText()));
      qbp2.setDestLimit(ctx_2.dest, new Integer(ast.getChild(0).getText()));
      break;

    case HiveParser.TOK_ANALYZE:
      // Case of analyze command

      String table_name = getUnescapedName((ASTNode) ast.getChild(0).getChild(0));


      qb.setTabAlias(table_name, table_name);
      qb.addAlias(table_name);
      qb.getParseInfo().setIsAnalyzeCommand(true);
      qb.getParseInfo().setNoScanAnalyzeCommand(this.noscan);
      qb.getParseInfo().setPartialScanAnalyzeCommand(this.partialscan);
      // Allow analyze the whole table and dynamic partitions
      HiveConf.setVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
      HiveConf.setVar(conf, HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");

      break;

    case HiveParser.TOK_UNION:
      // currently, we dont support subq1 union subq2 - the user has to
      // explicitly say:
      // select * from (subq1 union subq2) subqalias
      if (!qbp.getIsSubQ()) {
        throw new SemanticException(generateErrorMessage(ast,
            ErrorMsg.UNION_NOTIN_SUBQ.getMsg()));
      }

    case HiveParser.TOK_INSERT:
      ASTNode destination = (ASTNode) ast.getChild(0);
      Tree tab = destination.getChild(0);

      // Proceed if AST contains partition & If Not Exists
      if (destination.getChildCount() == 2 &&
          tab.getChildCount() == 2 &&
          destination.getChild(1).getType() == HiveParser.TOK_IFNOTEXISTS) {
        String tableName = tab.getChild(0).getChild(0).getText();

        Tree partitions = tab.getChild(1);
        int childCount = partitions.getChildCount();
        HashMap<String, String> partition = new HashMap<String, String>();
        for (int i = 0; i < childCount; i++) {
          String partitionName = partitions.getChild(i).getChild(0).getText();
          Tree pvalue = partitions.getChild(i).getChild(1);
          if (pvalue == null) {
            break;
          }
          String partitionVal = stripQuotes(pvalue.getText());
          partition.put(partitionName, partitionVal);
        }
        // if it is a dynamic partition throw the exception
        if (childCount == partition.size()) {
          try {
            Table table = db.getTable(tableName);
            Partition parMetaData = db.getPartition(table, partition, false);
            // Check partition exists if it exists skip the overwrite
            if (parMetaData != null) {
              phase1Result = false;
              skipRecursion = true;
              LOG.info("Partition already exists so insert into overwrite " +
                  "skipped for partition : " + parMetaData.toString());
              break;
            }
          } catch (HiveException e) {
            LOG.info("Error while getting metadata : ", e);
          }
        } else {
          throw new SemanticException(ErrorMsg.INSERT_INTO_DYNAMICPARTITION_IFNOTEXISTS
              .getMsg(partition.toString()));
        }
      }
      skipRecursion = false;
      break;
    case HiveParser.TOK_LATERAL_VIEW:
    case HiveParser.TOK_LATERAL_VIEW_OUTER:
      // todo: nested LV
      assert ast.getChildCount() == 1;
      qb.getParseInfo().getDestToLateralView().put(ctx_2.dest, ast);
      break;
    default:
      skipRecursion = false;
      break;
    }
  }

  if (!skipRecursion) {
    // Iterate over the rest of the children
    int child_count = ast.getChildCount();
    for (int child_pos = 0; child_pos < child_count && phase1Result; ++child_pos) {
      // Recurse
      phase1Result = phase1Result
          && doPhase1IncWithoutSplitGBY((ASTNode) ast.getChild(child_pos), qb2, ctx_2);
    }
  }
  return phase1Result;
}

  private ASTNode newSelectExpr2(SimpleQB curSQB, ASTNode origintokselexpr) throws SemanticException {
    // TODO Auto-generated method stub
    int type = origintokselexpr.getToken().getType();
    switch(type){
    case HiveParser.STAR :
      return ASTNodeUtils.newStarAST(newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(0)),
      newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(1)) );
    case HiveParser.MINUS :
      return ASTNodeUtils.newMinusAST(newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(0)),
          newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(1)) );
    case HiveParser.PLUS :
      return ASTNodeUtils.newPlusAST(newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(0)),
          newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(1)) );
    case HiveParser.DIVIDE :
      return ASTNodeUtils.newDividAST(newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(0)),
          newSelectExpr2(curSQB,(ASTNode)origintokselexpr.getChild(1)) );
    case HiveParser.TOK_FUNCTION :
       return ASTNodeUtils.newSelExpr2withFunc(origintokselexpr, curSQB.funcmap);
    case HiveParser.TOK_TABLE_OR_COL :
    case HiveParser.DOT :
       String colname = curSQB.ASTtoCol.get(origintokselexpr).columnname;
      return ASTNodeUtils.newtoktableorcol(colname);
    default:
      return origintokselexpr;


    }
  }

  private void processIncJoin(QB qb1, QB qb2, ASTNode join,SimpleQB curSQB)throws SemanticException {
    int numChildren = join.getChildCount();
    if ((numChildren != 2) && (numChildren != 3)
        && join.getToken().getType() != HiveParser.TOK_UNIQUEJOIN) {
      throw new SemanticException(generateErrorMessage(join,
          "Join with multiple children"));
    }

    for (int num = 0; num < numChildren; num++) {
      ASTNode child = (ASTNode) join.getChild(num);
      if (child.getToken().getType() == HiveParser.TOK_TABREF) {
        processTable(qb2, child);
      } else if (child.getToken().getType() == HiveParser.TOK_SUBQUERY) {
        processIncSubQuery(qb1,qb2, child,curSQB);
      } else if (child.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) {

      } else if (child.getToken().getType() == HiveParser.TOK_LATERAL_VIEW ||
          child.getToken().getType() == HiveParser.TOK_LATERAL_VIEW_OUTER) {

      } else if (isJoinToken(child)) {
        processIncJoin(qb1,qb2, child,curSQB);
      }
    }
  }

  protected void processIncJoinWithoutSplitGBY(QB qb, ASTNode join) throws SemanticException {
    int numChildren = join.getChildCount();
    if ((numChildren != 2) && (numChildren != 3)
        && join.getToken().getType() != HiveParser.TOK_UNIQUEJOIN) {
      throw new SemanticException(generateErrorMessage(join,
          "Join with multiple children"));
    }

    for (int num = 0; num < numChildren; num++) {
      ASTNode child = (ASTNode) join.getChild(num);
      if (child.getToken().getType() == HiveParser.TOK_TABREF) {
        processTable(qb, child);
      } else if (child.getToken().getType() == HiveParser.TOK_SUBQUERY) {
        processIncSubQueryWithoutSplitGBY(qb, child);
      } else if (child.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) {
        queryProperties.setHasPTF(true);
        processPTF(qb, child);
        PTFInvocationSpec ptfInvocationSpec = qb.getPTFInvocationSpec(child);
        String inputAlias = ptfInvocationSpec == null ? null :
          ((PartitionedTableFunctionSpec)ptfInvocationSpec.getFunction()).getAlias();;
        if ( inputAlias == null ) {
          throw new SemanticException(generateErrorMessage(child,
              "PTF invocation in a Join must have an alias"));
        }

      } else if (child.getToken().getType() == HiveParser.TOK_LATERAL_VIEW ||
          child.getToken().getType() == HiveParser.TOK_LATERAL_VIEW_OUTER) {
        // SELECT * FROM src1 LATERAL VIEW udtf() AS myTable JOIN src2 ...
        // is not supported. Instead, the lateral view must be in a subquery
        // SELECT * FROM (SELECT * FROM src1 LATERAL VIEW udtf() AS myTable) a
        // JOIN src2 ...
        throw new SemanticException(ErrorMsg.LATERAL_VIEW_WITH_JOIN
            .getMsg(join));
      } else if (isJoinToken(child)) {
        processIncJoinWithoutSplitGBY(qb, child);
      }
    }
  }

  protected String processIncSubQueryWithoutSplitGBY(QB qb, ASTNode subq) throws SemanticException {

    // This is a subquery and must have an alias
    if (subq.getChildCount() != 2) {
      throw new SemanticException(ErrorMsg.NO_SUBQUERY_ALIAS.getMsg(subq));
    }
    ASTNode subqref = (ASTNode) subq.getChild(0);
    String alias = unescapeIdentifier(subq.getChild(1).getText());

    // Recursively do the first phase of semantic analysis for the subquery
    QBExpr qbexpr = new QBExpr(alias);

    doPhase1IncQBExprWithoutSplitGBY(subqref, qbexpr, qb.getId(), alias);

    // If the alias is already there then we have a conflict
    if (qb.exists(alias)) {
      throw new SemanticException(ErrorMsg.AMBIGUOUS_TABLE_ALIAS.getMsg(subq
          .getChild(1)));
    }
    // Insert this map into the stats
    qb.setSubqAlias(alias, qbexpr);
    qb.addAlias(alias);

    unparseTranslator.addIdentifierTranslation((ASTNode) subq.getChild(1));

    return alias;
  }

  public void doPhase1IncQBExprWithoutSplitGBY(ASTNode ast, QBExpr qbexpr, String id, String alias)
  throws SemanticException {

    assert (ast.getToken() != null);
    switch (ast.getToken().getType()) {
    case HiveParser.TOK_QUERY: {
      QB qb = new QB(id, alias, true);
      Phase1Ctx ctx_1 = initPhase1Ctx();
      doPhase1IncWithoutSplitGBY(ast, qb, ctx_1);

      qbexpr.setOpcode(QBExpr.Opcode.NULLOP);
      qbexpr.setQB(qb);
    }
      break;
    case HiveParser.TOK_UNION: {
      qbexpr.setOpcode(QBExpr.Opcode.UNION);
      // query 1
      assert (ast.getChild(0) != null);
      QBExpr qbexpr1 = new QBExpr(alias + "-subquery1");
      doPhase1QBExpr((ASTNode) ast.getChild(0), qbexpr1, id + "-subquery1",
          alias + "-subquery1");
      qbexpr.setQBExpr1(qbexpr1);

      // query 2
      assert (ast.getChild(0) != null);
      QBExpr qbexpr2 = new QBExpr(alias + "-subquery2");
      doPhase1QBExpr((ASTNode) ast.getChild(1), qbexpr2, id + "-subquery2",
          alias + "-subquery2");
      qbexpr.setQBExpr2(qbexpr2);
    }
      break;
    }
}

  private String processIncSubQuery(QB qb1, QB qb2, ASTNode subq,SimpleQB curSQB) throws SemanticException {
    /*if(((ASTNode)subq.getChild(0)).equals(this.tok_query)){


    }else{
      return processSubQuery(qb2, subq);
    }*/
    // This is a subquery and must have an alias
    if (subq.getChildCount() != 2) {
      throw new SemanticException(ErrorMsg.NO_SUBQUERY_ALIAS.getMsg(subq));
    }
    ASTNode subqref = (ASTNode) subq.getChild(0);
    String alias = unescapeIdentifier(subq.getChild(1).getText());

    // Recursively do the first phase of semantic analysis for the subquery
    QBExpr qbexpr = new QBExpr(alias);

    doPhase1IncQBExpr(subqref, qbexpr, qb2.getId(), alias,qb1,curSQB);

    // If the alias is already there then we have a conflict
    if (qb2.exists(alias)) {
      throw new SemanticException(ErrorMsg.AMBIGUOUS_TABLE_ALIAS.getMsg(subq
          .getChild(1)));
    }
    // Insert this map into the stats
    qb2.setSubqAlias(alias, qbexpr);
    qb2.addAlias(alias);

    unparseTranslator.addIdentifierTranslation((ASTNode) subq.getChild(1));

    return alias;



  }


  private void doPhase1IncQBExpr(ASTNode subqref, QBExpr qbexpr, String id, String alias,QB qb1,SimpleQB curSQB) throws SemanticException {

    assert (subqref.getToken() != null);
    switch (subqref.getToken().getType()) {
    case HiveParser.TOK_QUERY: {
      QB qb = new QB(id, alias, true);
      Phase1Ctx ctx_1 = initPhase1Ctx();
      Phase1Ctx ctx_2 = initPhase1Ctx();
      //doPhase1Inc(subqref,qb1, ctx_1,qb, ctx_2,curSQB);
      doPhase1Inc(subqref,null, null,qb, ctx_2,curSQB);

      qbexpr.setOpcode(QBExpr.Opcode.NULLOP);
      qbexpr.setQB(qb);
    }
      break;
    case HiveParser.TOK_UNION: {
      //TODO
    }
      break;
    }
  }


  /**
   * create orderby AST
   *
   * TOK_ORDERBY
   *   TOK_TABSORTCOLNAMEASC/TOK_TABSORTCOLNAMEDESC
   *     EXPRESSION
   *   ....
   *   ....
   *   TOK_TABSORTCOLNAMEASC/TOK_TABSORTCOLNAMEDESC
   *     EXPRESSION
   * @throws SemanticException
   */

  private ASTNode createOrderByAST(SimpleQB curSQB,ASTNode orderby) throws SemanticException{
    ASTNode dest =new ASTNode(new CommonToken(HiveParser.TOK_ORDERBY, "TOK_ORDERBY"));
    ASTNode tokodychild = null;
    ASTNode expr = null;
    int ordercount =0;

    for(int index=0; index<orderby.getChildCount(); index++){
      ASTNode origorderchild = (ASTNode)orderby.getChild(index);
      if(origorderchild.getToken().getType() == HiveParser.TOK_TABSORTCOLNAMEASC ){
        tokodychild = new ASTNode(new CommonToken(HiveParser.TOK_TABSORTCOLNAMEASC, "TOK_TABSORTCOLNAMEASC"));
      }else{
        tokodychild = new ASTNode(new CommonToken(HiveParser.TOK_TABSORTCOLNAMEDESC, "TOK_TABSORTCOLNAMEDESC"));
      }
      dest.addChild(tokodychild);
      tokodychild.setParent(dest);
      ASTNode origexpr = (ASTNode)origorderchild.getChild(0);

        for(int i=0; i < curSQB.collist.size(); i++){
          CacheColumn col = curSQB.collist.get(i);
          if(ASTNodeUtils.isSameAST(origexpr, col.origcolumnAST)){
            expr = ASTNodeUtils.newtoktableorcol(col.columnname);
            expr.setParent(tokodychild);
            tokodychild.addChild(expr);
            ordercount++;
            break;
          }
          if(col.origcolumnAST.getToken().getType() ==HiveParser.DOT){
            if(ASTNodeUtils.isSameAST(origexpr, (ASTNode)col.origcolumnAST.getChild(1))){
              expr = ASTNodeUtils.newtoktableorcol(col.columnname);
              expr.setParent(tokodychild);
              tokodychild.addChild(expr);
              ordercount++;
              break;
            }
          }
          if(!col.alias.equals("")){
            if(ASTNodeUtils.isSameAST(origexpr, ASTNodeUtils.newtoktableorcol(col.alias))){
              expr = ASTNodeUtils.newtoktableorcol(col.alias);
              expr.setParent(tokodychild);
              tokodychild.addChild(expr);
              ordercount++;
              break;
            }
          }
        }
    }

    if(ordercount != orderby.getChildCount()){
      throw new SemanticException("INC ERROR: can't generate ORDER BY's Child!");
    }
    return dest;
  }

  @Override
  public void reset(){
    super.reset();
    inputs = new LinkedHashSet<ReadEntity>();
    outputs = new LinkedHashSet<WriteEntity>();
    loadTableWork.clear();
    loadFileWork.clear();
    topOps.clear();
    topSelOps.clear();
    destTableId = 1;
    idToTableNameMap.clear();
    qb = null;
    ast = null;
    uCtx = null;
    joinContext.clear();
    smbMapJoinContext.clear();
    opParseCtx.clear();
    groupOpToInputTables.clear();
    prunedPartitions.clear();

  }

  public ArrayList<QB> getJoinCacheQBList() {
    return joinCacheQBlist;
  }

  public QB getFinalQB() {
    return finalQB;
  }

  public ArrayList<String> getScanAllIncAliases(){
    return scanAllIncAlias;
  }
}
