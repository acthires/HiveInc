package org.apache.hadoop.hive.ql.optimizer.correlation;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.Stack;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.hive.ql.exec.ColumnInfo;
import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
import org.apache.hadoop.hive.ql.exec.GroupByOperator;
import org.apache.hadoop.hive.ql.exec.JoinOperator;
import org.apache.hadoop.hive.ql.exec.Operator;
import org.apache.hadoop.hive.ql.exec.OperatorFactory;
import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
import org.apache.hadoop.hive.ql.exec.RowSchema;
import org.apache.hadoop.hive.ql.exec.SelectOperator;
import org.apache.hadoop.hive.ql.exec.TableScanOperator;
import org.apache.hadoop.hive.ql.lib.DefaultGraphWalker;
import org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher;
import org.apache.hadoop.hive.ql.lib.Dispatcher;
import org.apache.hadoop.hive.ql.lib.GraphWalker;
import org.apache.hadoop.hive.ql.lib.Node;
import org.apache.hadoop.hive.ql.lib.NodeProcessor;
import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
import org.apache.hadoop.hive.ql.lib.Rule;
import org.apache.hadoop.hive.ql.metadata.Table;
import org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils;
import org.apache.hadoop.hive.ql.optimizer.Transform;
import org.apache.hadoop.hive.ql.parse.InterQueryFlowCtx;
import org.apache.hadoop.hive.ql.parse.MultiParseContext;
import org.apache.hadoop.hive.ql.parse.OpParseContext;
import org.apache.hadoop.hive.ql.parse.ParseContext;
import org.apache.hadoop.hive.ql.parse.RowResolver;
import org.apache.hadoop.hive.ql.parse.SemanticException;
import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
import org.apache.hadoop.hive.ql.plan.OperatorDesc;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.StructField;

public class InlineOptimizer implements Transform {

  private static final Log LOG = LogFactory.getLog(InlineOptimizer.class.getName());

  private boolean abort;

  private int inlineCnt;

  private MultiParseContext pCtx;

  /* Recently, we only deal with FS whose corresponding TS number is less than 2
   * when the FS itself does not involve complex query primitive such as join.
   * Those skipped FS should be added to InterQueryFlow Class and will later be
   * handled by MR task generator.
   */
  private final Set<Operator<? extends OperatorDesc>> skipedFSOperators;

  public InlineOptimizer() {
    super();
    pCtx = null;
    inlineCnt = 0;
    abort = false;
    skipedFSOperators = new HashSet<Operator<? extends OperatorDesc>>();
  }

  @Override
  public ParseContext transform(ParseContext pctx) throws SemanticException {

    pCtx = (MultiParseContext)pctx;

    InterQueryFlowCtx queryFlowCtx=pCtx.getQueryFlowCtx();

    for (int ver : queryFlowCtx.getTabVersion().keySet() ){
      FileSinkOperator fsop=queryFlowCtx.getWrOp(ver);

      if (fsop != null){
        List<TableScanOperator> tsList=queryFlowCtx.getRdOpList(ver);

        List<Operator<? extends OperatorDesc>> fsParents = fsop.getParentOperators();
        if (tsList ==null) {
          continue;
        }

        if (tsList.size() > 2 || tsList.size() == 2 && isQueryClonable(fsop)) {
           /* 1) if current fs has more than 2 corresponding ts ops, we prone to
            * treat its subtree as a common subquery OpTree. Therefore, no inline
            * optimization should be performed for this case. However, we will
            * have to keep an eye of the job submission order later.
            * Skipped FS ops should be maintained to help sort out which fs
            * should execute before which ts when MRCompiler.compile is done.
            * 2) never clone a query with JOIN or GBY ops in it as cloning JOIN may
            * increase execution cost and the conf field of GBY is not clonable.
            * 3) if current fs is leading a query which is generated by common sub
            * -query optimization, then the query it belongs to should not be cloned
            * either.
            */
           skipedFSOperators.add(fsop);
           LOG.info("Skip FS " + fsop.getIdentifier());
           continue;
        }

        int localInlineCnt = 0;
        for (TableScanOperator tsop : tsList) {
          //fsop.setParentOperators(null);
          List<Operator<?>> tsChildren = tsop.getChildOperators();

          if (tsChildren == null) {
            /* Another way to check if a tsop has been optimized is
             * to check whether it can be found in the topOps set
             * or not
             */
            LOG.info(tsop + " has been optimized by CSQ OPT");
            continue;
          }

          if (tsChildren.size() > 1) {
             throw new SemanticException(tsop + " has more than 1 child");
          }

          LOG.info("Try inlining " + fsop.getIdentifier() + " to " + tsop);

          List<Operator<?>> inlinedParents = new ArrayList<Operator<?>>();
          Operator<?> tsChild = tsChildren.get(0);
          tsChild.setParentOperators(null);

          boolean inlineSucc = true;
          Operator<? extends OperatorDesc> parent;

          for(Operator<? extends OperatorDesc> fsParent : fsParents) {
             if (localInlineCnt == tsList.size()-1) {
                parent = fsParent;
                List<Operator<?>> originalChildren = parent.getChildOperators();
                int pos = originalChildren.indexOf(fsop);
	        assert pos != -1;

                originalChildren.remove(pos);
                originalChildren.add(pos, tsChild);
             } else {
                parent = cloneWithRR(fsParent);

                if (abort) {
                   LOG.info("Abort Reason: clone is not supported !");
                   return pCtx;
                }

                List<Operator<?>> originalChildren = fsParent.getChildOperators();
                List<Operator<?>> cloneChildren = new ArrayList<Operator<?>>();
                for (Operator<?> originalChild : originalChildren) {
                   if (originalChild.equals(fsop)) {
                    cloneChildren.add(tsChild);
                  } else {
                    cloneChildren.add(originalChild);
                  }
                }
                if (cloneChildren.size() > 0) {
                  parent.setChildOperators(cloneChildren);
                }
             }

             inlineSucc = InlineTransform(parent, tsop, fsop);
             inlineSucc = inlineSucc && inlinedParents.add(parent);
          }

          if (inlineSucc) {
             localInlineCnt++;
             inlineCnt++;
          } else {
             throw new SemanticException("inline failed");
          }

          tsChild.setParentOperators(inlinedParents);

          tsop.setChildOperators(null);
          tsop.setParentOperators(null);
          fsop.setChildOperators(null);
          fsop.setParentOperators(null);

          removetopopTS(pCtx,tsop);
        }
      }
    }

    LOG.info(inlineCnt + " inlining applied.");

    // for validation
    if (LOG.isDebugEnabled()) {
       InlineWalker();
    }

    return pCtx;
  }

  private boolean isQueryClonable(Operator<? extends OperatorDesc> op) {
     List<Operator<? extends OperatorDesc>> parentOps = op.getParentOperators();
     if (parentOps != null && parentOps.size() > 0) {
        for (Operator<? extends OperatorDesc> parentOp : parentOps)
        {
           if ((parentOp.getChildOperators().size() > 1) ||
               (parentOp instanceof JoinOperator) ||
               (parentOp instanceof GroupByOperator) ||
               isQueryClonable(parentOp)) {
              return true;
           }
        }
     }
     return false;
  }

  private void insertRowResolvers(Operator<?> op, Operator<?> opClone) {
     // construct and insert new RR for clone op recursively
     RowResolver rr = pCtx.getOpParseCtx().get(op).getRowResolver();
     HashMap<String, LinkedHashMap<String, ColumnInfo>> rslvMap =
                rr.getRslvMap();

     RowResolver rrClone = new RowResolver();
     if (rslvMap != null) {
        for (Map.Entry<String, LinkedHashMap<String, ColumnInfo>> e : rslvMap.entrySet()) {
           String tab = e.getKey();
           LinkedHashMap<String, ColumnInfo> f_map = e.getValue();

           for (Map.Entry<String, ColumnInfo> entry : f_map.entrySet()) {
              String internalName = entry.getKey();
              ColumnInfo colInfo = entry.getValue();
              ColumnInfo cInfoClone = new ColumnInfo(colInfo);
              cInfoClone.setType(colInfo.getType());
              rrClone.put(tab, internalName, cInfoClone);
           }
        }
     }

     GenMapRedUtils.putOpInsertMap(opClone, rrClone, pCtx);

     List<Operator<? extends OperatorDesc>> parents = op.getParentOperators();
     List<Operator<? extends OperatorDesc>> parentClones = opClone.getParentOperators();
     if ((parents != null) && (!parents.isEmpty()) &&
         (parentClones != null) && (!parentClones.isEmpty())) {
        for (int pos = 0; pos < parents.size(); pos++) {
           insertRowResolvers(parents.get(pos), parentClones.get(pos));
        }
     }

     if (op instanceof TableScanOperator) {
        TableScanOperator tsop = (TableScanOperator)op;
        String alias = tsop.getConf().getAlias();
        Table tab = pCtx.getTopToTable().get(tsop);

        int queryId = inlineCnt + 1;
        pCtx.getTopOps().put("query"+queryId+":"+alias, opClone);
        pCtx.getTopToTable().put((TableScanOperator)opClone, tab);
     }

     // for RS op, we should also construct column expression map for them
     if (op instanceof ReduceSinkOperator) {
        Map<String, ExprNodeDesc> colMap = op.getColumnExprMap();

        if (colMap != null) {
           Map<String, ExprNodeDesc> colMapClone =
      		new HashMap<String, ExprNodeDesc>();

           for (Map.Entry<String, ExprNodeDesc> e : colMap.entrySet()) {
              String internal = e.getKey();
              ExprNodeDesc desc = e.getValue().clone();
              colMapClone.put(internal, desc);
           }

           LOG.info("Inline transform: construct column expr map for " +
      		opClone.getIdentifier());
           opClone.setColumnExprMap(colMapClone);
        }
     }
  }

  private Operator<? extends OperatorDesc> cloneWithRR(Operator<?> op) {
    try {
       Operator<? extends OperatorDesc> opClone = clone(op);
       insertRowResolvers(op, opClone);
       return opClone;
    } catch (CloneNotSupportedException e) {
       abort = true;
       return null;
    }
  }

  /* instead of using the clone method of class Operator<T>, we implement
   * our own method to generate copies for inline operators so that none
   * of those operators will share the same RowSchema, or more especially,
   * the same column infos with their original copies.
   */
  private Operator<? extends OperatorDesc> clone(Operator<? extends OperatorDesc> op)
    throws CloneNotSupportedException {

    LOG.info("Inline transform: cloning " + op);

    List<Operator<? extends OperatorDesc>> parents = op.getParentOperators();
    List<Operator<? extends OperatorDesc>> parentClones =
      new ArrayList<Operator<? extends OperatorDesc>>();

    if (parents != null) {
      for (Operator<? extends OperatorDesc> parent : parents) {
        //parentClones.add((Operator<? extends OperatorDesc>)(parent.clone()));
        parentClones.add(clone(parent));
      }
    }

    RowResolver inputRR = pCtx.getOpParseCtx().get(op).getRowResolver();
    RowSchema rwshc = new RowSchema(inputRR.getColumnInfos());
    OperatorDesc descClone = (OperatorDesc)(op.getConf().clone());

    Operator<? extends OperatorDesc> ret =
      (Operator<? extends OperatorDesc>) OperatorFactory.getAndMakeChild(
        descClone, rwshc, parentClones);

    LOG.info("Inline Transform: done with " + op.getType() + " " + op.getIdentifier());

    return ret;
  }

  private boolean InlineTransform(Operator<?> op, TableScanOperator tsop,
					FileSinkOperator fsop)
					throws SemanticException {
    if (!(op instanceof SelectOperator)) {
       throw new SemanticException("Inliner: unexpect op type " + op.getName());
    }

    Map<Operator<? extends OperatorDesc>, OpParseContext>
		ctxMap = pCtx.getOpParseCtx();

    RowResolver inputRR = ctxMap.get(op).getRowResolver();
    Map<String, ExprNodeDesc> colExprMap = op.getColumnExprMap();
    List<String> outputColNames = null;
    if (op instanceof SelectOperator) {
      outputColNames = ((SelectOperator)op).getConf().getOutputColumnNames();
    }

    LOG.info("Inliner: transform tab alias and internal col names for " + op);

    String alias = tsop.getConf().getAlias();

    Table fs_tbl = pCtx.getFsopToTable().get(fsop);
    // since fsop has correponding tsops in InterQueryFlowCtx, it must have
    // updated some table
    if (fs_tbl == null) {
       throw new SemanticException(fsop + " has no table");
    }

    ArrayList<StructField> fs_fields = fs_tbl.getFields();
    HashMap<String, LinkedHashMap<String, ColumnInfo>> rslvMap =
		inputRR.getRslvMap();

    /* transform table alias name and column alias names in parentOp of the inlined fsop
     * so that the inlined query will be treated as a sub-query
     */
    int i = 0;
    if (rslvMap != null) {
       LinkedHashMap<String, ColumnInfo> n_map = new LinkedHashMap<String, ColumnInfo>();
       List<String> dropList = new ArrayList<String>();

       for (Map.Entry<String, LinkedHashMap<String, ColumnInfo>> e : rslvMap.entrySet()) {
          String tab = e.getKey();
          LinkedHashMap<String, ColumnInfo> f_map = e.getValue();

          for (Map.Entry<String, ColumnInfo> entry : f_map.entrySet()) {
             LOG.info("Inline transform: modify tab & col alias (" + tab + "," + entry.getKey() +
			") -> (" + alias + "," + fs_fields.get(i).getFieldName() + ")");

             ColumnInfo colInfo = entry.getValue();
             String old_alias = colInfo.getInternalName();
             String col_alias = fs_fields.get(i).getFieldName();

             /* Note that we also need to change the output column names of the
              * the inlined op, otherwise its children in the query it is inlined
              * into might be able to find the names during execution
              */
             if (outputColNames != null) {
                int pos = outputColNames.indexOf(old_alias);
                if (pos >= 0) {
                   outputColNames.set(pos, col_alias);
                   LOG.info("Inline transform: alter output column name " + old_alias +
			" -> " + col_alias);
                }
             }

             colInfo.setInternalName(col_alias);
             n_map.put(col_alias, colInfo);
             i++;
          }
          dropList.add(tab);
       }

       LOG.info("Inline transform: insert new entries");
       for (Map.Entry<String, ColumnInfo> entry : n_map.entrySet()) {
          inputRR.put(alias, entry.getKey(), entry.getValue());
       }

       LOG.info("Inline transform: drop invalid alias");
       for (String tab : dropList) {
          inputRR.getRslvMap().remove(tab);
       }
    }

    List<Operator<? extends OperatorDesc>> childOps = tsop.getChildOperators();
    for (Operator<? extends OperatorDesc> childOp : childOps) {
       while (! (childOp instanceof SelectOperator)) {
          // Do nothing if RS exists before TS
          if (childOp instanceof ReduceSinkOperator) {
            break;
          }

          if (childOp.getChildOperators().size() == 1) {
             childOp = childOp.getChildOperators().get(0);
          } else {
             throw new SemanticException("Inline fail: " + childOp +
 			" has more than one child");
          }
       }

       if (childOp instanceof SelectOperator) {
          removeTopSelOp(pCtx, (SelectOperator)childOp);
       }
    }

    return true;
  }

  private void removetopopTS(MultiParseContext pctx,Operator<?> op){
    Iterator iter=pctx.getTopOps().entrySet().iterator();
    Object key=null;
    String remove=null;
    Operator<? extends OperatorDesc> value=null;
    while(iter.hasNext()){

      Map.Entry<String, Operator<? extends OperatorDesc>> entry =
	(Map.Entry<String, Operator<? extends OperatorDesc>>)iter.next();
      key=entry.getKey();
      value=entry.getValue();
      if(value.equals(op)){
        remove=(String)key;
      }

    }
    if(remove!=null ){
      pctx.getTopOps().remove(remove);
      pctx.getTopToTable().remove(op);
      pctx.getOpParseCtx().remove(op);
    }
  }

  private void removeTopSelOp(MultiParseContext pctx,Operator<?> op){
    Iterator iter=pctx.getTopSelOps().entrySet().iterator();
    Object key=null;
    String remove=null;
    Operator<? extends OperatorDesc> value=null;
    while(iter.hasNext()){

      Map.Entry<String, Operator<? extends OperatorDesc>> entry =
        (Map.Entry<String, Operator<? extends OperatorDesc>>)iter.next();
      key=entry.getKey();
      value=entry.getValue();
      if(value.equals(op)){
        remove=(String)key;
      }

    }
    if(remove!=null ){
      pctx.getTopSelOps().remove(remove);
    }
  }



  private void InlineWalker() throws SemanticException {
    LOG.info("Inline Walker: Just for validation");

    InlineProcCtx ctx = new InlineProcCtx(pCtx);
    Map<Rule, NodeProcessor> origOpRules = new LinkedHashMap<Rule, NodeProcessor>();
    Dispatcher disp = new DefaultRuleDispatcher(getInlineProc(), origOpRules, ctx);
    GraphWalker ogw = new DefaultGraphWalker(disp);

    List<Node> topNodes = new ArrayList<Node>();
    topNodes.addAll(pCtx.getTopOps().values());
    ogw.startWalking(topNodes, null);
  }

  private NodeProcessor getInlineProc() {
    return new NodeProcessor() {
      @Override
      public Object process(Node nd, Stack<Node> stack,
          NodeProcessorCtx ctx, Object... nodeOutputs) throws SemanticException {
        Operator<? extends OperatorDesc> op = (Operator<? extends OperatorDesc>) nd;
        InlineProcCtx inlineCtx = (InlineProcCtx) ctx;

        LOG.info("Walk to operator " + op);

        RowResolver inputRR = inlineCtx.getRowResolver(op);
        Set<String> aliases = inputRR.getTableNames();
        ObjectInspector[] oi = op.getInputObjInspectors();

        if (aliases.size() == 1 && aliases.contains("")) {
           LOG.info("INLINE_RR: no tab alias");
        }
        LOG.info("Row Schema: " + inputRR.getRowSchema().toString());
        LOG.info("Row Resolvers: " + inputRR.toString());

        if (op instanceof SelectOperator) {
           LOG.info("OutputNames: " +
               ((SelectOperator)op).getConf().getOutputColumnNames());
        } else if (op instanceof ReduceSinkOperator) {
           LOG.info("OutputKeyNames: " +
               ((ReduceSinkOperator)op).getConf().getOutputKeyColumnNames());
           LOG.info("OutputValueNames: " +
                ((ReduceSinkOperator)op).getConf().getOutputValueColumnNames());
        }
        // dump columnExprMap to see if the column name is correct
        Map<String, ExprNodeDesc> colExprMap = op.getColumnExprMap();
        if (colExprMap != null) {
           Set<String> columns = colExprMap.keySet();

           LOG.info("Column expression map: ");
           for (String col : columns) {
              ExprNodeDesc desc = colExprMap.get(col);
              LOG.info("Name: " + col + " Type: " + desc.getTypeString());
           }
        }

        return null;
      }
    };
  }

  protected class InlineProcCtx implements NodeProcessorCtx {

    private int inlineCount;

    private final Map<Operator<? extends OperatorDesc>, OpParseContext> opToParseCtxMap;

    public InlineProcCtx(MultiParseContext pctx) {
      inlineCount = 0;
      opToParseCtxMap = pctx.getOpParseCtx();
    }

    public RowResolver getRowResolver(Node op) {
       return opToParseCtxMap.get(op).getRowResolver();
    }

    public OpParseContext put(Operator<? extends OperatorDesc> key,
        			OpParseContext value) {
       return opToParseCtxMap.put(key, value);
    }

    public void incInlineCnt() { inlineCount++; }
    public int getInlineCnt() { return inlineCount; }
  }

}
