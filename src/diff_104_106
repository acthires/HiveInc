Index: build.properties
===================================================================
--- build.properties	(revision 104)
+++ build.properties	(revision 106)
@@ -75,8 +75,10 @@
 # module names needed for build process
 
 # full profile
-iterate.hive.full.all=ant,shims,common,serde,metastore,ql,contrib,service,cli,jdbc,beeline,hwi,hbase-handler,testutils,hcatalog
-iterate.hive.full.modules=shims,common,serde,metastore,ql,contrib,service,cli,jdbc,beeline,hwi,hbase-handler,testutils,hcatalog
+#iterate.hive.full.all=ql
+iterate.hive.full.all=ant,shims,common,serde,metastore,ql,contrib,service,cli,jdbc,beeline,hwi,hbase-handler,testutils
+#iterate.hive.full.modules=ant,shims,common,serde,metastore,ql,contrib,service,cli,jdbc,beeline,hwi,hbase-handler,testutils
+iterate.hive.full.modules=ql
 iterate.hive.full.tests=ql,contrib,hbase-handler,hwi,jdbc,beeline,metastore,odbc,serde,service,hcatalog
 iterate.hive.full.thrift=ql,service,metastore,serde
 iterate.hive.full.protobuf=ql
Index: build.xml
===================================================================
--- build.xml	(revision 104)
+++ build.xml	(revision 106)
@@ -230,7 +230,7 @@
   <target name="ivy-download" depends="ivy-init-dirs"
           description="To download ivy" unless="offline">
     <echo message="Project: ${ant.project.name}"/>
-    <!--<get src="${ivy_repo_url}" dest="${ivy.jar}" usetimestamp="true"/>-->
+    <get src="${ivy_repo_url}" dest="${ivy.jar}" usetimestamp="true"/>
   </target>
   
   <!--
@@ -409,7 +409,7 @@
     <echo message="Project: ${ant.project.name}"/>
     <!-- preserve the downloaded ivy .jar  -->
     <delete quiet="true" includeemptydirs="true">
-      <fileset dir="${build.dir.hive}" excludes="ivy/**/ivy*.jar"/>
+		<fileset dir="${build.dir.hive}" excludes="ivy/**/*ivy/*"/>
     </delete>
   </target>
 
@@ -1460,5 +1460,4 @@
         output.file="${mvn.pom.dir}/hive-shims-${version}.pom.asc"
         gpg.passphrase="${gpg.passphrase}"/>
   </target>  
-  
 </project>
Index: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
===================================================================
--- metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java	(revision 104)
+++ metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java	(revision 106)
@@ -26,6 +26,10 @@
 import java.net.ServerSocket;
 import java.net.Socket;
 import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Calendar;
+import java.util.Collection;
+import java.util.Date;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
@@ -41,6 +45,7 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.common.JavaUtils;
@@ -1291,4 +1296,23 @@
     return null;
   }
 
+  public static void addPath(FileSystem fs , String path , Collection<Path> retPathList
+      ,Calendar  startDate , Calendar endDate) throws IOException{
+    FileStatus srcs[] = fs.globStatus( new Path(path));
+    Arrays.sort(srcs);
+    for( FileStatus src : srcs ){
+      if( src.isDir()){
+        addPath( fs , path+"/*", retPathList ,startDate ,endDate);
+      }else{
+        long time = src.getModificationTime();
+        Date currentDate = new Date(time);
+        if( currentDate.after(startDate.getTime())  && currentDate.before(endDate.getTime())){
+          retPathList.add(src.getPath());
+        }
+      }
+    }
+  }
+
+
+
 }
Index: build-common.xml
===================================================================
--- build-common.xml	(revision 104)
+++ build-common.xml	(revision 106)
@@ -148,15 +148,13 @@
     <ivy:report todir="${build.ivy.report.dir}" settingsRef="${ant.project.name}.ivy.settings"
                 graph="false" />
   </target>
-
   <target name="ivy-retrieve" depends="ivy-resolve"
     description="Retrieve Ivy-managed artifacts">
     <echo message="Project: ${ant.project.name}"/>
-    <ivy:retrieve settingsRef="${ant.project.name}.ivy.settings"
+	<ivy:retrieve settingsRef="${ant.project.name}.ivy.settings"
       pattern="${build.ivy.lib.dir}/${ivy.artifact.retrieve.pattern}"
-      log="${ivyresolvelog}"/>
+	  log="${ivyresolvelog}"/> 
   </target>
-
   <target name="ivy-resolve-test" depends="ivy-init-settings" unless="offline">
     <echo message="Project: ${ant.project.name}"/>
     <ivy:resolve settingsRef="${ant.project.name}.ivy.settings"
Index: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
===================================================================
--- common/src/java/org/apache/hadoop/hive/conf/HiveConf.java	(revision 104)
+++ common/src/java/org/apache/hadoop/hive/conf/HiveConf.java	(revision 106)
@@ -181,13 +181,16 @@
   public static enum ConfVars {
     // multi-query confuguration variable
     // to determine whether to run multi query , the default value is false
-    HIVEMULTIQUERY("hive.multiquery", true),
+    HIVEMULTIQUERY("hive.multiquery", false),
     HIVEMULTIEXPLAIN("hive.multiexplain", false),
     // to limit accept max query numbers
     HIVEMULTIQUERYNUM("hive.multiquerynum", 15),
     HIVEINC("hive.inc",false),
     HIVEINCTABLE("hive.inc.table","lineitem"),
-    HIVEINCTESTAST("hive.inc.testast",false),
+    HIVEINCEXTRACTTMP("hive.inc.extracttmp",true),
+    // to determine whether to run inc query , the default value is false
+    HIVEINCQUERY("hive.incquery", true),
+
     // QL execution stuff
     SCRIPTWRAPPER("hive.exec.script.wrapper", null),
     PLAN("hive.exec.plan", ""),
Index: common/src/gen/org/apache/hive/common/package-info.java
===================================================================
--- common/src/gen/org/apache/hive/common/package-info.java	(revision 104)
+++ common/src/gen/org/apache/hive/common/package-info.java	(revision 106)
@@ -2,6 +2,6 @@
  * Generated by saveVersion.sh
  */
 @HiveVersionAnnotation(version="0.12.0", revision="89", branch="branches/kangyanli/hive-0.12.0/src",
-                         user="kangyanli", date="Tue Jan 6 15:40:59 CST 2015", url="svn://10.3.0.220/hive/branches/kangyanli/hive-0.12.0/src",
-                         srcChecksum="6224905633a1acc4ea24ac23e0012431")
+                         user="kangyanli", date="Tue Jan 13 18:55:35 CST 2015", url="svn://10.3.0.220/hive/branches/kangyanli/hive-0.12.0/src",
+                         srcChecksum="ff26bb405bfbdd4dfe0dfbf11592e2c5")
 package org.apache.hive.common;
Index: cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java
===================================================================
--- cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java	(revision 104)
+++ cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java	(revision 106)
@@ -53,6 +53,7 @@
 import org.apache.hadoop.hive.metastore.api.Schema;
 import org.apache.hadoop.hive.ql.CommandNeedRetryException;
 import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.IncDriver;
 import org.apache.hadoop.hive.ql.MultiDriver;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.exec.Utilities;
@@ -317,7 +318,22 @@
             console.printInfo("Time taken: " + timeTaken + " seconds" +
                 (counter == 0 ? "" : ", Fetched: " + counter + " row(s)"));
 
-          } else {
+          } else if (proc instanceof IncDriver) {
+            IncDriver qp = (IncDriver) proc;
+            PrintStream out = ss.out;
+            if (ss.getIsVerbose()) {
+              out.println(cmd);
+            }
+
+            qp.setTryCount(tryCount);
+            ret = qp.run(cmd).getResponseCode();
+            int cret = qp.close();
+            if (ret == 0) {
+              ret = cret;
+            }
+
+
+          }else {
             String firstToken = tokenizeCmd(cmd.trim())[0];
             String cmd_1 = getFirstCmd(cmd.trim(), firstToken.length());
 
Index: ql/src/java/org/apache/hadoop/hive/ql/MultiDriver.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/MultiDriver.java	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/MultiDriver.java	(revision 106)
@@ -691,278 +691,6 @@
     }
   }
 
-  /**
-   * Compile a new query, but potentially reset taskID counter. Not resetting task counter
-   * is useful for generating re-entrant QL queries.
-   *
-   * @param command
-   *          The HiveQL query to compile
-   * @param resetTaskIds
-   *          Resets taskID counter if true.
-   * @return 0 for ok
-   */
-  public int incrementalCompile(ArrayList<Pair<String, Configuration>> multiCmds, boolean resetTaskIds) {
-    PerfLogger perfLogger = PerfLogger.getPerfLogger();
-    perfLogger.PerfLogBegin(LOG, PerfLogger.MULTICOMPILE);
-
-    // holder for parent command type/string when executing reentrant queries
-    QueryState queryState = new QueryState();
-
-    if (plan != null) {
-      close();
-      plan = null;
-    }
-
-    if (resetTaskIds) {
-      TaskFactory.resetId();
-    }
-    saveSession(queryState);
-    SemanticAnalyzer sem = null;
-    try {
-
-      int querysize = 2;
-
-      Pair<String, Configuration> cmdConf = multiCmds.get(0);
-      String command = cmdConf.getKey();
-      cmds.add(command);
-      conf = (HiveConf) cmdConf.getValue();
-      hconfs.add(conf);
-      command = new VariableSubstitution().substitute(conf, command);
-      ctx = new Context(conf);
-      ctx.setTryCount(getTryCount());
-      ctx.setCmd(command);
-      ctx.setHDFSCleanup(true);
-
-      perfLogger.PerfLogBegin(LOG, PerfLogger.MULTIPARSE);
-      ParseDriver pd = new ParseDriver();
-      ASTNode tree = pd.parse(command, ctx);
-      tree = ParseUtils.findRootNonNullToken(tree);
-      perfLogger.PerfLogEnd(LOG, PerfLogger.MULTIPARSE);
-
-      perfLogger.PerfLogBegin(LOG, PerfLogger.MULTIANALYZE1);
-
-      LOG.info("Incremental Query Semantic Analysis DoPhase1 Start!");
-      ArrayList<QB> qblist = null;
-
-      sem = new SemanticAnalyzer(conf);
-      sem.initSem(ctx);
-      qblist = sem.analyzeIncPhase1(tree);
-
-      //multiPctx.put(i, sem.analyzePhase1(tree));
-
-
-
-      LOG.info("Incremental Query Semantic Analysis DoPhase1 Completed");
-
-
-      for (int i = 0; i < qblist.size(); i++) {
-        if(i != 0){
-        sem = new SemanticAnalyzer(conf);
-        }
-        List<HiveSemanticAnalyzerHook> saHooks =
-            getHooks(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK,
-                HiveSemanticAnalyzerHook.class);
-        // Do semantic analysis and plan generation
-        if (saHooks != null) {
-          HiveSemanticAnalyzerHookContext hookCtx = new HiveSemanticAnalyzerHookContextImpl();
-          hookCtx.setConf(conf);
-          for (HiveSemanticAnalyzerHook hook : saHooks) {
-            tree = hook.preAnalyze(hookCtx, tree);
-          }
-          // do ananlyze to get pctx and store in multiPctx
-          sem.initSem(ctx);
-          multiPctx.put(i, sem.analyzeIncPhase2(tree, qblist.get(i)));
-
-          hookCtx.update(sem);
-          for (HiveSemanticAnalyzerHook hook : saHooks) {
-            hook.postAnalyze(hookCtx, sem.getRootTasks());
-          }
-        } else {
-          sem.initSem(ctx);
-          multiPctx.put(i, sem.analyzeIncPhase2(tree, qblist.get(i)));
-        }
-
-        LOG.info("Incremental Query Semantic Analysis genPlan Completed");
-
-        // validate the plan
-        sem.validate();
-        perfLogger.PerfLogEnd(LOG, PerfLogger.MULTIANALYZE1);
-
-        // get the output schema
-        schemas.add(getSchema(sem, conf));
-
-
-        // do the authorization check
-        if (HiveConf.getBoolVar(conf,
-            HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED)) {
-          try {
-            perfLogger.PerfLogBegin(LOG, PerfLogger.DO_AUTHORIZATION);
-            doAuthorization(sem);
-          } catch (AuthorizationException authExp) {
-            errorMessage = "Authorization failed:" + authExp.getMessage()
-                + ". Use show grant to get more details.";
-            console.printError(errorMessage);
-            return 403;
-          } finally {
-            perfLogger.PerfLogEnd(LOG, PerfLogger.DO_AUTHORIZATION);
-          }
-        }
-      }
-
-      // Optree output
-      LOG.info("======Print Incremental Query  Optree After sem.analyzePhase1 =======");
-      for (int key = 0; key < multiPctx.size(); key++) {
-        ParseContext pCtx = multiPctx.get(key);
-        //String cmd = cmds.get(key);
-        LOG.info("The " + key + "st  Query:");
-       // LOG.info("Command:\t " + cmd);
-        LOG.info("OPtree:\t " + Operator.toString(pCtx.getTopOps().values()));
-      }
-      LOG.info("======Print Incremental Query  Optree After sem.analyzePhase1 =======");
-
-      //Set Explain
-      //set the  explain the query plan stmt variables
-      boolean extended = false;
-      boolean formatted = false;
-      boolean dependency = false;
-      boolean logical = false;
-      if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVEMULTIEXPLAIN)) {
-         ctx.setExplain(true);
-      }
-      ctx.setExplainLogical(logical);
-
-      //merge multipctx
-      MultiParseContext multipCtx;
-      multipCtx=mergePctx();
-
-      //log print
-      LOG.info("Optree After mergePctx:\n" + Operator.toString(multipCtx.getTopOps().values()));
-
-      // do InterQueryFlow analysis
-      // set InterQueryFlowCtx class's result to multipCtx
-      InterQueryFlowCtx queryFlowCtx = new InterQueryFlowCtx( conf, this.multiPctx);
-      queryFlowCtx.multiQueryFlowAnalysis();
-      multipCtx.setQueryFlowCtx(queryFlowCtx);
-
-      // Common-sub-tree
-      if(HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVEOPTCSQ)){
-        LOG.info("======Print merge-common-sub-tree test=======");
-        CommonSubtreeReuse reuseSubtree = new CommonSubtreeReuse(multiPctx, multipCtx);
-        reuseSubtree.subtreeReuse();
-      }
-
-      //sem2: do optimization and gen MR task
-      SemanticAnalyzer sem2;
-      sem2 = new SemanticAnalyzer(conf);
-      sem2.analyzePhase2(multipCtx);
-      sem2.validate();
-
-
-      if (ctx.getExplain()) {
-        ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));
-        List<Task<? extends Serializable>> tasks = sem2.getRootTasks();
-        //Task<? extends Serializable> fetchTask = sem.getFetchTask();
-        HashMap<Integer,FetchTask> FetchTaskList=sem2.getfetchtasklist();
-
-        if (tasks == null) {
-          if (FetchTaskList != null) {
-            tasks = new ArrayList<Task<? extends Serializable>>();
-            for(int i=0;i<multiPctx.size();i++) {
-              if( FetchTaskList.get(i)!=null){
-                tasks.add(FetchTaskList.get(i));
-              }
-            }
-          }
-        } else{
-          if (FetchTaskList != null) {
-            for(int i=0;i<multiPctx.size();i++) {
-              if( FetchTaskList.get(i)!=null){
-                tasks.add(FetchTaskList.get(i));
-              }
-            }
-          }
-
-        }
-
-        Task<? extends Serializable> explTask =
-            TaskFactory.get(new ExplainWork(ctx.getResFile().toString(),
-            multipCtx,
-            tasks,
-          //  ((ASTNode) ast.getChild(0)).toStringTree(),
-            null,
-            sem2.getInputs(),
-            extended,
-            formatted,
-            dependency,
-            logical),
-          conf);
-        List<FieldSchema> lst = explTask.getResultSchema();
-        schema = new Schema(lst, null);
-       // sem2.getRootTasks().clear();
-        sem2.getRootTasks().add(explTask);
-
-      }
-
-      plan= new MultiQueryPlan("Multiquery:"+cmds.get(0),sem2, perfLogger.getStartTime(PerfLogger.DRIVER_RUN));
-
-
-      // initialize Multi-FetchTask List right here
-      if(((MultiQueryPlan) plan).getfetchtasklist()!=null){
-        HashMap<Integer,FetchTask> FetchTaskList=((MultiQueryPlan) plan).getfetchtasklist();
-        int i;
-        for(i=0;i<multiPctx.size();i++) {
-          if( FetchTaskList.get(i)!=null){
-            FetchTaskList.get(i).initialize(conf, plan, null);
-          }
-        }
-      }
-
-      /*
-      if(sem != null){
-        sem.analyzeMultiQuery();
-        sem.analyzePhase2();
-      }
-      */
-      perfLogger.PerfLogBegin(LOG, PerfLogger.PREOPTTEST);
-      //int ret=0;
-      //try {
-      //multipreoptimizetest is for testing
-      //it read each pctx in the multipctx and execute one by one
-      //TODO it should not be called when the multiquery execute  normally
-      //    ret=multipreoptimizetest();
-      //}
-      //catch (CommandNeedRetryException e) {
-      // TODO Auto-generated catch block
-      //    e.printStackTrace();
-      //}
-      perfLogger.PerfLogEnd(LOG, PerfLogger.PREOPTTEST);
-      return 0;
-    } catch (Exception e) {
-      ErrorMsg error = ErrorMsg.getErrorMsg(e.getMessage());
-      errorMessage = "FAILED: " + e.getClass().getSimpleName();
-      if (error != ErrorMsg.GENERIC_ERROR) {
-        errorMessage += " [Error " + error.getErrorCode() + "]:";
-      }
-
-      // HIVE-4889
-      if ((e instanceof IllegalArgumentException) && e.getMessage() == null && e.getCause() != null) {
-        errorMessage += " " + e.getCause().getMessage();
-      } else {
-        errorMessage += " " + e.getMessage();
-      }
-
-      SQLState = error.getSQLState();
-      downstreamError = e;
-      console.printError(errorMessage, "\n"
-          + org.apache.hadoop.util.StringUtils.stringifyException(e));
-      return error.getErrorCode();
-    } finally {
-      perfLogger.PerfLogEnd(LOG, PerfLogger.MULTICOMPILE);
-      restoreSession(queryState);
-    }
-  }
-
-
   public static  void addbottomfakeoperator(MultiParseContext multipCtx,HashSet<Integer> set) {
     // TODO Auto-generated method stub
     List<Operator<? extends OperatorDesc>> oplistFS = new ArrayList<Operator<? extends OperatorDesc>>() ;
Index: ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java	(revision 106)
@@ -306,7 +306,6 @@
   }
 
   public static ExprNodeDesc backtrackToTS(ExprNodeDesc value, Operator current) {
-    // TODO Auto-generated method stub
     List<Operator> bottomRS = new ArrayList<Operator>();
     getBottomRSTS(current,bottomRS);
       try {
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/FromClauseParser.g
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/FromClauseParser.g	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/FromClauseParser.g	(revision 106)
@@ -176,8 +176,8 @@
 tableSource
 @init { gParent.msgs.push("table source"); }
 @after { gParent.msgs.pop(); }
-    : tabname=tableName (props=tableProperties)? (ts=tableSample)? (KW_AS? alias=Identifier)?
-    -> ^(TOK_TABREF $tabname $props? $ts? $alias?)
+    : tabname=tableName (props=tableProperties)? (ts=tableSample)? (ic=incrementalClause)?(KW_AS? alias=Identifier)?
+    -> ^(TOK_TABREF $tabname $props? $ts?  $ic? $alias?)
     ;
 
 tableName
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/IdentifiersParser.g
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/IdentifiersParser.g	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/IdentifiersParser.g	(revision 106)
@@ -222,8 +222,7 @@
 @init { gParent.msgs.push("constant"); }
 @after { gParent.msgs.pop(); }
     :
-    Number
-    | dateLiteral
+     Number
     | StringLiteral
     | stringLiteralSequence
     | BigintLiteral
@@ -525,5 +524,5 @@
     
 nonReserved
     :
-    KW_TRUE | KW_FALSE | KW_LIKE | KW_EXISTS | KW_ASC | KW_DESC | KW_ORDER | KW_GROUP | KW_BY | KW_AS | KW_INSERT | KW_OVERWRITE | KW_OUTER | KW_LEFT | KW_RIGHT | KW_FULL | KW_PARTITION | KW_PARTITIONS | KW_TABLE | KW_TABLES | KW_COLUMNS | KW_INDEX | KW_INDEXES | KW_REBUILD | KW_FUNCTIONS | KW_SHOW | KW_MSCK | KW_REPAIR | KW_DIRECTORY | KW_LOCAL | KW_USING | KW_CLUSTER | KW_DISTRIBUTE | KW_SORT | KW_UNION | KW_LOAD | KW_EXPORT | KW_IMPORT | KW_DATA | KW_INPATH | KW_IS | KW_NULL | KW_CREATE | KW_EXTERNAL | KW_ALTER | KW_CHANGE | KW_FIRST | KW_AFTER | KW_DESCRIBE | KW_DROP | KW_RENAME | KW_IGNORE | KW_PROTECTION | KW_TO | KW_COMMENT | KW_BOOLEAN | KW_TINYINT | KW_SMALLINT | KW_INT | KW_BIGINT | KW_FLOAT | KW_DOUBLE | KW_DATE | KW_DATETIME | KW_TIMESTAMP | KW_DECIMAL | KW_STRING | KW_ARRAY | KW_STRUCT | KW_UNIONTYPE | KW_PARTITIONED | KW_CLUSTERED | KW_SORTED | KW_INTO | KW_BUCKETS | KW_ROW | KW_ROWS | KW_FORMAT | KW_DELIMITED | KW_FIELDS | KW_TERMINATED | KW_ESCAPED | KW_COLLECTION | KW_ITEMS | KW_KEYS | KW_KEY_TYPE | KW_LINES | KW_STORED | KW_FILEFORMAT | KW_SEQUENCEFILE | KW_TEXTFILE | KW_RCFILE | KW_ORCFILE | KW_INPUTFORMAT | KW_OUTPUTFORMAT | KW_INPUTDRIVER | KW_OUTPUTDRIVER | KW_OFFLINE | KW_ENABLE | KW_DISABLE | KW_READONLY | KW_NO_DROP | KW_LOCATION | KW_BUCKET | KW_OUT | KW_OF | KW_PERCENT | KW_ADD | KW_REPLACE | KW_RLIKE | KW_REGEXP | KW_TEMPORARY | KW_EXPLAIN | KW_FORMATTED | KW_PRETTY | KW_DEPENDENCY | KW_LOGICAL | KW_SERDE | KW_WITH | KW_DEFERRED | KW_SERDEPROPERTIES | KW_DBPROPERTIES | KW_LIMIT | KW_SET | KW_UNSET | KW_TBLPROPERTIES | KW_IDXPROPERTIES | KW_VALUE_TYPE | KW_ELEM_TYPE | KW_MAPJOIN | KW_STREAMTABLE | KW_HOLD_DDLTIME | KW_CLUSTERSTATUS | KW_UTC | KW_UTCTIMESTAMP | KW_LONG | KW_DELETE | KW_PLUS | KW_MINUS | KW_FETCH | KW_INTERSECT | KW_VIEW | KW_IN | KW_DATABASES | KW_MATERIALIZED | KW_SCHEMA | KW_SCHEMAS | KW_GRANT | KW_REVOKE | KW_SSL | KW_UNDO | KW_LOCK | KW_LOCKS | KW_UNLOCK | KW_SHARED | KW_EXCLUSIVE | KW_PROCEDURE | KW_UNSIGNED | KW_WHILE | KW_READ | KW_READS | KW_PURGE | KW_RANGE | KW_ANALYZE | KW_BEFORE | KW_BETWEEN | KW_BOTH | KW_BINARY | KW_CONTINUE | KW_CURSOR | KW_TRIGGER | KW_RECORDREADER | KW_RECORDWRITER | KW_SEMI | KW_LATERAL | KW_TOUCH | KW_ARCHIVE | KW_UNARCHIVE | KW_COMPUTE | KW_STATISTICS | KW_USE | KW_OPTION | KW_CONCATENATE | KW_SHOW_DATABASE | KW_UPDATE | KW_RESTRICT | KW_CASCADE | KW_SKEWED | KW_ROLLUP | KW_CUBE | KW_DIRECTORIES | KW_FOR | KW_GROUPING | KW_SETS | KW_TRUNCATE | KW_NOSCAN | KW_USER | KW_ROLE | KW_INNER
+     KW_INCRE | KW_TRUE | KW_FALSE | KW_LIKE | KW_EXISTS | KW_ASC | KW_DESC | KW_ORDER | KW_GROUP | KW_BY | KW_AS | KW_INSERT | KW_OVERWRITE | KW_OUTER | KW_LEFT | KW_RIGHT | KW_FULL | KW_PARTITION | KW_PARTITIONS | KW_TABLE | KW_TABLES | KW_COLUMNS | KW_INDEX | KW_INDEXES | KW_REBUILD | KW_FUNCTIONS | KW_SHOW | KW_MSCK | KW_REPAIR | KW_DIRECTORY | KW_LOCAL | KW_USING | KW_CLUSTER | KW_DISTRIBUTE | KW_SORT | KW_UNION | KW_LOAD | KW_EXPORT | KW_IMPORT | KW_DATA | KW_INPATH | KW_IS | KW_NULL | KW_CREATE | KW_EXTERNAL | KW_ALTER | KW_CHANGE | KW_FIRST | KW_AFTER | KW_DESCRIBE | KW_DROP | KW_RENAME | KW_IGNORE | KW_PROTECTION | KW_TO | KW_COMMENT | KW_BOOLEAN | KW_TINYINT | KW_SMALLINT | KW_INT | KW_BIGINT | KW_FLOAT | KW_DOUBLE | KW_DATE | KW_DATETIME | KW_TIMESTAMP | KW_DECIMAL | KW_STRING | KW_ARRAY | KW_STRUCT | KW_UNIONTYPE | KW_PARTITIONED | KW_CLUSTERED | KW_SORTED | KW_INTO | KW_BUCKETS | KW_ROW | KW_ROWS | KW_FORMAT | KW_DELIMITED | KW_FIELDS | KW_TERMINATED | KW_ESCAPED | KW_COLLECTION | KW_ITEMS | KW_KEYS | KW_KEY_TYPE | KW_LINES | KW_STORED | KW_FILEFORMAT | KW_SEQUENCEFILE | KW_TEXTFILE | KW_RCFILE | KW_ORCFILE | KW_INPUTFORMAT | KW_OUTPUTFORMAT | KW_INPUTDRIVER | KW_OUTPUTDRIVER | KW_OFFLINE | KW_ENABLE | KW_DISABLE | KW_READONLY | KW_NO_DROP | KW_LOCATION | KW_BUCKET | KW_OUT | KW_OF | KW_PERCENT | KW_ADD | KW_REPLACE | KW_RLIKE | KW_REGEXP | KW_TEMPORARY | KW_EXPLAIN | KW_FORMATTED | KW_PRETTY | KW_DEPENDENCY | KW_LOGICAL | KW_SERDE | KW_WITH | KW_DEFERRED | KW_SERDEPROPERTIES | KW_DBPROPERTIES | KW_LIMIT | KW_SET | KW_UNSET | KW_TBLPROPERTIES | KW_IDXPROPERTIES | KW_VALUE_TYPE | KW_ELEM_TYPE | KW_MAPJOIN |  KW_STREAMTABLE | KW_HOLD_DDLTIME | KW_CLUSTERSTATUS | KW_UTC | KW_UTCTIMESTAMP | KW_LONG | KW_DELETE | KW_PLUS | KW_MINUS | KW_FETCH | KW_INTERSECT | KW_VIEW | KW_IN | KW_DATABASES | KW_MATERIALIZED | KW_SCHEMA | KW_SCHEMAS | KW_GRANT | KW_REVOKE | KW_SSL | KW_UNDO | KW_LOCK | KW_LOCKS | KW_UNLOCK | KW_SHARED | KW_EXCLUSIVE | KW_PROCEDURE | KW_UNSIGNED | KW_WHILE | KW_READ | KW_READS | KW_PURGE | KW_RANGE | KW_ANALYZE | KW_BEFORE | KW_BETWEEN | KW_BOTH | KW_BINARY | KW_CONTINUE | KW_CURSOR | KW_TRIGGER | KW_RECORDREADER | KW_RECORDWRITER | KW_SEMI | KW_LATERAL | KW_TOUCH | KW_ARCHIVE | KW_UNARCHIVE | KW_COMPUTE | KW_STATISTICS | KW_USE | KW_OPTION | KW_CONCATENATE | KW_SHOW_DATABASE | KW_UPDATE | KW_RESTRICT | KW_CASCADE | KW_SKEWED | KW_ROLLUP | KW_CUBE | KW_DIRECTORIES | KW_FOR | KW_GROUPING | KW_SETS | KW_TRUNCATE | KW_NOSCAN | KW_USER | KW_ROLE | KW_INNER
     ;
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java	(revision 106)
@@ -37,7 +37,6 @@
 import org.antlr.runtime.tree.Tree;
 import org.antlr.runtime.tree.TreeWizard;
 import org.antlr.runtime.tree.TreeWizard.ContextVisitor;
-import org.apache.commons.collections.CollectionUtils;
 import org.apache.commons.lang.StringUtils;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.common.FileUtils;
@@ -50,9 +49,7 @@
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.Order;
-import org.apache.hadoop.hive.ql.CommandNeedRetryException;
 import org.apache.hadoop.hive.ql.Context;
-import org.apache.hadoop.hive.ql.Driver;
 import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.QueryProperties;
 import org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator;
@@ -185,55 +182,55 @@
 
 
 public class SemanticAnalyzer extends BaseSemanticAnalyzer {
-  private HashMap<TableScanOperator, ExprNodeDesc> opToPartPruner;
-  private HashMap<TableScanOperator, PrunedPartitionList> opToPartList;
-  private HashMap<String, Operator<? extends OperatorDesc>> topOps;
-  private HashMap<String, Operator<? extends OperatorDesc>> topSelOps;
-  private LinkedHashMap<Operator<? extends OperatorDesc>, OpParseContext> opParseCtx;
-  private List<LoadTableDesc> loadTableWork;
-  private List<LoadFileDesc> loadFileWork;
-  private Map<JoinOperator, QBJoinTree> joinContext;
-  private Map<SMBMapJoinOperator, QBJoinTree> smbMapJoinContext;
-  private final HashMap<TableScanOperator, Table> topToTable;
-  private final Map<FileSinkOperator, Table> fsopToTable;
-  private final List<ReduceSinkOperator> reduceSinkOperatorsAddedByEnforceBucketingSorting;
-  private final HashMap<TableScanOperator, Map<String, String>> topToTableProps;
-  private QB qb;
-  private ASTNode ast;
-  private int destTableId;
-  private UnionProcContext uCtx;
+  protected HashMap<TableScanOperator, ExprNodeDesc> opToPartPruner;
+  protected HashMap<TableScanOperator, PrunedPartitionList> opToPartList;
+  protected HashMap<String, Operator<? extends OperatorDesc>> topOps;
+  protected HashMap<String, Operator<? extends OperatorDesc>> topSelOps;
+  protected LinkedHashMap<Operator<? extends OperatorDesc>, OpParseContext> opParseCtx;
+  protected List<LoadTableDesc> loadTableWork;
+  protected List<LoadFileDesc> loadFileWork;
+  protected Map<JoinOperator, QBJoinTree> joinContext;
+  protected Map<SMBMapJoinOperator, QBJoinTree> smbMapJoinContext;
+  protected final HashMap<TableScanOperator, Table> topToTable;
+  protected final Map<FileSinkOperator, Table> fsopToTable;
+  protected final List<ReduceSinkOperator> reduceSinkOperatorsAddedByEnforceBucketingSorting;
+  protected final HashMap<TableScanOperator, Map<String, String>> topToTableProps;
+  protected QB qb;
+  protected ASTNode ast;
+  protected int destTableId;
+  protected UnionProcContext uCtx;
   List<AbstractMapJoinOperator<? extends MapJoinDesc>> listMapJoinOpsNoReducer;
-  private HashMap<TableScanOperator, sampleDesc> opToSamplePruner;
-  private final Map<TableScanOperator, Map<String, ExprNodeDesc>> opToPartToSkewedPruner;
+  protected HashMap<TableScanOperator, sampleDesc> opToSamplePruner;
+  protected final Map<TableScanOperator, Map<String, ExprNodeDesc>> opToPartToSkewedPruner;
   //for multiquery
-  private HashMap<Integer,FetchTask> FetchTaskList;
+  protected HashMap<Integer,FetchTask> FetchTaskList;
   /**
    * a map for the split sampling, from ailias to an instance of SplitSample
    * that describes percentage and number.
    */
-  private final HashMap<String, SplitSample> nameToSplitSample;
+  protected final HashMap<String, SplitSample> nameToSplitSample;
   Map<GroupByOperator, Set<String>> groupOpToInputTables;
   Map<String, PrunedPartitionList> prunedPartitions;
-  private List<FieldSchema> resultSchema;
-  private CreateViewDesc createVwDesc;
-  private ArrayList<String> viewsExpanded;
-  private ASTNode viewSelect;
-  private final UnparseTranslator unparseTranslator;
-  private final GlobalLimitCtx globalLimitCtx = new GlobalLimitCtx();
+  protected List<FieldSchema> resultSchema;
+  protected CreateViewDesc createVwDesc;
+  protected ArrayList<String> viewsExpanded;
+  protected ASTNode viewSelect;
+  protected final UnparseTranslator unparseTranslator;
+  protected final GlobalLimitCtx globalLimitCtx = new GlobalLimitCtx();
 
   // prefix for column names auto generated by hive
-  private final String autogenColAliasPrfxLbl;
-  private final boolean autogenColAliasPrfxIncludeFuncName;
+  protected final String autogenColAliasPrfxLbl;
+  protected final boolean autogenColAliasPrfxIncludeFuncName;
 
   // Keep track of view alias to read entity corresponding to the view
   // For eg: for a query like 'select * from V3', where V3 -> V2, V2 -> V1, V1 -> T
   // keeps track of aliases for V3, V3:V2, V3:V2:V1.
   // This is used when T is added as an input for the query, the parents of T is
   // derived from the alias V3:V2:V1:T
-  private final Map<String, ReadEntity> viewAliasToInput = new HashMap<String, ReadEntity>();
+  protected final Map<String, ReadEntity> viewAliasToInput = new HashMap<String, ReadEntity>();
 
   // Max characters when auto generating the column name with func name
-  private static final int AUTOGEN_COLALIAS_PRFX_MAXLENGTH = 20;
+  protected static final int AUTOGEN_COLALIAS_PRFX_MAXLENGTH = 20;
 
   // flag for no scan during analyze ... compute statistics
   protected boolean noscan = false;
@@ -243,7 +240,7 @@
 
   // to store Pctx of each query
 
-  private static class Phase1Ctx {
+  protected static class Phase1Ctx {
     String dest;
     int nextNum;
     String cachetable;
@@ -254,16 +251,16 @@
   private final List<ReduceSinkOperator> RStmpPos = new ArrayList<ReduceSinkOperator>();
   private final List<ASTNode> ASTtmpPos = new ArrayList<ASTNode>();
   private final String crtTmpTablename = "tmp";
-  private int incPos = -1;
+  private final int incPos = -1;
   private final String subQueryName = "incTmpTable";
   private final List<ASTNode> addedSubQuery = new ArrayList<ASTNode>();
   boolean isCreateTmpPhase = false;
-  private String incTable = ((HiveConf) conf).getVar(HiveConf.ConfVars.HIVEINCTABLE);
+  private final String incTable = ((HiveConf) conf).getVar(HiveConf.ConfVars.HIVEINCTABLE);
   private final List<String> topAliasInTmp = new ArrayList<String>();
-  private boolean isInsertSubQuery = false;
+  private final boolean isInsertSubQuery = false;
   private final HashMap<String, String> aliasToTabs = new HashMap<String,String>();
   private final HashMap<String[],String> colNameMap = new HashMap<String[],String>();//map from [tabName,colName] to newColName in subQuery, avoid same columnName of different table
-  private ParseContext pCtxInc = null;
+  private final ParseContext pCtxInc = null;
   private final List<String> tmpTableCols = new ArrayList<String>();
 
   private static class IncCtx {
@@ -391,7 +388,7 @@
     }
   }
 
-  private LinkedHashMap<String, ASTNode> doPhase1GetAggregationsFromSelect(
+  protected LinkedHashMap<String, ASTNode> doPhase1GetAggregationsFromSelect(
       ASTNode selExpr, QB qb, String dest) throws SemanticException {
 
     // Iterate over the selects search for aggregation Trees.
@@ -428,7 +425,7 @@
     return aggregationTrees;
   }
 
-  private void doPhase1GetColumnAliasesFromSelect(
+  protected void doPhase1GetColumnAliasesFromSelect(
       ASTNode selectExpr, QBParseInfo qbp) {
     for (int i = 0; i < selectExpr.getChildCount(); ++i) {
       ASTNode selExpr = (ASTNode) selectExpr.getChild(i);
@@ -450,7 +447,7 @@
    *          the aggregation subtree.
    * @throws SemanticException
    */
-  private void doPhase1GetAllAggregations(ASTNode expressionTree,
+  protected void doPhase1GetAllAggregations(ASTNode expressionTree,
       HashMap<String, ASTNode> aggregations, List<ASTNode> wdwFns) throws SemanticException {
     int exprTokenType = expressionTree.getToken().getType();
     if (exprTokenType == HiveParser.TOK_FUNCTION
@@ -488,55 +485,8 @@
     }
   }
 
-  private void doPhase1IncGetAllAggregations(ASTNode expressionTree,
-      HashMap<String, ASTNode> aggregations, List<ASTNode> wdwFns) throws SemanticException {
-    int exprTokenType = expressionTree.getToken().getType();
-    if (exprTokenType == HiveParser.TOK_FUNCTION
-        || exprTokenType == HiveParser.TOK_FUNCTIONSTAR) {
-      assert (expressionTree.getChildCount() != 0);
-      if (expressionTree.getChild(expressionTree.getChildCount()-1).getType()
-          == HiveParser.TOK_WINDOWSPEC) {
-        wdwFns.add(expressionTree);
-        return;
-      }
-      if (expressionTree.getChild(0).getType() == HiveParser.Identifier) {
-        String functionName = unescapeIdentifier(expressionTree.getChild(0)
-            .getText());
-        if(FunctionRegistry.impliesOrder(functionName)) {
-          throw new SemanticException(ErrorMsg.MISSING_OVER_CLAUSE.getMsg(functionName));
-        }
-        if (FunctionRegistry.getGenericUDAFResolver(functionName) != null) {
-          if(containsLeadLagUDF(expressionTree)) {
-            throw new SemanticException(ErrorMsg.MISSING_OVER_CLAUSE.getMsg(functionName));
-          }
-          aggregations.put(expressionTree.toStringTree(), expressionTree);
-          FunctionInfo fi = FunctionRegistry.getFunctionInfo(functionName);
-          if (!fi.isNative()) {
-            unparseTranslator.addIdentifierTranslation((ASTNode) expressionTree
-                .getChild(0));
-          }
-          return;
-        }
-      }
-    }
-    if (exprTokenType == HiveParser.TOK_FUNCTIONDI){
-      assert (expressionTree.getChildCount() != 0);
-      if (expressionTree.getChild(expressionTree.getChildCount()-1).getType()
-          == HiveParser.TOK_WINDOWSPEC) {
-        wdwFns.add(expressionTree);
-        return;
-      }
 
-      //TOdO: add the distinct handle
-
-    }
-    for (int i = 0; i < expressionTree.getChildCount(); i++) {
-      doPhase1IncGetAllAggregations((ASTNode) expressionTree.getChild(i),
-          aggregations, wdwFns);
-    }
-  }
-
-  private List<ASTNode> doPhase1GetDistinctFuncExprs(
+  protected List<ASTNode> doPhase1GetDistinctFuncExprs(
       HashMap<String, ASTNode> aggregationTrees) throws SemanticException {
     List<ASTNode> exprs = new ArrayList<ASTNode>();
     for (Map.Entry<String, ASTNode> entry : aggregationTrees.entrySet()) {
@@ -549,7 +499,7 @@
     return exprs;
   }
 
-  public static String generateErrorMessage(ASTNode ast, String message) {
+  protected static String generateErrorMessage(ASTNode ast, String message) {
     StringBuilder sb = new StringBuilder();
     sb.append(ast.getLine());
     sb.append(":");
@@ -569,7 +519,7 @@
    *
    * @return the alias of the table
    */
-  private String processTable(QB qb, ASTNode tabref) throws SemanticException {
+  protected String processTable(QB qb, ASTNode tabref) throws SemanticException {
     // For each table reference get the table name
     // and the alias (if alias is not present, the table name
     // is used as an alias)
@@ -693,7 +643,7 @@
     return alias;
   }
 
-  private void assertCombineInputFormat(Tree numerator, String message) throws SemanticException {
+  protected void assertCombineInputFormat(Tree numerator, String message) throws SemanticException {
     String inputFormat = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEINPUTFORMAT);
     if (!inputFormat.equals(CombineHiveInputFormat.class.getName())) {
       throw new SemanticException(generateErrorMessage((ASTNode) numerator,
@@ -701,7 +651,7 @@
     }
   }
 
-  private String processSubQuery(QB qb, ASTNode subq) throws SemanticException {
+  protected String processSubQuery(QB qb, ASTNode subq) throws SemanticException {
 
     // This is a subquery and must have an alias
     if (subq.getChildCount() != 2) {
@@ -729,7 +679,7 @@
     return alias;
   }
 
-  private boolean isJoinToken(ASTNode node) {
+  protected boolean isJoinToken(ASTNode node) {
     if ((node.getToken().getType() == HiveParser.TOK_JOIN)
         || (node.getToken().getType() == HiveParser.TOK_CROSSJOIN)
         || (node.getToken().getType() == HiveParser.TOK_LEFTOUTERJOIN)
@@ -752,7 +702,7 @@
    * @throws SemanticException
    */
   @SuppressWarnings("nls")
-  private void processJoin(QB qb, ASTNode join) throws SemanticException {
+  protected void processJoin(QB qb, ASTNode join) throws SemanticException {
     int numChildren = join.getChildCount();
     if ((numChildren != 2) && (numChildren != 3)
         && join.getToken().getType() != HiveParser.TOK_UNIQUEJOIN) {
@@ -802,7 +752,7 @@
    * @throws SemanticException
    */
 
-  private String processLateralView(QB qb, ASTNode lateralView)
+  protected String processLateralView(QB qb, ASTNode lateralView)
       throws SemanticException {
     int numChildren = lateralView.getChildCount();
 
@@ -1529,11 +1479,11 @@
     return phase1Result;
   }
 
-  private void getMetaData(QBExpr qbexpr) throws SemanticException {
+  protected void getMetaData(QBExpr qbexpr) throws SemanticException {
     getMetaData(qbexpr, null);
   }
 
-  private void getMetaData(QBExpr qbexpr, ReadEntity parentInput)
+  protected void getMetaData(QBExpr qbexpr, ReadEntity parentInput)
       throws SemanticException {
     if (qbexpr.getOpcode() == QBExpr.Opcode.NULLOP) {
       getMetaData(qbexpr.getQB(), parentInput);
@@ -1819,7 +1769,7 @@
     }
   }
 
-  private void replaceViewReferenceWithDefinition(QB qb, Table tab,
+  protected void replaceViewReferenceWithDefinition(QB qb, Table tab,
       String tab_name, String alias) throws SemanticException {
 
     ParseDriver pd = new ParseDriver();
@@ -1861,7 +1811,7 @@
     qb.rewriteViewToSubq(alias, tab_name, qbexpr);
   }
 
-  private boolean isPresent(String[] list, String elem) {
+  protected boolean isPresent(String[] list, String elem) {
     for (String s : list) {
       if (s.toLowerCase().equals(elem)) {
         return true;
@@ -1872,7 +1822,7 @@
   }
 
   @SuppressWarnings("nls")
-  private void parseJoinCondPopulateAlias(QBJoinTree joinTree, ASTNode condn,
+  protected void parseJoinCondPopulateAlias(QBJoinTree joinTree, ASTNode condn,
       ArrayList<String> leftAliases, ArrayList<String> rightAliases,
       ArrayList<String> fields) throws SemanticException {
     // String[] allAliases = joinTree.getAllAliases();
@@ -1970,7 +1920,7 @@
     }
   }
 
-  private void populateAliases(List<String> leftAliases,
+  protected void populateAliases(List<String> leftAliases,
       List<String> rightAliases, ASTNode condn, QBJoinTree joinTree,
       List<String> leftSrc) throws SemanticException {
     if ((leftAliases.size() != 0) && (rightAliases.size() != 0)) {
@@ -1994,7 +1944,7 @@
     }
   }
 
-  private void parseJoinCondition(QBJoinTree joinTree, ASTNode joinCond, List<String> leftSrc)
+  protected void parseJoinCondition(QBJoinTree joinTree, ASTNode joinCond, List<String> leftSrc)
       throws SemanticException {
     if (joinCond == null) {
       return;
@@ -2032,7 +1982,7 @@
    *          left sources
    * @throws SemanticException
    */
-  private void parseJoinCondition(QBJoinTree joinTree, ASTNode joinCond,
+  protected void parseJoinCondition(QBJoinTree joinTree, ASTNode joinCond,
       List<String> leftSrc, JoinType type) throws SemanticException {
     if (joinCond == null) {
       return;
@@ -2229,7 +2179,7 @@
   }
 
   @SuppressWarnings("nls")
-  private Operator genHavingPlan(String dest, QB qb, Operator input)
+  protected Operator genHavingPlan(String dest, QB qb, Operator input)
       throws SemanticException {
 
     ASTNode havingExpr = qb.getParseInfo().getHavingForClause(dest);
@@ -2252,7 +2202,7 @@
   }
 
   @SuppressWarnings("nls")
-  private Operator genFilterPlan(String dest, QB qb, Operator input)
+  protected Operator genFilterPlan(String dest, QB qb, Operator input)
       throws SemanticException {
 
     ASTNode whereExpr = qb.getParseInfo().getWhrForClause(dest);
@@ -2270,7 +2220,7 @@
    *          the input operator
    */
   @SuppressWarnings("nls")
-  private Operator genFilterPlan(QB qb, ASTNode condn, Operator input)
+  protected Operator genFilterPlan(QB qb, ASTNode condn, Operator input)
       throws SemanticException {
 
     OpParseContext inputCtx = opParseCtx.get(input);
@@ -2287,7 +2237,7 @@
   }
 
   @SuppressWarnings("nls")
-  private Integer genColListRegex(String colRegex, String tabAlias,
+  protected Integer genColListRegex(String colRegex, String tabAlias,
       ASTNode sel, ArrayList<ExprNodeDesc> col_list,
       RowResolver input, Integer pos, RowResolver output, List<String> aliases, boolean subQuery)
       throws SemanticException {
@@ -2378,21 +2328,21 @@
     return HiveConf.getColumnInternalName(pos);
   }
 
-  private String getScriptProgName(String cmd) {
+  protected String getScriptProgName(String cmd) {
     int end = cmd.indexOf(" ");
     return (end == -1) ? cmd : cmd.substring(0, end);
   }
 
-  private String getScriptArgs(String cmd) {
+  protected String getScriptArgs(String cmd) {
     int end = cmd.indexOf(" ");
     return (end == -1) ? "" : cmd.substring(end, cmd.length());
   }
 
-  private static int getPositionFromInternalName(String internalName) {
+  protected static int getPositionFromInternalName(String internalName) {
     return HiveConf.getPositionFromInternalName(internalName);
   }
 
-  private String fetchFilesNotInLocalFilesystem(String cmd) {
+  protected String fetchFilesNotInLocalFilesystem(String cmd) {
     SessionState ss = SessionState.get();
     String progName = getScriptProgName(cmd);
 
@@ -2411,7 +2361,7 @@
     return cmd;
   }
 
-  private TableDesc getTableDescFromSerDe(ASTNode child, String cols,
+  protected TableDesc getTableDescFromSerDe(ASTNode child, String cols,
       String colTypes, boolean defaultCols) throws SemanticException {
     if (child.getType() == HiveParser.TOK_SERDENAME) {
       String serdeName = unescapeSQLString(child.getChild(0).getText());
@@ -2487,7 +2437,7 @@
     return null;
   }
 
-  private void failIfColAliasExists(Set<String> nameSet, String name)
+  protected void failIfColAliasExists(Set<String> nameSet, String name)
       throws SemanticException {
     if (nameSet.contains(name)) {
       throw new SemanticException(ErrorMsg.COLUMN_ALIAS_ALREADY_EXISTS
@@ -2497,7 +2447,7 @@
   }
 
   @SuppressWarnings("nls")
-  private Operator genScriptPlan(ASTNode trfm, QB qb, Operator input)
+  protected Operator genScriptPlan(ASTNode trfm, QB qb, Operator input)
       throws SemanticException {
     // If there is no "AS" clause, the output schema will be "key,value"
     ArrayList<ColumnInfo> outputCols = new ArrayList<ColumnInfo>();
@@ -2657,7 +2607,7 @@
     return output;
   }
 
-  private Class<? extends RecordReader> getRecordReader(ASTNode node)
+  protected Class<? extends RecordReader> getRecordReader(ASTNode node)
       throws SemanticException {
     String name;
 
@@ -2675,7 +2625,7 @@
     }
   }
 
-  private Class<? extends RecordReader> getDefaultRecordReader()
+  protected Class<? extends RecordReader> getDefaultRecordReader()
       throws SemanticException {
     String name;
 
@@ -2689,7 +2639,7 @@
     }
   }
 
-  private Class<? extends RecordWriter> getRecordWriter(ASTNode node)
+  protected Class<? extends RecordWriter> getRecordWriter(ASTNode node)
       throws SemanticException {
     String name;
 
@@ -2707,7 +2657,7 @@
     }
   }
 
-  private List<Integer> getGroupingSetsForRollup(int size) {
+  protected List<Integer> getGroupingSetsForRollup(int size) {
     List<Integer> groupingSetKeys = new ArrayList<Integer>();
     for (int i = 0; i <= size; i++) {
       groupingSetKeys.add((1 << i) - 1);
@@ -2715,7 +2665,7 @@
     return groupingSetKeys;
   }
 
-  private List<Integer> getGroupingSetsForCube(int size) {
+  protected List<Integer> getGroupingSetsForCube(int size) {
     int count = 1 << size;
     List<Integer> results = new ArrayList<Integer>(count);
     for (int i = 0; i < count; ++i) {
@@ -2727,7 +2677,7 @@
   // This function returns the grouping sets along with the grouping expressions
   // Even if rollups and cubes are present in the query, they are converted to
   // grouping sets at this point
-  private ObjectPair<List<ASTNode>, List<Integer>> getGroupByGroupingSetsForClause(
+  protected ObjectPair<List<ASTNode>, List<Integer>> getGroupByGroupingSetsForClause(
       QBParseInfo parseInfo, String dest) throws SemanticException {
     List<Integer> groupingSets = new ArrayList<Integer>();
     List<ASTNode> groupByExprs = getGroupByForClause(parseInfo, dest);
@@ -2742,7 +2692,7 @@
     return new ObjectPair<List<ASTNode>, List<Integer>>(groupByExprs, groupingSets);
   }
 
-  private List<Integer> getGroupingSets(List<ASTNode> groupByExpr, QBParseInfo parseInfo,
+  protected List<Integer> getGroupingSets(List<ASTNode> groupByExpr, QBParseInfo parseInfo,
       String dest) throws SemanticException {
     Map<String, Integer> exprPos = new HashMap<String, Integer>();
     for (int i = 0; i < groupByExpr.size(); ++i) {
@@ -2779,7 +2729,7 @@
     return result;
   }
 
-  private boolean checkForNoAggr(List<Integer> bitmaps) {
+  protected boolean checkForNoAggr(List<Integer> bitmaps) {
     boolean ret = true;
     for (int mask : bitmaps) {
       ret &= mask == 0;
@@ -2787,7 +2737,7 @@
     return ret;
   }
 
-  private int setBit(int bitmap, int bitIdx) {
+  protected int setBit(int bitmap, int bitIdx) {
     return bitmap | (1 << bitIdx);
   }
 
@@ -2836,7 +2786,7 @@
     }
   }
 
-  private static String[] getColAlias(ASTNode selExpr, String defaultName,
+  protected static String[] getColAlias(ASTNode selExpr, String defaultName,
       RowResolver inputRR, boolean includeFuncName, int colNum) {
     String colAlias = null;
     String tabAlias = null;
@@ -2912,7 +2862,7 @@
    * Returns whether the pattern is a regex expression (instead of a normal
    * string). Normal string is a string with all alphabets/digits and "_".
    */
-  private static boolean isRegex(String pattern) {
+  protected static boolean isRegex(String pattern) {
     for (int i = 0; i < pattern.length(); i++) {
       if (!Character.isLetterOrDigit(pattern.charAt(i))
           && pattern.charAt(i) != '_') {
@@ -2923,7 +2873,7 @@
   }
 
 
-  private Operator<?> genSelectPlan(String dest, QB qb, Operator<?> input)
+  protected Operator<?> genSelectPlan(String dest, QB qb, Operator<?> input)
       throws SemanticException {
     ASTNode selExprList = qb.getParseInfo().getSelForClause(dest);
 
@@ -2937,7 +2887,7 @@
   }
 
   @SuppressWarnings("nls")
-  private Operator<?> genSelectPlan(ASTNode selExprList, QB qb,
+  protected Operator<?> genSelectPlan(ASTNode selExprList, QB qb,
       Operator<?> input, boolean outerLV) throws SemanticException {
 
     if (LOG.isDebugEnabled()) {
@@ -3183,7 +3133,7 @@
     return output;
   }
 
-  private String recommendName(ExprNodeDesc exp, String colAlias) {
+  protected String recommendName(ExprNodeDesc exp, String colAlias) {
     if (!colAlias.startsWith(autogenColAliasPrfxLbl)) {
       return null;
     }
@@ -3302,7 +3252,7 @@
     return r;
   }
 
-  private static GenericUDAFEvaluator.Mode groupByDescModeToUDAFMode(
+  protected static GenericUDAFEvaluator.Mode groupByDescModeToUDAFMode(
       GroupByDesc.Mode mode, boolean isDistinct) {
     switch (mode) {
     case COMPLETE:
@@ -3345,7 +3295,7 @@
    * @return the ExprNodeDesc of the constant parameter if the given internalName represents
    *         a constant parameter; otherwise, return null
    */
-  private ExprNodeDesc isConstantParameterInAggregationParameters(String internalName,
+  protected ExprNodeDesc isConstantParameterInAggregationParameters(String internalName,
       List<ExprNodeDesc> reduceValues) {
     // only the pattern of "VALUE._col([0-9]+)" should be handled.
 
@@ -3383,7 +3333,7 @@
    * @return the new GroupByOperator
    */
   @SuppressWarnings("nls")
-  private Operator genGroupByPlanGroupByOperator(QBParseInfo parseInfo,
+  protected Operator genGroupByPlanGroupByOperator(QBParseInfo parseInfo,
       String dest, Operator input, ReduceSinkOperator rs, GroupByDesc.Mode mode,
       Map<String, GenericUDAFEvaluator> genericUDAFEvaluators)
       throws SemanticException {
@@ -3515,7 +3465,7 @@
   //
   // This function is called for GroupBy2 to pass the additional grouping keys introduced by
   // GroupBy1 for the grouping set (corresponding to the rollup).
-  private void addGroupingSetKey(List<ExprNodeDesc> groupByKeys,
+  protected void addGroupingSetKey(List<ExprNodeDesc> groupByKeys,
       RowResolver groupByInputRowResolver,
       RowResolver groupByOutputRowResolver,
       List<String> outputColumnNames,
@@ -3546,7 +3496,7 @@
   //
   // This function is called for ReduceSink to add the additional grouping keys introduced by
   // GroupBy1 into the reduce keys.
-  private void processGroupingSetReduceSinkOperator(RowResolver reduceSinkInputRowResolver,
+  protected void processGroupingSetReduceSinkOperator(RowResolver reduceSinkInputRowResolver,
       RowResolver reduceSinkOutputRowResolver,
       List<ExprNodeDesc> reduceKeys,
       List<String> outputKeyColumnNames,
@@ -3588,7 +3538,7 @@
    * @return the new GroupByOperator
    */
   @SuppressWarnings("nls")
-  private Operator genGroupByPlanGroupByOperator1(QBParseInfo parseInfo,
+  protected Operator genGroupByPlanGroupByOperator1(QBParseInfo parseInfo,
       String dest, Operator reduceSinkOperatorInfo, GroupByDesc.Mode mode,
       Map<String, GenericUDAFEvaluator> genericUDAFEvaluators,
       boolean distPartAgg,
@@ -3792,7 +3742,7 @@
    * A dummy grouping id. is added. At runtime, the group by operator
    * creates 'n' rows per input row, where 'n' is the number of grouping sets.
    */
-  private void createNewGroupingKey(List<ExprNodeDesc> groupByKeys,
+  protected void createNewGroupingKey(List<ExprNodeDesc> groupByKeys,
       List<String> outputColumnNames,
       RowResolver groupByOutputRowResolver,
       Map<String, ExprNodeDesc> colExprMap) {
@@ -3825,7 +3775,7 @@
    * @return the new GroupByOperator
    */
   @SuppressWarnings("nls")
-  private Operator genGroupByPlanMapGroupByOperator(QB qb,
+  protected Operator genGroupByPlanMapGroupByOperator(QB qb,
       String dest,
       List<ASTNode> grpByExprs,
       Operator inputOperatorInfo,
@@ -3968,7 +3918,7 @@
    * @throws SemanticException
    */
   @SuppressWarnings("nls")
-  private ReduceSinkOperator genGroupByPlanReduceSinkOperator(QB qb,
+  protected ReduceSinkOperator genGroupByPlanReduceSinkOperator(QB qb,
       String dest,
       Operator inputOperatorInfo,
       List<ASTNode> grpByExprs,
@@ -4053,7 +4003,7 @@
     return rsOp;
   }
 
-  private ArrayList<ExprNodeDesc> getReduceKeysForReduceSink(List<ASTNode> grpByExprs, String dest,
+  protected ArrayList<ExprNodeDesc> getReduceKeysForReduceSink(List<ASTNode> grpByExprs, String dest,
       RowResolver reduceSinkInputRowResolver, RowResolver reduceSinkOutputRowResolver,
       List<String> outputKeyColumnNames, Map<String, ExprNodeDesc> colExprMap)
       throws SemanticException {
@@ -4082,7 +4032,7 @@
     return reduceKeys;
   }
 
-  private List<List<Integer>> getDistinctColIndicesForReduceSink(QBParseInfo parseInfo,
+  protected List<List<Integer>> getDistinctColIndicesForReduceSink(QBParseInfo parseInfo,
       String dest,
       List<ExprNodeDesc> reduceKeys, RowResolver reduceSinkInputRowResolver,
       RowResolver reduceSinkOutputRowResolver, List<String> outputKeyColumnNames,
@@ -4134,7 +4084,7 @@
     return distinctColIndices;
   }
 
-  private void getReduceValuesForReduceSinkNoMapAgg(QBParseInfo parseInfo, String dest,
+  protected void getReduceValuesForReduceSinkNoMapAgg(QBParseInfo parseInfo, String dest,
       RowResolver reduceSinkInputRowResolver, RowResolver reduceSinkOutputRowResolver,
       List<String> outputValueColumnNames, ArrayList<ExprNodeDesc> reduceValues,
       Map<String, ExprNodeDesc> colExprMap) throws SemanticException {
@@ -4164,7 +4114,7 @@
   }
 
   @SuppressWarnings("nls")
-  private ReduceSinkOperator genCommonGroupByPlanReduceSinkOperator(QB qb, List<String> dests,
+  protected ReduceSinkOperator genCommonGroupByPlanReduceSinkOperator(QB qb, List<String> dests,
       Operator inputOperatorInfo) throws SemanticException {
 
     RowResolver reduceSinkInputRowResolver = opParseCtx.get(inputOperatorInfo)
@@ -4245,7 +4195,7 @@
   // Remove expression node descriptor and children of it for a given predicate
   // from mapping if it's already on RS keys.
   // Remaining column expressions would be a candidate for an RS value
-  private void removeMappingForKeys(ASTNode predicate, Map<ASTNode, ExprNodeDesc> mapping,
+  protected void removeMappingForKeys(ASTNode predicate, Map<ASTNode, ExprNodeDesc> mapping,
       List<ExprNodeDesc> keys) {
     ExprNodeDesc expr = (ExprNodeDesc) mapping.get(predicate);
     if (expr != null && ExprNodeDescUtils.indexOf(expr, keys) >= 0) {
@@ -4258,7 +4208,7 @@
   }
 
   // Remove expression node desc and all children of it from mapping
-  private void removeRecursively(ASTNode current, Map<ASTNode, ExprNodeDesc> mapping) {
+  protected void removeRecursively(ASTNode current, Map<ASTNode, ExprNodeDesc> mapping) {
     mapping.remove(current);
     for (int i = 0; i < current.getChildCount(); i++) {
       removeRecursively((ASTNode) current.getChild(i), mapping);
@@ -4282,7 +4232,7 @@
    * @throws SemanticException
    */
   @SuppressWarnings("nls")
-  private Operator genGroupByPlanReduceSinkOperator2MR(QBParseInfo parseInfo,
+  protected Operator genGroupByPlanReduceSinkOperator2MR(QBParseInfo parseInfo,
       String dest,
       Operator groupByOperatorInfo,
       int numPartitionFields,
@@ -4370,7 +4320,7 @@
    * @throws SemanticException
    */
   @SuppressWarnings("nls")
-  private Operator genGroupByPlanGroupByOperator2MR(QBParseInfo parseInfo,
+  protected Operator genGroupByPlanGroupByOperator2MR(QBParseInfo parseInfo,
       String dest,
       Operator reduceSinkOperatorInfo2,
       GroupByDesc.Mode mode,
@@ -4493,7 +4443,7 @@
    *           Reducer: iterate/merge (mode = COMPLETE)
    **/
   @SuppressWarnings({"nls"})
-  private Operator genGroupByPlan1MR(String dest, QB qb, Operator input)
+  protected Operator genGroupByPlan1MR(String dest, QB qb, Operator input)
       throws SemanticException {
 
     QBParseInfo parseInfo = qb.getParseInfo();
@@ -4534,7 +4484,7 @@
   }
 
   @SuppressWarnings({"nls"})
-  private Operator genGroupByPlan1ReduceMultiGBY(List<String> dests, QB qb, Operator input)
+  protected Operator genGroupByPlan1ReduceMultiGBY(List<String> dests, QB qb, Operator input)
       throws SemanticException {
 
     QBParseInfo parseInfo = qb.getParseInfo();
@@ -4665,7 +4615,7 @@
    *           Reducer: merge/terminate (mode = FINAL)
    */
   @SuppressWarnings("nls")
-  private Operator genGroupByPlan2MRMultiGroupBy(String dest, QB qb,
+  protected Operator genGroupByPlan2MRMultiGroupBy(String dest, QB qb,
       Operator input) throws SemanticException {
 
     // ////// Generate GroupbyOperator for a map-side partial aggregation
@@ -4736,7 +4686,7 @@
    *           Reducer: merge/terminate (mode = FINAL)
    */
   @SuppressWarnings("nls")
-  private Operator genGroupByPlan2MR(String dest, QB qb, Operator input)
+  protected Operator genGroupByPlan2MR(String dest, QB qb, Operator input)
       throws SemanticException {
 
     QBParseInfo parseInfo = qb.getParseInfo();
@@ -4796,7 +4746,7 @@
     return groupByOperatorInfo2;
   }
 
-  private boolean optimizeMapAggrGroupBy(String dest, QB qb) {
+  protected boolean optimizeMapAggrGroupBy(String dest, QB qb) {
     List<ASTNode> grpByExprs = getGroupByForClause(qb.getParseInfo(), dest);
     if ((grpByExprs != null) && !grpByExprs.isEmpty()) {
       return false;
@@ -4809,7 +4759,7 @@
     return true;
   }
 
-  static private void extractColumns(Set<String> colNamesExprs,
+  static protected void extractColumns(Set<String> colNamesExprs,
       ExprNodeDesc exprNode) throws SemanticException {
     if (exprNode instanceof ExprNodeColumnDesc) {
       colNamesExprs.add(((ExprNodeColumnDesc) exprNode).getColumn());
@@ -4824,7 +4774,7 @@
     }
   }
 
-  static private boolean hasCommonElement(Set<String> set1, Set<String> set2) {
+  static protected boolean hasCommonElement(Set<String> set1, Set<String> set2) {
     for (String elem1 : set1) {
       if (set2.contains(elem1)) {
         return true;
@@ -4834,7 +4784,7 @@
     return false;
   }
 
-  private void checkExpressionsForGroupingSet(List<ASTNode> grpByExprs,
+  protected void checkExpressionsForGroupingSet(List<ASTNode> grpByExprs,
       List<ASTNode> distinctGrpByExprs,
       Map<String, ASTNode> aggregationTrees,
       RowResolver inputRowResolver) throws SemanticException {
@@ -4933,7 +4883,7 @@
    * grouping keys: group by expressions + grouping id.
    */
   @SuppressWarnings("nls")
-  private Operator genGroupByPlanMapAggrNoSkew(String dest, QB qb,
+  protected Operator genGroupByPlanMapAggrNoSkew(String dest, QB qb,
       Operator inputOperatorInfo) throws SemanticException {
 
     QBParseInfo parseInfo = qb.getParseInfo();
@@ -5101,7 +5051,7 @@
    * Reducer: merge/terminate (mode = FINAL)
    */
   @SuppressWarnings("nls")
-  private Operator genGroupByPlanMapAggr2MR(String dest, QB qb,
+  protected Operator genGroupByPlanMapAggr2MR(String dest, QB qb,
       Operator inputOperatorInfo) throws SemanticException {
 
     QBParseInfo parseInfo = qb.getParseInfo();
@@ -5201,7 +5151,7 @@
   }
 
   @SuppressWarnings("nls")
-  private Operator genConversionOps(String dest, QB qb, Operator input)
+  protected Operator genConversionOps(String dest, QB qb, Operator input)
       throws SemanticException {
 
     Integer dest_type = qb.getMetaData().getDestTypeForAlias(dest);
@@ -5222,7 +5172,7 @@
     return input;
   }
 
-  private int getReducersBucketing(int totalFiles, int maxReducers) {
+  protected int getReducersBucketing(int totalFiles, int maxReducers) {
     int numFiles = (int)Math.ceil((double)totalFiles / (double)maxReducers);
     while (true) {
       if (totalFiles % numFiles == 0) {
@@ -5232,7 +5182,7 @@
     }
   }
 
-  private static class SortBucketRSCtx {
+  protected static class SortBucketRSCtx {
     ArrayList<ExprNodeDesc> partnCols;
     boolean multiFileSpray;
     int numFiles;
@@ -5307,7 +5257,7 @@
   }
 
   @SuppressWarnings("nls")
-  private Operator genBucketingSortingDest(String dest, Operator input, QB qb,
+  protected Operator genBucketingSortingDest(String dest, Operator input, QB qb,
       TableDesc table_desc, Table dest_tab, SortBucketRSCtx ctx) throws SemanticException {
 
     // If the table is bucketed, and bucketing is enforced, do the following:
@@ -5384,7 +5334,7 @@
    * @param qb
    * @return true if HOLD_DDLTIME is set, false otherwise.
    */
-  private boolean checkHoldDDLTime(QB qb) {
+  protected boolean checkHoldDDLTime(QB qb) {
     ASTNode hints = qb.getParseInfo().getHints();
     if (hints == null) {
       return false;
@@ -5399,7 +5349,7 @@
   }
 
   @SuppressWarnings("nls")
-  private Operator genFileSinkPlan(String dest, QB qb, Operator input)
+  protected Operator genFileSinkPlan(String dest, QB qb, Operator input)
       throws SemanticException {
 
     RowResolver inputRR = opParseCtx.get(input).getRowResolver();
@@ -5938,7 +5888,7 @@
   }
 
   @SuppressWarnings("nls")
-  private Operator genLimitPlan(String dest, QB qb, Operator input, int limit)
+  protected Operator genLimitPlan(String dest, QB qb, Operator input, int limit)
       throws SemanticException {
     // A map-only job can be optimized - instead of converting it to a
     // map-reduce job, we can have another map
@@ -5964,7 +5914,7 @@
     return limitMap;
   }
 
-  private Operator genUDTFPlan(GenericUDTF genericUDTF,
+  protected Operator genUDTFPlan(GenericUDTF genericUDTF,
       String outputTableAlias, ArrayList<String> colAliases, QB qb,
       Operator input, boolean outerLV) throws SemanticException {
 
@@ -6056,7 +6006,7 @@
   }
 
   @SuppressWarnings("nls")
-  private Operator genLimitMapRedPlan(String dest, QB qb, Operator input,
+  protected Operator genLimitMapRedPlan(String dest, QB qb, Operator input,
       int limit, boolean extraMRStep) throws SemanticException {
     // A map-only job can be optimized - instead of converting it to a
     // map-reduce job, we can have another map
@@ -6076,7 +6026,7 @@
     return genLimitPlan(dest, qb, curr, limit);
   }
 
-  private ArrayList<ExprNodeDesc> getParitionColsFromBucketCols(String dest, QB qb, Table tab,
+  protected ArrayList<ExprNodeDesc> getParitionColsFromBucketCols(String dest, QB qb, Table tab,
       TableDesc table_desc, Operator input, boolean convert)
       throws SemanticException {
     List<String> tabBucketCols = tab.getBucketCols();
@@ -6099,7 +6049,7 @@
     return genConvertCol(dest, qb, tab, table_desc, input, posns, convert);
   }
 
-  private ArrayList<ExprNodeDesc> genConvertCol(String dest, QB qb, Table tab,
+  protected ArrayList<ExprNodeDesc> genConvertCol(String dest, QB qb, Table tab,
       TableDesc table_desc, Operator input, List<Integer> posns, boolean convert)
       throws SemanticException {
     StructObjectInspector oi = null;
@@ -6149,7 +6099,7 @@
     return expressions;
   }
 
-  private ArrayList<ExprNodeDesc> getSortCols(String dest, QB qb, Table tab, TableDesc table_desc,
+  protected ArrayList<ExprNodeDesc> getSortCols(String dest, QB qb, Table tab, TableDesc table_desc,
       Operator input, boolean convert)
       throws SemanticException {
     RowResolver inputRR = opParseCtx.get(input).getRowResolver();
@@ -6173,7 +6123,7 @@
     return genConvertCol(dest, qb, tab, table_desc, input, posns, convert);
   }
 
-  private ArrayList<Integer> getSortOrders(String dest, QB qb, Table tab, Operator input)
+  protected ArrayList<Integer> getSortOrders(String dest, QB qb, Table tab, Operator input)
       throws SemanticException {
     RowResolver inputRR = opParseCtx.get(input).getRowResolver();
     List<Order> tabSortCols = tab.getSortCols();
@@ -6192,7 +6142,7 @@
   }
 
   @SuppressWarnings("nls")
-  private Operator genReduceSinkPlanForSortingBucketing(Table tab, Operator input,
+  protected Operator genReduceSinkPlanForSortingBucketing(Table tab, Operator input,
       ArrayList<ExprNodeDesc> sortCols,
       List<Integer> sortOrders,
       ArrayList<ExprNodeDesc> partitionCols,
@@ -6256,7 +6206,7 @@
   }
 
   @SuppressWarnings("nls")
-  private Operator genReduceSinkPlan(String dest, QB qb, Operator input,
+  protected Operator genReduceSinkPlan(String dest, QB qb, Operator input,
       int numReducers) throws SemanticException {
 
     RowResolver inputRR = opParseCtx.get(input).getRowResolver();
@@ -6367,7 +6317,7 @@
     return output;
   }
 
-  private Operator genJoinOperatorChildren(QBJoinTree join, Operator left,
+  protected Operator genJoinOperatorChildren(QBJoinTree join, Operator left,
       Operator[] right, HashSet<Integer> omitOpts) throws SemanticException {
 
     RowResolver outputRS = new RowResolver();
@@ -6465,7 +6415,7 @@
   }
 
   @SuppressWarnings("nls")
-  private Operator genJoinReduceSinkChild(QB qb, QBJoinTree joinTree,
+  protected Operator genJoinReduceSinkChild(QB qb, QBJoinTree joinTree,
       Operator child, String srcName, int pos) throws SemanticException {
     RowResolver inputRS = opParseCtx.get(child).getRowResolver();
     RowResolver outputRS = new RowResolver();
@@ -6528,20 +6478,10 @@
     rsOp.setColumnExprMap(colExprMap);
     rsOp.setInputAlias(srcName);
 
-    //kangyanli added to get tmpTable columns
-    if(((HiveConf) conf).getBoolVar(HiveConf.ConfVars.HIVEINC)){
-      if(pos == 1 && isCreateTmpPhase){
-        if(srcName.equals(subQueryName.toLowerCase())){
-          RStmpPos.add(rsOp);
-        }
-      }
-    }
-    //kangyanli added end
-
     return rsOp;
   }
 
-  private Operator genJoinOperator(QB qb, QBJoinTree joinTree,
+  protected Operator genJoinOperator(QB qb, QBJoinTree joinTree,
       Map<String, Operator> map) throws SemanticException {
     QBJoinTree leftChild = joinTree.getJoinSrc();
     Operator joinSrcOp = null;
@@ -6612,7 +6552,7 @@
    * @return the selection operator.
    * @throws SemanticException
    */
-  private Operator insertSelectForSemijoin(ArrayList<ASTNode> fields,
+  protected Operator insertSelectForSemijoin(ArrayList<ASTNode> fields,
       Operator input) throws SemanticException {
 
     RowResolver inputRR = opParseCtx.get(input).getRowResolver();
@@ -6636,7 +6576,7 @@
     return output;
   }
 
-  private Operator genMapGroupByForSemijoin(QB qb, ArrayList<ASTNode> fields, // the
+  protected Operator genMapGroupByForSemijoin(QB qb, ArrayList<ASTNode> fields, // the
       // ASTNode
       // of
       // the
@@ -6691,7 +6631,7 @@
     return op;
   }
 
-  private void genJoinOperatorTypeCheck(Operator left, Operator[] right)
+  protected void genJoinOperatorTypeCheck(Operator left, Operator[] right)
       throws SemanticException {
     // keys[i] -> ArrayList<exprNodeDesc> for the i-th join operator key list
     ArrayList<ArrayList<ExprNodeDesc>> keys = new ArrayList<ArrayList<ExprNodeDesc>>();
@@ -6744,7 +6684,7 @@
     }
   }
 
-  private Operator genJoinPlan(QB qb, Map<String, Operator> map)
+  protected Operator genJoinPlan(QB qb, Map<String, Operator> map)
       throws SemanticException {
     QBJoinTree joinTree = qb.getQbJoinTree();
     Operator joinOp = genJoinOperator(qb, joinTree, map);
@@ -6755,7 +6695,7 @@
    * Extract the filters from the join condition and push them on top of the
    * source operators. This procedure traverses the query tree recursively,
    */
-  private void pushJoinFilters(QB qb, QBJoinTree joinTree,
+  protected void pushJoinFilters(QB qb, QBJoinTree joinTree,
       Map<String, Operator> map) throws SemanticException {
     if (joinTree.getJoinSrc() != null) {
       pushJoinFilters(qb, joinTree.getJoinSrc(), map);
@@ -6775,7 +6715,7 @@
     }
   }
 
-  private List<String> getMapSideJoinTables(QB qb) {
+  protected List<String> getMapSideJoinTables(QB qb) {
     List<String> cols = new ArrayList<String>();
 
 
@@ -6807,11 +6747,11 @@
   // The join alias is modified before being inserted for consumption by sort-merge
   // join queries. If the join is part of a sub-query the alias is modified to include
   // the sub-query alias.
-  private String getModifiedAlias(QB qb, String alias) {
+  protected String getModifiedAlias(QB qb, String alias) {
     return QB.getAppendedAliasFromId(qb.getId(), alias);
   }
 
-  private QBJoinTree genUniqueJoinTree(QB qb, ASTNode joinParseTree,
+  protected QBJoinTree genUniqueJoinTree(QB qb, ASTNode joinParseTree,
       Map<String, Operator> aliasToOpInfo)
       throws SemanticException {
     QBJoinTree joinTree = new QBJoinTree();
@@ -6911,7 +6851,7 @@
     return joinTree;
   }
 
-  private QBJoinTree genJoinTree(QB qb, ASTNode joinParseTree,
+  protected QBJoinTree genJoinTree(QB qb, ASTNode joinParseTree,
       Map<String, Operator> aliasToOpInfo)
       throws SemanticException {
     QBJoinTree joinTree = new QBJoinTree();
@@ -7079,7 +7019,7 @@
     return joinTree;
   }
 
-  private void parseStreamTables(QBJoinTree joinTree, QB qb) {
+  protected void parseStreamTables(QBJoinTree joinTree, QB qb) {
     List<String> streamAliases = joinTree.getStreamAliases();
 
     for (Node hintNode : qb.getParseInfo().getHints().getChildren()) {
@@ -7100,7 +7040,7 @@
   /**
    * Merges node to target
    */
-  private void mergeJoins(QB qb, QBJoinTree node, QBJoinTree target, int pos) {
+  protected void mergeJoins(QB qb, QBJoinTree node, QBJoinTree target, int pos) {
     String[] nodeRightAliases = node.getRightAliases();
     String[] trgtRightAliases = target.getRightAliases();
     String[] rightAliases = new String[nodeRightAliases.length
@@ -7232,7 +7172,7 @@
     }
   }
 
-  private int findMergePos(QBJoinTree node, QBJoinTree target) {
+  protected int findMergePos(QBJoinTree node, QBJoinTree target) {
     int res = -1;
     String leftAlias = node.getLeftAlias();
     if (leftAlias == null) {
@@ -7275,7 +7215,7 @@
   // in a join tree ((A-B)-C)-D where C is not mergeable with A-B,
   // D can be merged with A-B into single join If and only if C and D has same join type
   // In this case, A-B-D join will be executed first and ABD-C join will be executed in next
-  private void mergeJoinTree(QB qb) {
+  protected void mergeJoinTree(QB qb) {
     QBJoinTree tree = qb.getQbJoinTree();
     if (tree.getJoinSrc() == null) {
       return;
@@ -7336,7 +7276,7 @@
   }
 
   // Join types should be all the same for merging (or returns null)
-  private JoinType getType(JoinCond[] conds) {
+  protected JoinType getType(JoinCond[] conds) {
     JoinType type = conds[0].getJoinType();
     for (int k = 1; k < conds.length; k++) {
       if (type != conds[k].getJoinType()) {
@@ -7346,7 +7286,7 @@
     return type;
   }
 
-  private Operator insertSelectAllPlanForGroupBy(Operator input)
+  protected Operator insertSelectAllPlanForGroupBy(Operator input)
       throws SemanticException {
     OpParseContext inputCtx = opParseCtx.get(input);
     RowResolver inputRR = inputCtx.getRowResolver();
@@ -7373,7 +7313,7 @@
 
   // Return the common distinct expression
   // There should be more than 1 destination, with group bys in all of them.
-  private List<ASTNode> getCommonDistinctExprs(QB qb, Operator input) {
+  protected List<ASTNode> getCommonDistinctExprs(QB qb, Operator input) {
     QBParseInfo qbp = qb.getParseInfo();
     // If a grouping set aggregation is present, common processing is not possible
     if (!qbp.getDestCubes().isEmpty() || !qbp.getDestRollups().isEmpty()
@@ -7438,7 +7378,7 @@
     return oldASTList;
   }
 
-  private Operator createCommonReduceSink(QB qb, Operator input)
+  protected Operator createCommonReduceSink(QB qb, Operator input)
       throws SemanticException {
     // Go over all the tables and extract the common distinct key
     List<ASTNode> distExprs = getCommonDistinctExprs(qb, input);
@@ -7533,7 +7473,7 @@
   // Groups the clause names into lists so that any two clauses in the same list has the same
   // group by and distinct keys and no clause appears in more than one list. Returns a list of the
   // lists of clauses.
-  private List<List<String>> getCommonGroupByDestGroups(QB qb,
+  protected List<List<String>> getCommonGroupByDestGroups(QB qb,
       Map<String, Operator<? extends OperatorDesc>> inputs) throws SemanticException {
 
     QBParseInfo qbp = qb.getParseInfo();
@@ -7638,7 +7578,7 @@
     return commonGroupByDestGroups;
   }
 
-  private void combineExprNodeLists(List<ExprNodeDesc> list, List<ExprNodeDesc> list2,
+  protected void combineExprNodeLists(List<ExprNodeDesc> list, List<ExprNodeDesc> list2,
       List<ExprNodeDesc> combinedList) {
     combinedList.addAll(list);
     for (ExprNodeDesc elem : list2) {
@@ -7649,7 +7589,7 @@
   }
 
   // Returns whether or not two lists contain the same elements independent of order
-  private boolean matchExprLists(List<ExprNodeDesc> list1, List<ExprNodeDesc> list2) {
+  protected boolean matchExprLists(List<ExprNodeDesc> list1, List<ExprNodeDesc> list2) {
 
     if (list1.size() != list2.size()) {
       return false;
@@ -7664,7 +7604,7 @@
   }
 
   // Returns a list of the distinct exprs without duplicates for a given clause name
-  private List<ExprNodeDesc> getDistinctExprs(QBParseInfo qbp, String dest, RowResolver inputRR)
+  protected List<ExprNodeDesc> getDistinctExprs(QBParseInfo qbp, String dest, RowResolver inputRR)
       throws SemanticException {
 
     List<ASTNode> distinctAggExprs = qbp.getDistinctFuncExprsForClause(dest);
@@ -7685,7 +7625,7 @@
   }
 
   // see if there are any distinct expressions
-  private boolean distinctExprsExists(QB qb) {
+  protected boolean distinctExprsExists(QB qb) {
     QBParseInfo qbp = qb.getParseInfo();
 
     TreeSet<String> ks = new TreeSet<String>();
@@ -7701,7 +7641,7 @@
   }
 
   @SuppressWarnings("nls")
-  private Operator genBodyPlan(QB qb, Operator input) throws SemanticException {
+  protected Operator genBodyPlan(QB qb, Operator input) throws SemanticException {
     QBParseInfo qbp = qb.getParseInfo();
 
     TreeSet<String> ks = new TreeSet<String>(qbp.getClauseNames());
@@ -7840,7 +7780,7 @@
     return curr;
   }
 
-  private Map<String, Operator<? extends OperatorDesc>> createInputForDests(QB qb,
+  protected Map<String, Operator<? extends OperatorDesc>> createInputForDests(QB qb,
       Operator<? extends OperatorDesc> input, Set<String> dests) throws SemanticException {
     Map<String, Operator<? extends OperatorDesc>> inputs =
         new HashMap<String, Operator<? extends OperatorDesc>>();
@@ -7850,7 +7790,7 @@
     return inputs;
   }
 
-  private Operator genPostGroupByBodyPlan(Operator curr, String dest, QB qb)
+  protected Operator genPostGroupByBodyPlan(Operator curr, String dest, QB qb)
       throws SemanticException {
 
     QBParseInfo qbp = qb.getParseInfo();
@@ -7958,7 +7898,7 @@
   }
 
   @SuppressWarnings("nls")
-  private Operator genUnionPlan(String unionalias, String leftalias,
+  protected Operator genUnionPlan(String unionalias, String leftalias,
       Operator leftOp, String rightalias, Operator rightOp)
       throws SemanticException {
 
@@ -8109,7 +8049,7 @@
    * @return
    * @throws SemanticException
    */
-  private Operator<? extends OperatorDesc> genInputSelectForUnion(
+  protected Operator<? extends OperatorDesc> genInputSelectForUnion(
       Operator<? extends OperatorDesc> origInputOp, Map<String, ColumnInfo> origInputFieldMap,
       String origInputAlias, RowResolver unionoutRR, String unionalias)
       throws SemanticException {
@@ -8185,7 +8125,7 @@
    * @return exprNodeDesc
    * @exception SemanticException
    */
-  private ExprNodeDesc genSamplePredicate(TableSample ts,
+  protected ExprNodeDesc genSamplePredicate(TableSample ts,
       List<String> bucketCols, boolean useBucketCols, String alias,
       RowResolver rwsch, QBMetaData qbm, ExprNodeDesc planExpr)
       throws SemanticException {
@@ -8238,12 +8178,12 @@
     return equalsExpr;
   }
 
-  private String getAliasId(String alias, QB qb) {
+  protected String getAliasId(String alias, QB qb) {
     return (qb.getId() == null ? alias : qb.getId() + ":" + alias);
   }
 
   @SuppressWarnings("nls")
-  private Operator genTablePlan(String alias, QB qb) throws SemanticException {
+  protected Operator genTablePlan(String alias, QB qb) throws SemanticException {
 
     String alias_id = getAliasId(alias, qb);
     Table tab = qb.getMetaData().getSrcForAlias(alias);
@@ -8463,7 +8403,7 @@
     return output;
   }
 
-  private boolean isSkewedCol(String alias, QB qb, String colName) {
+  protected boolean isSkewedCol(String alias, QB qb, String colName) {
     boolean isSkewedCol = false;
     List<String> skewedCols = qb.getSkewedColumnNames(alias);
     for (String skewedCol : skewedCols) {
@@ -8474,7 +8414,7 @@
     return isSkewedCol;
   }
 
-  private void setupStats(TableScanDesc tsDesc, QBParseInfo qbp, Table tab, String alias,
+  protected void setupStats(TableScanDesc tsDesc, QBParseInfo qbp, Table tab, String alias,
       RowResolver rwsch)
       throws SemanticException {
 
@@ -8535,7 +8475,7 @@
     }
   }
 
-  private Operator genPlan(QBExpr qbexpr) throws SemanticException {
+  protected Operator genPlan(QBExpr qbexpr) throws SemanticException {
     if (qbexpr.getOpcode() == QBExpr.Opcode.NULLOP) {
       return genPlan(qbexpr.getQB());
     }
@@ -8675,7 +8615,7 @@
     }
   }
 
-  private Operator genLateralViewPlanForDest(String dest, QB qb, Operator op)
+  protected Operator genLateralViewPlanForDest(String dest, QB qb, Operator op)
       throws SemanticException {
     ASTNode lateralViewTree = qb.getParseInfo().getDestToLateralView().get(dest);
     if (lateralViewTree != null) {
@@ -8684,7 +8624,7 @@
     return op;
   }
 
-  private Operator genLateralViewPlan(QB qb, Operator op, ASTNode lateralViewTree)
+  protected Operator genLateralViewPlan(QB qb, Operator op, ASTNode lateralViewTree)
       throws SemanticException {
     RowResolver lvForwardRR = new RowResolver();
     RowResolver source = opParseCtx.get(op).getRowResolver();
@@ -8767,7 +8707,7 @@
    *          - a list to which the new internal column names will be added, in
    *          the same order as in the dest row resolver
    */
-  private void LVmergeRowResolvers(RowResolver source, RowResolver dest,
+  protected void LVmergeRowResolvers(RowResolver source, RowResolver dest,
       ArrayList<String> outputInternalColNames) {
     for (ColumnInfo c : source.getColumnInfos()) {
       String internalName = getColumnInternalName(outputInternalColNames.size());
@@ -8936,7 +8876,7 @@
     return resultSchema;
   }
 
-  private void saveViewDefinition() throws SemanticException {
+  protected void saveViewDefinition() throws SemanticException {
 
     // Make a copy of the statement's result schema, since we may
     // modify it below as part of imposing view column names.
@@ -9046,7 +8986,7 @@
     createVwDesc.setViewExpandedText(expandedText);
   }
 
-  private List<FieldSchema> convertRowSchemaToViewSchema(RowResolver rr) {
+  protected List<FieldSchema> convertRowSchemaToViewSchema(RowResolver rr) {
     List<FieldSchema> fieldSchemas = new ArrayList<FieldSchema>();
     for (ColumnInfo colInfo : rr.getColumnInfos()) {
       if (colInfo.isHiddenVirtualCol()) {
@@ -9105,7 +9045,7 @@
   /**
    * Find ExprNodeDesc for the expression cached in the RowResolver. Returns null if not exists.
    */
-  private ExprNodeDesc getExprNodeDescCached(ASTNode expr, RowResolver input)
+  protected ExprNodeDesc getExprNodeDescCached(ASTNode expr, RowResolver input)
       throws SemanticException {
     ColumnInfo colInfo = input.getExpression(expr);
     if (colInfo != null) {
@@ -9295,7 +9235,7 @@
     }
   }
 
-  private void validate(Task<? extends Serializable> task, boolean reworkMapredWork)
+  protected void validate(Task<? extends Serializable> task, boolean reworkMapredWork)
       throws SemanticException {
     Utilities.reworkMapRedWork(task, reworkMapredWork, conf);
     if (task.getChildTasks() == null) {
@@ -9322,7 +9262,7 @@
    *          property map
    * @return Modified table property map
    */
-  private Map<String, String> addDefaultProperties(Map<String, String> tblProp) {
+  protected Map<String, String> addDefaultProperties(Map<String, String> tblProp) {
     Map<String, String> retValue;
     if (tblProp == null) {
       retValue = new HashMap<String, String>();
@@ -9352,7 +9292,7 @@
    * the semantic analyzer need to deal with the select statement with respect
    * to the SerDe and Storage Format.
    */
-  private ASTNode analyzeCreateTable(ASTNode ast, QB qb)
+  protected ASTNode analyzeCreateTable(ASTNode ast, QB qb)
       throws SemanticException {
     String tableName = getUnescapedName((ASTNode) ast.getChild(0));
     String likeTableName = null;
@@ -9597,7 +9537,7 @@
     return null;
   }
 
-  private ASTNode analyzeCreateView(ASTNode ast, QB qb)
+  protected ASTNode analyzeCreateView(ASTNode ast, QB qb)
       throws SemanticException {
     String tableName = getUnescapedName((ASTNode) ast.getChild(0));
     List<FieldSchema> cols = null;
@@ -9664,7 +9604,7 @@
   // validate the create view statement
   // the statement could be CREATE VIEW, REPLACE VIEW, or ALTER VIEW AS SELECT
   // check semantic conditions
-  private void validateCreateView(CreateViewDesc createVwDesc)
+  protected void validateCreateView(CreateViewDesc createVwDesc)
     throws SemanticException {
     try {
       Table oldView = db.getTable(createVwDesc.getViewName(), false);
@@ -9713,7 +9653,7 @@
   }
 
   // Process the position alias in GROUPBY and ORDERBY
-  private void processPositionAlias(ASTNode ast) throws SemanticException {
+  protected void processPositionAlias(ASTNode ast) throws SemanticException {
     if (HiveConf.getBoolVar(conf,
           HiveConf.ConfVars.HIVE_GROUPBY_ORDERBY_POSITION_ALIAS) == false) {
       return;
@@ -9842,7 +9782,7 @@
    * @param tree
    * @throws SemanticException
    */
-  private void validateAnalyzeNoscan(ASTNode tree) throws SemanticException {
+  protected void validateAnalyzeNoscan(ASTNode tree) throws SemanticException {
     // since it is noscan, it is true table name in command
     String tableName = getUnescapedName((ASTNode) tree.getChild(0).getChild(0));
     Table tbl;
@@ -9867,7 +9807,7 @@
    * @param tree
    * @throws SemanticException
    */
-  private void validateAnalyzePartialscan(ASTNode tree) throws SemanticException {
+  protected void validateAnalyzePartialscan(ASTNode tree) throws SemanticException {
     // since it is partialscan, it is true table name in command
     String tableName = getUnescapedName((ASTNode) tree.getChild(0).getChild(0));
     Table tbl;
@@ -9902,7 +9842,7 @@
    * It will check if this is analyze ... compute statistics noscan
    * @param tree
    */
-  private void checkNoScan(ASTNode tree) {
+  protected void checkNoScan(ASTNode tree) {
     if (tree.getChildCount() > 1) {
       ASTNode child0 = (ASTNode) tree.getChild(0);
       ASTNode child1;
@@ -9922,7 +9862,7 @@
    * It will check if this is analyze ... compute statistics partialscan
    * @param tree
    */
-  private void checkPartialScan(ASTNode tree) {
+  protected void checkPartialScan(ASTNode tree) {
     if (tree.getChildCount() > 1) {
       ASTNode child0 = (ASTNode) tree.getChild(0);
       ASTNode child1;
@@ -9956,7 +9896,7 @@
    * - For a SubQuery: set the source to the alias returned by processSubQuery
    * - For a PTF invocation: recursively call processPTFChain.
    */
-  private PTFInputSpec processPTFSource(QB qb, ASTNode inputNode) throws SemanticException{
+  protected PTFInputSpec processPTFSource(QB qb, ASTNode inputNode) throws SemanticException{
 
     PTFInputSpec qInSpec = null;
     int type = inputNode.getType();
@@ -9994,7 +9934,7 @@
    * - a partitionTableFunctionSource can be a tableReference, a SubQuery or another
    *   PTF invocation.
    */
-  private PartitionedTableFunctionSpec processPTFChain(QB qb, ASTNode ptf)
+  protected PartitionedTableFunctionSpec processPTFChain(QB qb, ASTNode ptf)
       throws SemanticException{
     int child_count = ptf.getChildCount();
     if (child_count < 2) {
@@ -10059,7 +9999,7 @@
    *   ^(TOK_PTBLFUNCTION name partitionTableFunctionSource partitioningSpec? arguments*)
    * - setup a PTFInvocationSpec for this top level PTF invocation.
    */
-  private void processPTF(QB qb, ASTNode ptf) throws SemanticException{
+  protected void processPTF(QB qb, ASTNode ptf) throws SemanticException{
 
     PartitionedTableFunctionSpec ptfSpec = processPTFChain(qb, ptf);
 
@@ -10072,7 +10012,7 @@
     qb.addPTFNodeToSpec(ptf, spec);
   }
 
-  private void handleQueryWindowClauses(QB qb, Phase1Ctx ctx_1, ASTNode node)
+  protected void handleQueryWindowClauses(QB qb, Phase1Ctx ctx_1, ASTNode node)
       throws SemanticException {
     WindowingSpec spec = qb.getWindowingSpec(ctx_1.dest);
     for(int i=0; i < node.getChildCount(); i++) {
@@ -10080,7 +10020,7 @@
     }
   }
 
-  private PartitionSpec processPartitionSpec(ASTNode node) {
+  protected PartitionSpec processPartitionSpec(ASTNode node) {
     PartitionSpec pSpec = new PartitionSpec();
     int exprCnt = node.getChildCount();
     for(int i=0; i < exprCnt; i++) {
@@ -10091,7 +10031,7 @@
     return pSpec;
   }
 
-  private OrderSpec processOrderSpec(ASTNode sortNode) {
+  protected OrderSpec processOrderSpec(ASTNode sortNode) {
     OrderSpec oSpec = new OrderSpec();
     int exprCnt = sortNode.getChildCount();
     for(int i=0; i < exprCnt; i++) {
@@ -10108,7 +10048,7 @@
     return oSpec;
   }
 
-  private PartitioningSpec processPTFPartitionSpec(ASTNode pSpecNode)
+  protected PartitioningSpec processPTFPartitionSpec(ASTNode pSpecNode)
   {
     PartitioningSpec partitioning = new PartitioningSpec();
     ASTNode firstChild = (ASTNode) pSpecNode.getChild(0);
@@ -10135,7 +10075,7 @@
     return partitioning;
   }
 
-  private WindowFunctionSpec processWindowFunction(ASTNode node, ASTNode wsNode)
+  protected WindowFunctionSpec processWindowFunction(ASTNode node, ASTNode wsNode)
     throws SemanticException {
     WindowFunctionSpec wfSpec = new WindowFunctionSpec();
 
@@ -10171,7 +10111,7 @@
     return wfSpec;
   }
 
-  private boolean containsLeadLagUDF(ASTNode expressionTree) {
+  protected boolean containsLeadLagUDF(ASTNode expressionTree) {
     int exprTokenType = expressionTree.getToken().getType();
     if (exprTokenType == HiveParser.TOK_FUNCTION) {
       assert (expressionTree.getChildCount() != 0);
@@ -10194,7 +10134,7 @@
     return false;
   }
 
-  private void processQueryWindowClause(WindowingSpec spec, ASTNode node)
+  protected void processQueryWindowClause(WindowingSpec spec, ASTNode node)
       throws SemanticException {
     ASTNode nameNode = (ASTNode) node.getChild(0);
     ASTNode wsNode = (ASTNode) node.getChild(1);
@@ -10207,7 +10147,7 @@
     spec.addWindowSpec(nameNode.getText(), ws);
   }
 
-  private WindowSpec processWindowSpec(ASTNode node) throws SemanticException {
+  protected WindowSpec processWindowSpec(ASTNode node) throws SemanticException {
     String sourceId = null;
     PartitionSpec partition = null;
     OrderSpec order = null;
@@ -10257,7 +10197,7 @@
     return ws;
   }
 
-  private WindowFrameSpec processWindowFrame(ASTNode node) throws SemanticException {
+  protected WindowFrameSpec processWindowFrame(ASTNode node) throws SemanticException {
     int type = node.getType();
     BoundarySpec start = null, end = null;
 
@@ -10274,7 +10214,7 @@
     return new WindowFrameSpec(start, end);
   }
 
-  private BoundarySpec processBoundary(int frameType, ASTNode node)  throws SemanticException {
+  protected BoundarySpec processBoundary(int frameType, ASTNode node)  throws SemanticException {
     BoundarySpec bs = frameType == HiveParser.TOK_WINDOWRANGE ?
         new RangeBoundarySpec() : new ValueBoundarySpec();
     int type = node.getType();
@@ -10320,7 +10260,7 @@
    * - current logic used is to look for HiveParser.TOK_TABLE_OR_COL
    * - if there is none then the expression is a constant.
    */
-  private static class ConstantExprCheck implements ContextVisitor {
+  protected static class ConstantExprCheck implements ContextVisitor {
     boolean isConstant = true;
 
     public void visit(Object t, Object parent, int childIndex, Map labels) {
@@ -10342,7 +10282,7 @@
     }
   }
 
-  private static class AggregationExprCheck implements ContextVisitor {
+  protected static class AggregationExprCheck implements ContextVisitor {
     HashMap<String, ASTNode> destAggrExprs;
     boolean isAggr = false;
 
@@ -10373,7 +10313,7 @@
    * Returns false if there is a SelectExpr that is not a constant or an aggr.
    *
    */
-  private boolean isValidGroupBySelectList(QB currQB, String clause){
+  protected boolean isValidGroupBySelectList(QB currQB, String clause){
     ConstantExprCheck constantExprCheck = new ConstantExprCheck();
     AggregationExprCheck aggrExprCheck = new AggregationExprCheck(
         currQB.getParseInfo().getAggregationExprsForClause(clause));
@@ -10412,7 +10352,7 @@
 
 //--------------------------- PTF handling: PTFInvocationSpec to PTFDesc --------------------------
 
-  private PTFDesc translatePTFInvocationSpec(PTFInvocationSpec ptfQSpec, RowResolver inputRR)
+  protected PTFDesc translatePTFInvocationSpec(PTFInvocationSpec ptfQSpec, RowResolver inputRR)
       throws SemanticException{
     PTFDesc ptfDesc = null;
     PTFTranslator translator = new PTFTranslator();
@@ -10542,7 +10482,7 @@
     }
   }
 
-  private Operator genPTFPlanForComponentQuery(PTFInvocationSpec ptfQSpec, Operator input)
+  protected Operator genPTFPlanForComponentQuery(PTFInvocationSpec ptfQSpec, Operator input)
       throws SemanticException {
     /*
      * 1. Create the PTFDesc from the Qspec attached to this QB.
@@ -10693,7 +10633,7 @@
     return input;
   }
 
-  private Operator genReduceSinkPlanForWindowing(WindowingSpec spec,
+  protected Operator genReduceSinkPlanForWindowing(WindowingSpec spec,
       RowResolver inputRR,
       Operator input) throws SemanticException{
     ArrayList<ExprNodeDesc> partCols = new ArrayList<ExprNodeDesc>();
@@ -10951,6 +10891,7 @@
 
     return pCtx;
   }
+
   public void MultidoPhase2forTest(ParseContext pCtx) throws SemanticException {
     initCtx(pCtx.getContext());
     init();
@@ -11004,8 +10945,15 @@
     return;
   }
 
+  public void Setfetchtasklist(HashMap<Integer,FetchTask> fetchtasklist){
+    this.FetchTaskList=fetchtasklist;
+  }
+  public HashMap<Integer,FetchTask> getfetchtasklist(){
+    return this.FetchTaskList;
+  }
 
 
+
   public void analyzePhase2(ParseContext pCtx) throws SemanticException{
     initCtx(pCtx.getContext());
     init();
@@ -11054,65 +11002,19 @@
 
   }
 
-  public  ArrayList<QB> analyzeIncPhase1(ASTNode ast) throws SemanticException {
+
+  /**
+   * compile and generate MR tasks based on QB
+   * */
+  public void analyzeQB(ASTNode ast,QB qb1) throws SemanticException {
     ASTNode child = ast;
     this.ast = ast;
     viewsExpanded = new ArrayList<String>();
 
-    LOG.info("Starting Semantic Analysis");
+    LOG.info("Starting Semantic Analysis use QB");
+    SessionState.get().setCommandType(HiveOperation.QUERY);
 
-    // analyze and process the position alias
-    processPositionAlias(ast);
-
-    // analyze create table command
-
-    if (ast.getToken().getType() == HiveParser.TOK_CREATETABLE) {
-      // if it is not CTAS, we don't need to go further and just return
-      if ((child = analyzeCreateTable(ast, qb)) == null) {
-        return null;
-      }
-    } else {
-      SessionState.get().setCommandType(HiveOperation.QUERY);
-    }
-
-    // analyze create view command
-    if (ast.getToken().getType() == HiveParser.TOK_CREATEVIEW ||
-        ast.getToken().getType() == HiveParser.TOK_ALTERVIEW_AS) {
-      child = analyzeCreateView(ast, qb);
-      SessionState.get().setCommandType(HiveOperation.CREATEVIEW);
-      if (child == null) {
-        return null;
-      }
-      viewSelect = child;
-      // prevent view from referencing itself
-      viewsExpanded.add(SessionState.get().getCurrentDatabase() + "." + createVwDesc.getViewName());
-    }
-
-    // continue analyzing from the child ASTNode.
-    Phase1Ctx ctx_1 = initPhase1Ctx();
-    ctx_1.isinc = true;
-
-    QB qb2 = new QB(null, null, false);
-    ArrayList<QB> qblist = new ArrayList<QB>();
-
-    Phase1Ctx ctx_2 = initPhase1Ctx();
-    if (!doPhase1Inc(child, qb, ctx_1,qb2,ctx_2)) {
-      // if phase1Result false return
-      return null;
-    }
-    qblist.add(qb);
-    qblist.add(qb2);
-
-
-    return qblist;
-  }
-  public ParseContext analyzeIncPhase2(ASTNode ast,QB qb) throws SemanticException {
-
-    ASTNode child = ast;
-    this.ast = ast;
-    this.qb = qb;
-    LOG.info("Completed phase 1 of Semantic Analysis");
-
+    this.qb = qb1;
     getMetaData(qb);
     LOG.info("Completed getting MetaData in Semantic Analysis");
 
@@ -11121,117 +11023,8 @@
     // such as JDBC would prefer instead of the c0, c1 we'll end
     // up with later.
     Operator sinkOp = genPlan(qb);
-    LOG.info("Completed genPlan " + sinkOp.toString());
-    resultSchema =
-        convertRowSchemaToViewSchema(opParseCtx.get(sinkOp).getRowResolver());
 
-    if (createVwDesc != null) {
-      saveViewDefinition();
 
-      // validate the create view statement
-      // at this point, the createVwDesc gets all the information for semantic check
-      validateCreateView(createVwDesc);
-
-      // Since we're only creating a view (not executing it), we
-      // don't need to optimize or translate the plan (and in fact, those
-      // procedures can interfere with the view creation). So
-      // skip the rest of this method.
-      ctx.setResDir(null);
-      ctx.setResFile(null);
-      return null;
-    }
-
-    ParseContext pCtx = new ParseContext(conf, qb, child, opToPartPruner,
-        opToPartList, topOps, topSelOps, opParseCtx, joinContext, smbMapJoinContext,
-        topToTable, topToTableProps, fsopToTable,
-        loadTableWork, loadFileWork, ctx, idToTableNameMap, destTableId, uCtx,
-        listMapJoinOpsNoReducer, groupOpToInputTables, prunedPartitions,
-        opToSamplePruner, globalLimitCtx, nameToSplitSample, inputs, rootTasks,
-        opToPartToSkewedPruner, viewAliasToInput,
-        reduceSinkOperatorsAddedByEnforceBucketingSorting, queryProperties);
-
-    // Generate table access stats if required
-    if (HiveConf.getBoolVar(this.conf, HiveConf.ConfVars.HIVE_STATS_COLLECT_TABLEKEYS) == true) {
-      TableAccessAnalyzer tableAccessAnalyzer = new TableAccessAnalyzer(pCtx);
-      setTableAccessInfo(tableAccessAnalyzer.analyzeTableAccess());
-    }
-      LOG.info("Completed plan generation");
-
-    return pCtx;
-  }
-
-  public void Setfetchtasklist(HashMap<Integer,FetchTask> fetchtasklist){
-    this.FetchTaskList=fetchtasklist;
-  }
-  public HashMap<Integer,FetchTask> getfetchtasklist(){
-    return this.FetchTaskList;
-  }
-
-  public ASTNode genAndrunCrtTmpQuery(ASTNode ast, Context ctx) throws SemanticException{
-    initCtx(ctx);
-    init();
-
-    ASTNode child = ast;
-    this.ast = ast;
-    viewsExpanded = new ArrayList<String>();
-
-    // analyze and process the position alias
-    processPositionAlias(ast);
-
-    // analyze create table command
-    if (ast.getToken().getType() == HiveParser.TOK_CREATETABLE) {
-      // if it is not CTAS, we don't need to go further and just return
-      if ((child = analyzeCreateTable(ast, qb)) == null) {
-        return null;
-      }
-    } else {
-      SessionState.get().setCommandType(HiveOperation.QUERY);
-    }
-
-    // analyze create view command
-    if (ast.getToken().getType() == HiveParser.TOK_CREATEVIEW ||
-        ast.getToken().getType() == HiveParser.TOK_ALTERVIEW_AS) {
-      child = analyzeCreateView(ast, qb);
-      SessionState.get().setCommandType(HiveOperation.CREATEVIEW);
-      if (child == null) {
-        return null;
-      }
-      viewSelect = child;
-      // prevent view from referencing itself
-      viewsExpanded.add(SessionState.get().getCurrentDatabase() + "." + createVwDesc.getViewName());
-    }
-
-    LOG.info("Gen and run creat tmp query Begin!");
-    IncCtx incCtx = initIncCtx();
-    isCreateTmpPhase = true;
-    LOG.info("Before joinReorder, whole ast is :" + child.dump());
-
-    extractPosOfTmp(child);
-    if(isInsertSubQuery){
-      /*
-       *  dont't handle join here because only need change join conditions out of incPos,
-       *   we had done it in addSubQuery function;
-       */
-      analyzeAlias(child,subQueryName,false);
-      topAliasInTmp.add(subQueryName.toLowerCase());
-      LOG.info("After joinReorder, whole ast is :" + child.dump());
-    }else{
-      LOG.info("No tmp table to extract!");
-    }
-
-    //QB qbInc = new QB(null, null, false);
-    // continue analyzing from the child ASTNode.
-    Phase1Ctx ctx_1 = initPhase1Ctx();
-    if (!doPhase1(child, qb, ctx_1)) {
-      // if phase1Result false return
-      return null;
-    }
-
-    getMetaData(qb);
-
-    Operator sinkOp = genPlan(qb);
-
-
     resultSchema =
         convertRowSchemaToViewSchema(opParseCtx.get(sinkOp).getRowResolver());
 
@@ -11248,1397 +11041,9 @@
       // skip the rest of this method.
       ctx.setResDir(null);
       ctx.setResFile(null);
-      return null;
-    }
-
-    pCtxInc = new ParseContext(conf, qb, child, opToPartPruner,
-        opToPartList, topOps, topSelOps, opParseCtx, joinContext, smbMapJoinContext,
-        topToTable, topToTableProps, fsopToTable,
-        loadTableWork, loadFileWork, ctx, idToTableNameMap, destTableId, uCtx,
-        listMapJoinOpsNoReducer, groupOpToInputTables, prunedPartitions,
-        opToSamplePruner, globalLimitCtx, nameToSplitSample, inputs, rootTasks,
-        opToPartToSkewedPruner, viewAliasToInput,
-        reduceSinkOperatorsAddedByEnforceBucketingSorting, queryProperties);
-
-    // Generate table access stats if required
-    if (HiveConf.getBoolVar(this.conf, HiveConf.ConfVars.HIVE_STATS_COLLECT_TABLEKEYS) == true) {
-      TableAccessAnalyzer tableAccessAnalyzer = new TableAccessAnalyzer(pCtxInc);
-      setTableAccessInfo(tableAccessAnalyzer.analyzeTableAccess());
-    }
-
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Before logical optimization\n" + Operator.toString(pCtxInc.getTopOps().values()));
-    }
-
-    Optimizer optm = new Optimizer();
-    optm.setPctx(pCtxInc);
-    optm.initialize(conf);
-    pCtxInc = optm.optimize();
-
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("After logical optimization\n" + Operator.toString(pCtxInc.getTopOps().values()));
-    }
-
-    // Generate column access stats if required - wait until column pruning takes place
-    // during optimization
-    if (HiveConf.getBoolVar(this.conf, HiveConf.ConfVars.HIVE_STATS_COLLECT_SCANCOLS) == true) {
-      ColumnAccessAnalyzer columnAccessAnalyzer = new ColumnAccessAnalyzer(pCtxInc);
-      setColumnAccessInfo(columnAccessAnalyzer.analyzeColumnAccess());
-    }
-
-    if(RStmpPos.size() <= 0){
-
-      LOG.info("No tmp table extract! AST tree keep same: \n "+ child.dump());
-
-      if (!ctx.getExplainLogical()) {
-        // At this point we have the complete operator tree
-        // from which we want to create the map-reduce plan
-        MapReduceCompiler compiler = new MapReduceCompiler();
-        compiler.init(conf, console, db);
-        compiler.compile(pCtxInc, rootTasks, inputs, outputs);
-        fetchTask = pCtxInc.getFetchTask();
-      }
-
-      LOG.info("Completed plan generation");
-      return child;
-
-    }else if(RStmpPos.size() == 1){ // only support one tmp table
-
-      for(int i=0; i < RStmpPos.size(); i++){
-        Operator tmpRSOp = RStmpPos.get(i);
-
-        String crtTmpQL = genCrtTmpQL(tmpRSOp);
-
-        try {
-          LOG.info("drop table "+ crtTmpTablename);
-          (new Driver(conf)).run("drop table "+ crtTmpTablename);
-          LOG.info("creat tmp QL is: " + crtTmpQL);
-          (new Driver(conf)).run(crtTmpQL);
-        } catch (CommandNeedRetryException e) {
-          // TODO Auto-generated catch block
-          e.printStackTrace();
-        }
-
-        LOG.info("Gen and run creat tmp query End!");
-
-        return doExtractTmpCompile();
-      }
-    }
-    return null;
-  }
-
-  private String genCrtTmpQL(Operator tmpRSOp) throws SemanticException{
-    String crtTmpQL = null;
-    List<String> cols = new ArrayList<String>();
-
-    Iterator<Entry<String, ExprNodeDesc>> iterator = tmpRSOp.getColumnExprMap().entrySet().iterator();
-    while(iterator.hasNext()){
-      Entry<String, ExprNodeDesc> entry = iterator.next();
-      ExprNodeColumnDesc backtrackedCol = (ExprNodeColumnDesc) ExprNodeDescUtils.backtrackToTS(entry.getValue(),tmpRSOp);
-      cols.add(backtrackedCol.getTabAlias() + "_" + backtrackedCol.getColumn() + " " + entry.getValue().getTypeString());
-      tmpTableCols.add(backtrackedCol.getTabAlias() + "_" + backtrackedCol.getColumn());
-    }
-    crtTmpQL = "create table " + crtTmpTablename + " (";
-    for(int i = 0 ;i <cols.size(); i++){
-      if(i < cols.size() -1 ){
-        crtTmpQL= crtTmpQL + cols.get(i) + " , ";
-      }else{
-        crtTmpQL= crtTmpQL + cols.get(i);
-      }
-    }
-    crtTmpQL = crtTmpQL + " )";
-    return crtTmpQL;
-  }
-
-  /**
-   * recursively handle the ASTtree and record positions of tmpTables in incCtx.tmpPos
-   * @param root
-   * @param incCtx
-   * @throws SemanticException
-   */
-  private void extractPosOfTmp(ASTNode root) throws SemanticException{
-
-    if(root.getToken().getType() == HiveParser.TOK_FROM){
-      ASTNode frm = (ASTNode) root.getChild(0);
-      if(isJoinToken(frm)){
-        //LOG.info("Before join Reorder, frm ast is " + frm.dump());
-        QBJoinTree joinTree = genJoinTreeInc(frm);
-        ASTNode AST = frm;
-        List<ASTNode> ASTtrees = new ArrayList<ASTNode>();
-        for(; isJoinToken(AST) ; AST = (ASTNode )AST.getChild(0)){
-          ASTtrees.add(AST);
-        }
-
-        incPos = getIncPos(ASTtrees);
-        if(incPos == -1){
-          LOG.info("No inc Table in this from clause!");
-          return;
-        }
-        String[] subQueryAliases = joinReorderAST(joinTree, ASTtrees);
-
-        //let incJoin's rightChild as tmpTable, leftChild include incTable;
-        ASTNode incJoin = ASTtrees.get(incPos);
-        ASTNode tmp = (ASTNode) incJoin.getChild(0);
-        incJoin.setChild(0,incJoin.getChild(1));
-        incJoin.setChild(1,tmp);
-
-        if(isJoinToken( (ASTNode) incJoin.getChild(1))){
-          addSubQuery(ASTtrees,subQueryAliases);
-          ASTtmpPos.add((ASTNode) incJoin.getChild(1));
-          CollectionUtils.addAll(topAliasInTmp, subQueryAliases);
-          isInsertSubQuery = true;
-        }
-
-        //LOG.info("After join Reorder, frm ast is " + frm.dump());
-      }
-    }
-
-    int child_count = root.getChildCount();
-    for (int child_pos = 0; child_pos < child_count; ++child_pos) {
-      // Recurse
-      ASTNode curr = (ASTNode) root.getChild(child_pos);
-      if(addedSubQuery.contains(curr)){
-        continue;
-      }
-      extractPosOfTmp(curr);
-    }
-
-  }
-  private QBJoinTree genJoinTreeInc(ASTNode joinParseTree)
-      throws SemanticException{
-
-    QBJoinTree joinTree = new QBJoinTree();
-    JoinCond[] condn = new JoinCond[1];
-
-    switch (joinParseTree.getToken().getType()) {
-    case HiveParser.TOK_LEFTOUTERJOIN:
-      joinTree.setNoOuterJoin(false);
-      condn[0] = new JoinCond(0, 1, JoinType.LEFTOUTER);
-      break;
-    case HiveParser.TOK_RIGHTOUTERJOIN:
-      joinTree.setNoOuterJoin(false);
-      condn[0] = new JoinCond(0, 1, JoinType.RIGHTOUTER);
-      break;
-    case HiveParser.TOK_FULLOUTERJOIN:
-      joinTree.setNoOuterJoin(false);
-      condn[0] = new JoinCond(0, 1, JoinType.FULLOUTER);
-      break;
-    case HiveParser.TOK_LEFTSEMIJOIN:
-      joinTree.setNoSemiJoin(false);
-      condn[0] = new JoinCond(0, 1, JoinType.LEFTSEMI);
-      break;
-    default:
-      condn[0] = new JoinCond(0, 1, JoinType.INNER);
-      joinTree.setNoOuterJoin(true);
-      break;
-    }
-
-    joinTree.setJoinCond(condn);
-
-    ASTNode left = (ASTNode) joinParseTree.getChild(0);
-    ASTNode right = (ASTNode) joinParseTree.getChild(1);
-
-    if ((left.getToken().getType() == HiveParser.TOK_TABREF)
-        || (left.getToken().getType() == HiveParser.TOK_SUBQUERY)
-        || (left.getToken().getType() == HiveParser.TOK_PTBLFUNCTION)) {
-      String tableName = getUnescapedUnqualifiedTableName((ASTNode) left.getChild(0))
-          .toLowerCase();
-      String alias = left.getChildCount() == 1 ? tableName
-          : unescapeIdentifier(left.getChild(left.getChildCount() - 1)
-          .getText().toLowerCase());
-      // ptf node form is: ^(TOK_PTBLFUNCTION $name $alias? partitionTableFunctionSource partitioningSpec? expression*)
-      // guranteed to have an lias here: check done in processJoin
-      alias = (left.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) ?
-          unescapeIdentifier(left.getChild(1).getText().toLowerCase()) :
-            alias;
-      aliasToTabs.put(alias, tableName);
-      joinTree.setLeftAlias(alias);
-      String[] leftAliases = new String[1];
-      leftAliases[0] = alias;
-      joinTree.setLeftAliases(leftAliases);
-      String[] children = new String[2];
-      children[0] = alias;
-      joinTree.setBaseSrc(children);
-      joinTree.setId(qb.getId());
-      /*joinTree.getAliasToOpInfo().put(
-          getModifiedAlias(qb, alias), aliasToOpInfo.get(alias));*/
-    } else if (isJoinToken(left)) {
-      QBJoinTree leftTree = genJoinTreeInc(left);
-      joinTree.setJoinSrc(leftTree);
-      String[] leftChildAliases = leftTree.getLeftAliases();
-      String leftAliases[] = new String[leftChildAliases.length + 1];
-      for (int i = 0; i < leftChildAliases.length; i++) {
-        leftAliases[i] = leftChildAliases[i];
-      }
-      leftAliases[leftChildAliases.length] = leftTree.getRightAliases()[0];
-      joinTree.setLeftAliases(leftAliases);
-    } else {
-      assert (false);
-    }
-
-    if ((right.getToken().getType() == HiveParser.TOK_TABREF)
-        || (right.getToken().getType() == HiveParser.TOK_SUBQUERY)
-        || (right.getToken().getType() == HiveParser.TOK_PTBLFUNCTION)) {
-      String tableName = getUnescapedUnqualifiedTableName((ASTNode) right.getChild(0))
-          .toLowerCase();
-      String alias = right.getChildCount() == 1 ? tableName
-          : unescapeIdentifier(right.getChild(right.getChildCount() - 1)
-          .getText().toLowerCase());
-      // ptf node form is: ^(TOK_PTBLFUNCTION $name $alias? partitionTableFunctionSource partitioningSpec? expression*)
-      // guranteed to have an lias here: check done in processJoin
-      alias = (right.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) ?
-          unescapeIdentifier(right.getChild(1).getText().toLowerCase()) :
-            alias;
-      aliasToTabs.put(alias, tableName);
-      String[] rightAliases = new String[1];
-      rightAliases[0] = alias;
-      joinTree.setRightAliases(rightAliases);
-      String[] children = joinTree.getBaseSrc();
-      if (children == null) {
-        children = new String[2];
-      }
-      children[1] = alias;
-      joinTree.setBaseSrc(children);
-      joinTree.setId(qb.getId());
-      /*joinTree.getAliasToOpInfo().put(
-          getModifiedAlias(qb, alias), aliasToOpInfo.get(alias));*/
-      // remember rhs table for semijoin
-      if (joinTree.getNoSemiJoin() == false) {
-        joinTree.addRHSSemijoin(alias);
-      }
-    } else {
-      assert false;
-    }
-
-    ArrayList<ArrayList<ASTNode>> expressions = new ArrayList<ArrayList<ASTNode>>();
-    expressions.add(new ArrayList<ASTNode>());
-    expressions.add(new ArrayList<ASTNode>());
-    joinTree.setExpressions(expressions);
-
-    ArrayList<Boolean> nullsafes = new ArrayList<Boolean>();
-    joinTree.setNullSafes(nullsafes);
-
-    ArrayList<ArrayList<ASTNode>> filters = new ArrayList<ArrayList<ASTNode>>();
-    filters.add(new ArrayList<ASTNode>());
-    filters.add(new ArrayList<ASTNode>());
-    joinTree.setFilters(filters);
-    joinTree.setFilterMap(new int[2][]);
-
-    ArrayList<ArrayList<ASTNode>> filtersForPushing =
-        new ArrayList<ArrayList<ASTNode>>();
-    filtersForPushing.add(new ArrayList<ASTNode>());
-    filtersForPushing.add(new ArrayList<ASTNode>());
-    joinTree.setFiltersForPushing(filtersForPushing);
-
-    ASTNode joinCond = (ASTNode) joinParseTree.getChild(2);
-    ArrayList<String> leftSrc = new ArrayList<String>();
-    parseJoinCondition(joinTree, joinCond, leftSrc);
-    joinTree.setLeftSrc(leftSrc);
-    if (leftSrc.size() == 1) {
-      joinTree.setLeftAlias(leftSrc.get(0));
-    }
-
-    return joinTree;
-
-}
-
-  private int getIncPos(List<ASTNode> ASTtrees) {
-    for(int i= ASTtrees.size()-1; i>=0; i--){
-
-      ASTNode left = (ASTNode) ASTtrees.get(i).getChild(0);
-
-      if ((left.getToken().getType() == HiveParser.TOK_TABREF)
-          || (left.getToken().getType() == HiveParser.TOK_SUBQUERY)
-          || (left.getToken().getType() == HiveParser.TOK_PTBLFUNCTION)) {
-        String tableName = getUnescapedUnqualifiedTableName((ASTNode) left.getChild(0))
-        .toLowerCase();
-        String alias = left.getChildCount() == 1 ? tableName
-            : unescapeIdentifier(left.getChild(left.getChildCount() - 1)
-            .getText().toLowerCase());
-        // ptf node form is: ^(TOK_PTBLFUNCTION $name $alias? partitionTableFunctionSource partitioningSpec? expression*)
-        // guranteed to have an lias here: check done in processJoin
-        alias = (left.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) ?
-            unescapeIdentifier(left.getChild(1).getText().toLowerCase()) :
-              alias;
-        if(tableName.equals(incTable)){
-          incTable = alias;
-          return i;
-        }
-      }
-
-      ASTNode right = (ASTNode) ASTtrees.get(i).getChild(1);
-
-      if ((right.getToken().getType() == HiveParser.TOK_TABREF)
-          || (right.getToken().getType() == HiveParser.TOK_SUBQUERY)
-          || (right.getToken().getType() == HiveParser.TOK_PTBLFUNCTION)) {
-        String tableName = getUnescapedUnqualifiedTableName((ASTNode) right.getChild(0))
-        .toLowerCase();
-        String alias = right.getChildCount() == 1 ? tableName
-            : unescapeIdentifier(right.getChild(right.getChildCount() - 1)
-            .getText().toLowerCase());
-        // ptf node form is: ^(TOK_PTBLFUNCTION $name $alias? partitionTableFunctionSource partitioningSpec? expression*)
-        // guranteed to have an lias here: check done in processJoin
-        alias = (right.getToken().getType() == HiveParser.TOK_PTBLFUNCTION) ?
-            unescapeIdentifier(right.getChild(1).getText().toLowerCase()) :
-              alias;
-        if(tableName.equals(incTable)){
-          incTable = alias;
-          return i;
-        }
-      }
-    }
-    return -1;
-  }
-
-  /**
-   * 1. change ASTtrees according joinTree and ctx_1;
-   * 2. after joinReorder, ASTtree's incJoin's right child include the incTable;
-   * 3. return the leftAliases of incJoin
-   * because left child of incJoin need to surrounded by TOK_SUBQUERY;
-   */
-  String[] joinReorderAST(QBJoinTree joinTree,List<ASTNode> ASTtrees){
-    QBJoinTree tree = joinTree;
-    if (tree.getJoinSrc() == null) {
-      return null;
-    }
-    // make array with QBJoinTree : outer most(0) --> inner most(n)
-    List<QBJoinTree> QBJoinTrees = new ArrayList<QBJoinTree>();
-    for (;tree != null; tree = tree.getJoinSrc()) {
-      QBJoinTrees.add(tree);
-    }
-    assert QBJoinTrees.size() == ASTtrees.size();
-
-    Set<String> incAliases = new HashSet<String>();
-    incAliases.add(incTable);
-    int pre = incPos;
-    /*traverse the trees list to move the incTable outer*/
-    for (int i = pre-1; i >= 0; i--) {
-      QBJoinTree preQBJoinTree = QBJoinTrees.get(pre);
-      QBJoinTree currQBJoinTree = QBJoinTrees.get(i);
-      String[] leftAliases = preQBJoinTree.getLeftAliases();
-      String[] rightAliases = preQBJoinTree.getRightAliases();
-      boolean isExchange = false;
-      if(canExchange(incAliases,currQBJoinTree.getLeftSrc())){
-      //otherwise, incTable is needed in both curr join and pre join,
-        //so we can't move incTable to outer
-
-        /*exchange the incTable and currAST's right table in AST tree*/
-        if(java.util.Arrays.asList(leftAliases).containsAll(incAliases)){//incTable in left
-          exchangeInc(ASTtrees, preQBJoinTree, currQBJoinTree, 0,i);
-          isExchange = true;
-          incPos = i;
-        }else if(java.util.Arrays.asList(rightAliases).containsAll(incAliases)){// incTable in right
-          exchangeInc(ASTtrees, preQBJoinTree, currQBJoinTree, 1,i);
-          isExchange =true;
-          incPos = i;
-        }else{
-          isExchange = false;
-        }
-      }
-      if(!isExchange){
-        // add all incQBJoinTree's aliases to incAliases
-        CollectionUtils.addAll(incAliases,leftAliases);
-        CollectionUtils.addAll(incAliases,rightAliases);
-      }
-      pre = i;
-    }
-    return QBJoinTrees.get(incPos).getLeftAliases();
-  }
-
-  private boolean canExchange(Set<String> incAliases, List<String> leftSrc) {
-    for(String leftAlias:leftSrc){
-      if(incAliases.contains(leftAlias)){
-        return false;
-      }
-    }
-    return true;
-  }
-
-  /**
-   *  incAST is left child of currAST
-   * this function exchange the incAST's incAliasPos child and currAST's right child
-   * @param currPos
-   */
-  void exchangeInc(List<ASTNode> ASTtrees, QBJoinTree preQBJoinTree,QBJoinTree currQBJoinTree,int incAliasPos, int currPos){
-    ASTNode currAST = ASTtrees.get(currPos);
-    ASTNode preAST = ASTtrees.get( currPos + 1 );
-
-    ASTNode tmpTable = (ASTNode) preAST.getChild(incAliasPos);
-    preAST.setChild(incAliasPos, currAST.getChild(1));
-    currAST.setChild(1, tmpTable);//right child of currAST
-
-    if(preAST.getChildCount() == 3 && currAST.getChildCount() == 3){
-      ASTNode tmpCondn = (ASTNode) preAST.getChild(2);
-      preAST.setChild(2, currAST.getChild(2));
-      currAST.setChild(2, tmpCondn);
-    }else if(preAST.getChildCount() == 2 && currAST.getChildCount() == 3){
-      preAST.addChild(currAST.getChild(2));
-      currAST.deleteChild(2);
-    }else if(preAST.getChildCount() == 3 && currAST.getChildCount() == 2){
-      currAST.addChild(preAST.getChild(2));
-      preAST.deleteChild(2);
-    }
-
-    //preAST is TOK_RIGHTOUERJOIN and currAST is TOK_JOIN, we need to change the both's type
-    if(preAST.getToken().getType() != currAST.getToken().getType()){
-
-      ASTNode newCurrAST = new ASTNode((CommonToken) preAST.getToken());
-      int posCurrAST= currAST.getChildIndex();
-      currAST.getParent().setChild(posCurrAST, newCurrAST);
-      newCurrAST.setParent(currAST.getParent());
-      for(int i = 0 ; i < currAST.getChildCount() ; i++){
-        newCurrAST.addChild(currAST.getChild(i));
-        currAST.getChild(i).setParent(newCurrAST);
-      }
-      ASTtrees.set(currPos, newCurrAST);
-
-      ASTNode newPreAST = new ASTNode((CommonToken) currAST.getToken());
-      int posPreAST= preAST.getChildIndex();
-      preAST.getParent().setChild(posPreAST, newPreAST);
-      newPreAST.setParent(preAST.getParent());
-      for(int i = 0 ; i < preAST.getChildCount() ; i++){
-        newPreAST.addChild(preAST.getChild(i));
-        preAST.getChild(i).setParent(newPreAST);
-      }
-      ASTtrees.set(currPos+1, newPreAST);
-
-    }
-
-    // set preQBJoinTree's leftAliases or rightAliases
-    String[] tmpAliases = null;
-    if(incAliasPos == 0){
-      tmpAliases = preQBJoinTree.getLeftAliases();
-      preQBJoinTree.setLeftAliases(currQBJoinTree.getRightAliases());
-    }else if(incAliasPos == 1){
-      tmpAliases = preQBJoinTree.getRightAliases();
-      preQBJoinTree.setRightAliases(currQBJoinTree.getRightAliases());
-    }
-
-    // set currQBJoin's rightAliases
-    currQBJoinTree.setRightAliases(tmpAliases);
-
-    // set currQBJoin's leftAliases
-    String[] leftChildAliases = preQBJoinTree.getLeftAliases();
-    String[] rightChildAliases = preQBJoinTree.getRightAliases();
-    String leftAliases[] = new String[leftChildAliases.length + rightChildAliases.length];
-    for (int j = 0; j < leftChildAliases.length; j++) {
-      leftAliases[j] = leftChildAliases[j];
-    }
-    for (int j = 0; j < rightChildAliases.length; j++) {
-      leftAliases[leftChildAliases.length + j] = rightChildAliases[j];
-    }
-    currQBJoinTree.setLeftAliases(leftAliases);
-
-  }
-
-  /**
-   *  if pos is 0 ,insert TOK_SUBQUERY as Parent's left Child
-   * if pos is 1,insert TOK_SUBQUERY as Parent's right Child
-   * construct like follows:
-   *
-   * paret
-   *  (TOK_SUBQUERY
-   *    (TOK_QUERY
-   *      (TOK_FROM child)
-   *      (TOK_INSERT
-   *        TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)
-   *        TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)
-   *      )
-   *    )
-   *    name
-   *  )
-   *      */
-  void insertTokSubQuery(ASTNode parent,int pos,String[] subQueryAliases){
-    ASTNode child = (ASTNode) parent.getChild(pos);
-
-    /*paret(TOK_SUBQUERY)*/
-    ASTNode subQuery = new ASTNode(new CommonToken(HiveParser.TOK_SUBQUERY,"TOK_SUBQUERY"));
-    parent.setChild(pos, subQuery);
-    subQuery.setParent(parent);
-
-    /*TOK_SUBQUERY(TOK_QUERY)*/
-    ASTNode query = new ASTNode(new CommonToken(HiveParser.TOK_QUERY,"TOK_QUERY"));
-    subQuery.addChild(query);
-    query.setParent(subQuery);
-
-    /*TOK_SUBQUERY(tmp)*/
-    ASTNode subQueryAlias = new ASTNode(new CommonToken(HiveParser.Identifier,subQueryName));
-    subQuery.addChild(subQueryAlias);
-    subQueryAlias.setParent(subQuery);
-
-    /*TOK_QUERY(TOK_FROM) TOK_FROM(child)*/
-    ASTNode from = new ASTNode(new CommonToken(HiveParser.TOK_FROM,"TOK_FROM"));
-    query.addChild(from);
-    from.setParent(query);
-    from.addChild(child);
-    child.setParent(from);
-
-    /*TOK_QUERY(TOK_INSERT)*/
-    ASTNode insert = new ASTNode(new CommonToken(HiveParser.TOK_INSERT,"TOK_INSERT"));
-    query.addChild(insert);
-    insert.setParent(query);
-
-    /*TOK_INSERT(TOK_DESTINATION (TOK_DIR TOK_TMP_FILE))*/
-    ASTNode destination = new ASTNode(new CommonToken(HiveParser.TOK_DESTINATION,"TOK_DESTINATION"));
-    ASTNode dir = new ASTNode(new CommonToken(HiveParser.TOK_DIR,"TOK_DIR"));
-    ASTNode tmpFile = new ASTNode(new CommonToken(HiveParser.TOK_TMP_FILE,"TOK_TMP_FILE"));
-    insert.addChild(destination);
-    destination.addChild(dir);
-    dir.addChild(tmpFile);
-    tmpFile.setParent(dir);
-    dir.setParent(destination);
-    destination.setParent(insert);
-
-    /*TOK_INSERT(TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF))*/
-    ASTNode select = new ASTNode(new CommonToken(HiveParser.TOK_SELECT,"TOK_SELECT"));
-    buildSelExprs(select,subQueryAliases);
-    //ASTNode selExpr = new ASTNode(new CommonToken(HiveParser.TOK_SELEXPR,"TOK_SELEXPR"));
-    //ASTNode allColRef = new ASTNode(new CommonToken(HiveParser.TOK_ALLCOLREF,"TOK_ALLCOLREF"));
-    insert.addChild(select);
-    //select.addChild(selExpr);
-    //selExpr.addChild(allColRef);
-    //allColRef.setParent(selExpr);
-    //selExpr.setParent(select);
-    select.setParent(insert);
-
-    addedSubQuery.add(subQuery);
-  }
-
-  private void buildSelExprs(ASTNode select, String[] subQueryAliases) {
-    for(int i=0;i<subQueryAliases.length;i++){
-      String tabAlias = subQueryAliases[i];
-      String tabName = aliasToTabs.get(tabAlias);
-      try {
-        Table tab = db.getTable(tabName);
-        StructObjectInspector rowObjectInspector = (StructObjectInspector) tab
-        .getDeserializer().getObjectInspector();
-        List<? extends StructField> fields = rowObjectInspector
-        .getAllStructFieldRefs();
-        for (int j = 0; j < fields.size(); j++) {
-          String[] tabCol = new String[2];
-          tabCol[0] = tabAlias;
-          tabCol[1] = fields.get(j).getFieldName();
-          String newColName = tabCol[0] + "_"+ tabCol[1];
-          colNameMap.put(tabCol, newColName);
-          ASTNode selExpr = new ASTNode(new CommonToken(HiveParser.TOK_SELEXPR,"TOK_SELEXPR"));
-          ASTNode dot = new ASTNode(new CommonToken(HiveParser.DOT,"."));
-          ASTNode taborcol = new ASTNode(new CommonToken(HiveParser.TOK_TABLE_OR_COL,"TOK_TABLE_OR_COL"));
-          ASTNode oldTabName = new ASTNode(new CommonToken(HiveParser.Identifier,tabAlias));
-          ASTNode oldColName = new ASTNode(new CommonToken(HiveParser.Identifier,tabCol[1]));
-          ASTNode newCol = new ASTNode(new CommonToken(HiveParser.Identifier,newColName));
-          select.addChild(selExpr);
-          selExpr.setParent(select);
-          selExpr.addChild(dot);
-          dot.setParent(selExpr);
-          dot.addChild(taborcol);
-          taborcol.setParent(dot);
-          dot.addChild(oldColName);
-          oldColName.setParent(dot);
-          taborcol.addChild(oldTabName);
-          oldTabName.setParent(taborcol);
-
-          selExpr.addChild(newCol);
-          newCol.setParent(selExpr);
-        }
-      } catch (HiveException e) {
-        // TODO Auto-generated catch block
-        e.printStackTrace();
-      } catch (SerDeException e) {
-        // TODO Auto-generated catch block
-        e.printStackTrace();
-      }
-    }
-  }
-
-  /**
-   * 1. add TOK_SUBQUERY to right child of incJoin in ASTtrees
-   * 2. change outer's aliases in subQueryAliases to subQuery's name
-   */
-  void addSubQuery(List<ASTNode> ASTtrees, String[] subQueryAliases){
-
-    ASTNode incParent = ASTtrees.get(incPos);
-
-    //add subQuery as incParent's rightChild;
-    LOG.info("add TOK_SUBQUERY to " + incParent.toString() + "'s right child!");
-    insertTokSubQuery(incParent,1,subQueryAliases);
-
-    // change the outer join's condition aliases name in subQueryAliases to subQueryName
-    for(int i=0; i<= incPos; i++){
-      ASTNode joinAST = ASTtrees.get(i);
-      ASTNode condn = (ASTNode) joinAST.getChild(2);
-      changeConditionAlias(condn,subQueryName,subQueryAliases,false);
-
-    }
-
-  }
-/**
- * change aliases of condtion in TOK_WHERE, TOK_JOIN
- * the condition can be connect with "and", "or"
- * @param Cond
- * @param destName
- * @param changedAliases
- */
-  private boolean changeConditionAlias(ASTNode Cond, String destName,String[] changedAliases, boolean isDelCondn) {
-    //TODO:between
-    if (Cond == null) {
-      return false;
-    }
-
-    boolean isDel = false;
-    switch (Cond.getToken().getType()) {
-
-    case HiveParser.KW_OR:
-    case HiveParser.KW_AND:
-      boolean isDelect0 = changeConditionAlias((ASTNode) Cond.getChild(0), destName, changedAliases,isDelCondn);
-      boolean isDelect1 = changeConditionAlias((ASTNode) Cond.getChild(1), destName, changedAliases,isDelCondn);
-      if(isDelect0 && isDelect1){
-        isDel = true;
-      }else if(isDelect0 && !isDelect1){
-        int pos = Cond.getChildIndex();
-        Cond.getParent().setChild(pos, (ASTNode) Cond.getChild(1));
-        LOG.info("Delete " + Cond.getChild(0).getText());
-      }else if(!isDelect0 && isDelect1){
-        int pos = Cond.getChildIndex();
-        Cond.getParent().setChild(pos, (ASTNode) Cond.getChild(0));
-        LOG.info("Delete " + Cond.getChild(1).getText());
-      }
-
-      break;
-    case HiveParser.TOK_FUNCTION:
-      for(int i=1;i<Cond.getChildCount();i++){// i=0 is the function name, i=1..n is function parameter
-        ASTNode tmp = (ASTNode) Cond.getChild(i);
-        boolean isDelete = false;
-        if(tmp.getToken().getType() == HiveParser.DOT){
-          isDelete = changTabColwithDot(tmp,destName,changedAliases,isDelCondn);
-        }else if(tmp.getToken().getType() == HiveParser.TOK_TABLE_OR_COL){
-          isDelete = changColName(tmp,isDelCondn);
-        }else{
-          isDelete = changeConditionAlias(tmp, destName, changedAliases,isDelCondn);
-        }
-        if(isDelete){
-          isDel = true;
-        }
-      }
-      break;
-    case HiveParser.EQUAL_NS:
-    case HiveParser.EQUAL:
-    case HiveParser.KW_LIKE:
-    case HiveParser.GREATERTHAN:
-    case HiveParser.GREATERTHANOREQUALTO:
-    case HiveParser.LESSTHAN:
-    case HiveParser.LESSTHANOREQUALTO:
-    case HiveParser.NOTEQUAL:
-    case HiveParser.STAR:
-    case HiveParser.MINUS:
-    case HiveParser.PLUS:
-    case HiveParser.DIV:
-    case HiveParser.DIVIDE:
-      boolean isDelete = false;
-      ASTNode leftAST = (ASTNode) Cond.getChild(0);
-      if(leftAST.getToken().getType() == HiveParser.DOT){
-        isDelete = changTabColwithDot(leftAST,destName,changedAliases,isDelCondn);
-      }else if(leftAST.getToken().getType() == HiveParser.TOK_TABLE_OR_COL){
-        isDelete = changColName(leftAST,isDelCondn);
-      }else if(leftAST.getToken().getType() == HiveParser.StringLiteral
-          || leftAST.getToken().getType() == HiveParser.Number){
-        isDelete = false;
-      }else{
-        isDelete = changeConditionAlias(leftAST, destName, changedAliases,isDelCondn);
-      }
-      if(isDelete){
-        isDel = true;
-      }
-
-      ASTNode rightAST = (ASTNode) Cond.getChild(1);
-      if(rightAST.getToken().getType() == HiveParser.DOT){
-        isDelete = changTabColwithDot(rightAST,destName,changedAliases,isDelCondn);
-      }else if(rightAST.getToken().getType() == HiveParser.TOK_TABLE_OR_COL){
-        isDelete = changColName(rightAST,isDelCondn);
-      }else if(rightAST.getToken().getType() == HiveParser.StringLiteral
-          || rightAST.getToken().getType() == HiveParser.Number){
-        isDelete = false;
-      }else{
-        isDelete = changeConditionAlias(rightAST, destName, changedAliases,isDelCondn);
-      }
-      if(isDelete){
-        isDel = true;
-      }
-
-      break;
-      default:
-        isDel = false;
-
-    }
-    return isDel;
-  }
-
-  private boolean changColName(ASTNode toktableorcol,boolean isDelCondn) {
-    String colName = toktableorcol.getChild(0).getText().toLowerCase();
-    Iterator<Entry<String[], String>> iterator = colNameMap.entrySet().iterator();
-    while(iterator.hasNext()){// change colName
-      Entry<String[], String> entry = iterator.next();
-      String[] oldtabAndCol = entry.getKey();
-      String newcol = entry.getValue();
-      if((oldtabAndCol[1].equals(colName) || newcol.equals(colName)) && !tmpTableCols.contains(newcol) && isDelCondn){
-        return true;
-      }
-      if(oldtabAndCol[1].equals(colName)){
-        ASTNode tmpAlias = new ASTNode(new CommonToken(HiveParser.Identifier,newcol));
-        tmpAlias.setParent(toktableorcol);
-        toktableorcol.setChild(0,tmpAlias);
-        LOG.info("change " + oldtabAndCol[1] + " to "+ entry.getValue() + " in join left condition");
-      }
-    }
-    return false;
-  }
-
-  private boolean changTabColwithDot(ASTNode dot, String destName, String[] changedAliases,boolean isDelCondn) {
-    ASTNode left = (ASTNode) dot.getChild(0);
-    ASTNode right = (ASTNode) dot.getChild(1);
-    String tab = unescapeIdentifier(left.getChild(0).getText()
-        .toLowerCase());
-    String colName = right.getText().toLowerCase();
-    for(int j=0;j < changedAliases.length; j++){ // change tabName
-      if(changedAliases[j].equals(tab)){
-        ASTNode tmpAlias = new ASTNode(new CommonToken(HiveParser.Identifier,destName));
-        tmpAlias.setParent(left);
-        left.setChild(0,tmpAlias);
-        LOG.info("change " + tab + " to "+ destName + " in join left condition");
-      }
-    }
-    Iterator<Entry<String[], String>> iterator = colNameMap.entrySet().iterator();
-    while(iterator.hasNext()){// change colName
-      Entry<String[], String> entry = iterator.next();
-      String[] tabAndCol = entry.getKey();
-      String newcol = entry.getValue();
-      if((tabAndCol[1].equals(colName) || newcol.equals(colName)) && !tmpTableCols.contains(newcol) && isDelCondn){
-        return true;
-      }
-      if(tabAndCol[0].equals(tab) && tabAndCol[1].equals(colName)){
-        ASTNode tmpAlias = new ASTNode(new CommonToken(HiveParser.Identifier,newcol));
-        tmpAlias.setParent(dot);
-        dot.setChild(1,tmpAlias);
-        LOG.info("change " + tabAndCol[1] + " to "+ entry.getValue() + " in join left condition");
-      }
-    }
-    return false;
-  }
-
-  public ASTNode doExtractTmpCompile() throws SemanticException{
-    //FileSinkOperator fs = (FileSinkOperator) analyzeTmp();
-    LOG.info("Begin insert FileSinkOperator.");
-
-    FileSinkOperator fs = (FileSinkOperator) createFileSinkOperator(qb);
-    fs.setParentOperators(RStmpPos.get(0).getParentOperators());
-    for(Operator parent :RStmpPos.get(0).getParentOperators()){
-      parent.setChildOperators(Utilities.makeList(fs));
-    }
-
-    changPCtx(pCtxInc);
-    LOG.info("After insert FileSinkOperator, the extract tmp tree is: " + Operator.toString(pCtxInc.getTopOps().values()));
-
-    // Generate column access stats if required - wait until column pruning takes place
-    // during optimization
-    if (HiveConf.getBoolVar(this.conf, HiveConf.ConfVars.HIVE_STATS_COLLECT_SCANCOLS) == true) {
-      ColumnAccessAnalyzer columnAccessAnalyzer = new ColumnAccessAnalyzer(pCtxInc);
-      setColumnAccessInfo(columnAccessAnalyzer.analyzeColumnAccess());
-    }
-
-    if (!ctx.getExplainLogical()) {
-      // At this point we have the complete operator tree
-      // from which we want to create the map-reduce plan
-      MapReduceCompiler compiler = new MapReduceCompiler();
-      compiler.init(conf, console, db);
-      compiler.compile(pCtxInc, rootTasks, inputs, outputs);
-      fetchTask = pCtxInc.getFetchTask();
-    }
-
-    LOG.info("Completed plan generation");
-
-    updateAST();
-
-    analyzeAlias(ast,crtTmpTablename,true);
-    LOG.info("After updateAST and analyzeAlias, AST is: " + ast.dump());
-    return ast;
-    //testAST(root);
-  }
-
-  private void testAST(ASTNode root) throws SemanticException {
-    QB qbtest = new QB(null, null, false);
-    // continue analyzing from the child ASTNode.
-    Phase1Ctx ctx_1 = initPhase1Ctx();
-    if (!doPhase1(root, qbtest, ctx_1)) {
-      // if phase1Result false return
-      LOG.info("test AST which include tabref tmp wrong!");
       return;
     }
-    //getMetaData(qbtest);
 
-    //Operator sinkOp = genPlan(qbtest);
-    LOG.info("no error in testAST which include tabref tmp!");
-  }
-
-  /*creat a new FileSink Operator to write data to tmp table*/
-  private Operator createFileSinkOperator(QB qb) throws SemanticException{
-
-    String dest = "insclause-"+crtTmpTablename;
-    assert RStmpPos.get(0).getParentOperators().size() == 1;
-    Operator input = RStmpPos.get(0).getParentOperators().get(0);
-
-    ASTNode tab = new ASTNode(new CommonToken(HiveParser.TOK_TAB,"TOK_TAB"));
-    ASTNode tabname = new ASTNode(new CommonToken(HiveParser.TOK_TABNAME,"TOK_TABNAME"));
-    ASTNode crtTmpTab = new ASTNode(new CommonToken(HiveParser.Identifier,crtTmpTablename));
-    tab.addChild(tabname);
-    tabname.addChild(crtTmpTab);
-    crtTmpTab.setParent(tabname);
-    tabname.setParent(tab);
-    tableSpec ts = new tableSpec(db, conf, tab);
-
-    RowResolver inputRR = opParseCtx.get(input).getRowResolver();
-    QBMetaData qbm = qb.getMetaData();
-
-    if (ts.tableHandle.isView()) {
-      throw new SemanticException(ErrorMsg.DML_AGAINST_VIEW.getMsg());
-    }
-
-    Class<?> outputFormatClass = ts.tableHandle.getOutputFormatClass();
-    if (!HiveOutputFormat.class.isAssignableFrom(outputFormatClass)) {
-      throw new SemanticException(ErrorMsg.INVALID_OUTPUT_FORMAT_TYPE
-          .getMsg(ast, "The class is " + outputFormatClass.toString()));
-    }
-
-    // tableSpec ts is got from the query (user specified),
-    // which means the user didn't specify partitions in their query,
-    // but whether the table itself is partitioned is not know.
-    if (ts.specType != SpecType.STATIC_PARTITION) {
-      // This is a table or dynamic partition
-      qb.getMetaData().setDestForAlias(dest, ts.tableHandle);
-      // has dynamic as well as static partitions
-      if (ts.partSpec != null && ts.partSpec.size() > 0) {
-        qb.getMetaData().setPartSpecForAlias(dest, ts.partSpec);
-      }
-    } else {
-      // This is a partition
-      qb.getMetaData().setDestForAlias(dest, ts.partHandle);
-    }
-    if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {
-      // Set that variable to automatically collect stats during the MapReduce job
-      qb.getParseInfo().setIsInsertToTable(true);
-      // Add the table spec for the destination table.
-      qb.getParseInfo().addTableSpec(ts.tableName.toLowerCase(), ts);
-    }
-
-    Table dest_tab = null; // destination table if any
-    Partition dest_part = null;// destination partition if any
-    String queryTmpdir = null; // the intermediate destination directory
-    Path dest_path = null; // the final destination directory
-    TableDesc table_desc = null;
-    int currentTableId = 0;
-    boolean isLocal = false;
-    SortBucketRSCtx rsCtx = new SortBucketRSCtx();
-    DynamicPartitionCtx dpCtx = null;
-    LoadTableDesc ltd = null;
-    boolean holdDDLTime = checkHoldDDLTime(qb);
-    ListBucketingCtx lbCtx = null;
-
-    dest_tab = ts.tableHandle;
-    // Is the user trying to insert into a external tables
-    if ((!conf.getBoolVar(HiveConf.ConfVars.HIVE_INSERT_INTO_EXTERNAL_TABLES)) &&
-        (dest_tab.getTableType().equals(TableType.EXTERNAL_TABLE))) {
-      throw new SemanticException(
-          ErrorMsg.INSERT_EXTERNAL_TABLE.getMsg(dest_tab.getTableName()));
-    }
-
-    Map<String, String> partSpec = ts.partSpec;
-    dest_path = dest_tab.getPath();
-
-    // check for partition
-    List<FieldSchema> parts = dest_tab.getPartitionKeys();
-    if (parts != null && parts.size() > 0) { // table is partitioned
-      if (partSpec == null || partSpec.size() == 0) { // user did NOT specify partition
-        throw new SemanticException(generateErrorMessage(
-            qb.getParseInfo().getDestForClause(dest),
-            ErrorMsg.NEED_PARTITION_ERROR.getMsg()));
-      }
-      // the HOLD_DDLTIIME hint should not be used with dynamic partition since the
-      // newly generated partitions should always update their DDLTIME
-      if (holdDDLTime) {
-        throw new SemanticException(generateErrorMessage(
-            qb.getParseInfo().getDestForClause(dest),
-            ErrorMsg.HOLD_DDLTIME_ON_NONEXIST_PARTITIONS.getMsg()));
-      }
-      dpCtx = qbm.getDPCtx(dest);
-      if (dpCtx == null) {
-        Utilities.validatePartSpec(dest_tab, partSpec);
-        dpCtx = new DynamicPartitionCtx(dest_tab, partSpec,
-            conf.getVar(HiveConf.ConfVars.DEFAULTPARTITIONNAME),
-            conf.getIntVar(HiveConf.ConfVars.DYNAMICPARTITIONMAXPARTSPERNODE));
-        qbm.setDPCtx(dest, dpCtx);
-      }
-
-      if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONING)) { // allow DP
-        // turn on hive.task.progress to update # of partitions created to the JT
-        HiveConf.setBoolVar(conf, HiveConf.ConfVars.HIVEJOBPROGRESS, true);
-
-      } else { // QBMetaData.DEST_PARTITION capture the all-SP case
-        throw new SemanticException(generateErrorMessage(
-            qb.getParseInfo().getDestForClause(dest),
-            ErrorMsg.DYNAMIC_PARTITION_DISABLED.getMsg()));
-      }
-      if (dpCtx.getSPPath() != null) {
-        dest_path = new Path(dest_tab.getPath(), dpCtx.getSPPath());
-      }
-      if ((dest_tab.getNumBuckets() > 0) &&
-          (conf.getBoolVar(HiveConf.ConfVars.HIVEENFORCEBUCKETING))) {
-        dpCtx.setNumBuckets(dest_tab.getNumBuckets());
-      }
-    }
-
-    boolean isNonNativeTable = dest_tab.isNonNative();
-    //boolean isNonNativeTable = false;
-    if (isNonNativeTable) {
-      queryTmpdir = dest_path.toUri().getPath();
-    } else {
-      queryTmpdir = ctx.getExternalTmpFileURI(dest_path.toUri());
-    }
-    if (dpCtx != null) {
-      // set the root of the temporay path where dynamic partition columns will populate
-      dpCtx.setRootPath(queryTmpdir);
-    }
-    // this table_desc does not contain the partitioning columns
-    table_desc = Utilities.getTableDesc(dest_tab);
-
-    // Add sorting/bucketing if needed
-    input = genBucketingSortingDest(dest, input, qb, table_desc, dest_tab, rsCtx);
-
-    idToTableNameMap.put(String.valueOf(destTableId), dest_tab.getTableName());
-    currentTableId = destTableId;
-    destTableId++;
-
-    lbCtx = constructListBucketingCtx(dest_tab.getSkewedColNames(),
-        dest_tab.getSkewedColValues(), dest_tab.getSkewedColValueLocationMaps(),
-        dest_tab.isStoredAsSubDirectories(), conf);
-
-    // Create the work for moving the table
-    // NOTE: specify Dynamic partitions in dest_tab for WriteEntity
-    if (!isNonNativeTable) {
-      ltd = new LoadTableDesc(queryTmpdir, ctx.getExternalTmpFileURI(dest_path.toUri()),
-          table_desc, dpCtx);
-      ltd.setReplace(!qb.getParseInfo().isInsertIntoTable(dest_tab.getDbName(),
-          dest_tab.getTableName()));
-      ltd.setLbCtx(lbCtx);
-
-      if (holdDDLTime) {
-        LOG.info("this query will not update transient_lastDdlTime!");
-        ltd.setHoldDDLTime(true);
-      }
-      loadTableWork.add(ltd);
-    }
-
-    WriteEntity output = null;
-
-    // Here only register the whole table for post-exec hook if no DP present
-    // in the case of DP, we will register WriteEntity in MoveTask when the
-    // list of dynamically created partitions are known.
-    if ((dpCtx == null || dpCtx.getNumDPCols() == 0)) {
-      output = new WriteEntity(dest_tab);
-      if (!outputs.add(output)) {
-        throw new SemanticException(ErrorMsg.OUTPUT_SPECIFIED_MULTIPLE_TIMES
-            .getMsg(dest_tab.getTableName()));
-      }
-    }
-    if ((dpCtx != null) && (dpCtx.getNumDPCols() >= 0)) {
-      // No static partition specified
-      if (dpCtx.getNumSPCols() == 0) {
-        output = new WriteEntity(dest_tab, false);
-        outputs.add(output);
-      }
-      // part of the partition specified
-      // Create a DummyPartition in this case. Since, the metastore does not store partial
-      // partitions currently, we need to store dummy partitions
-      else {
-        try {
-          String ppath = dpCtx.getSPPath();
-          ppath = ppath.substring(0, ppath.length() - 1);
-          DummyPartition p =
-              new DummyPartition(dest_tab, dest_tab.getDbName()
-                  + "@" + dest_tab.getTableName() + "@" + ppath,
-                  partSpec);
-          output = new WriteEntity(p, false);
-          outputs.add(output);
-        } catch (HiveException e) {
-          throw new SemanticException(e.getMessage(), e);
-        }
-      }
-    }
-
-    ctx.getLoadTableOutputMap().put(ltd, output);
-
-
-    input = genConversionSelectOperator(dest, qb, input, table_desc, dpCtx);
-    inputRR = opParseCtx.get(input).getRowResolver();
-
-    ArrayList<ColumnInfo> vecCol = new ArrayList<ColumnInfo>();
-
-    try {
-      StructObjectInspector rowObjectInspector = (StructObjectInspector) table_desc
-          .getDeserializer().getObjectInspector();
-      List<? extends StructField> fields = rowObjectInspector
-          .getAllStructFieldRefs();
-      for (int i = 0; i < fields.size(); i++) {
-        vecCol.add(new ColumnInfo(fields.get(i).getFieldName(), TypeInfoUtils
-            .getTypeInfoFromObjectInspector(fields.get(i)
-            .getFieldObjectInspector()), "", false));
-      }
-    } catch (Exception e) {
-      throw new SemanticException(e.getMessage(), e);
-    }
-
-    RowSchema fsRS = new RowSchema(vecCol);
-
-    // The output files of a FileSink can be merged if they are either not being written to a table
-    // or are being written to a table which is either not bucketed or enforce bucketing is not set
-    // and table the table is either not sorted or enforce sorting is not set
-    boolean canBeMerged = (dest_tab == null || !((dest_tab.getNumBuckets() > 0 &&
-        conf.getBoolVar(HiveConf.ConfVars.HIVEENFORCEBUCKETING)) ||
-        (dest_tab.getSortCols() != null && dest_tab.getSortCols().size() > 0 &&
-        conf.getBoolVar(HiveConf.ConfVars.HIVEENFORCESORTING))));
-
-    FileSinkDesc fileSinkDesc = new FileSinkDesc(
-      queryTmpdir,
-      table_desc,
-      conf.getBoolVar(HiveConf.ConfVars.COMPRESSRESULT),
-      currentTableId,
-      rsCtx.isMultiFileSpray(),
-      canBeMerged,
-      rsCtx.getNumFiles(),
-      rsCtx.getTotalFiles(),
-      rsCtx.getPartnCols(),
-      dpCtx);
-
-    /* Set List Bucketing context. */
-    if (lbCtx != null) {
-      lbCtx.processRowSkewedIndex(fsRS);
-      lbCtx.calculateSkewedValueSubDirList();
-    }
-    fileSinkDesc.setLbCtx(lbCtx);
-
-    // set it in plan instead of runtime in FileSinkOperator
-    fileSinkDesc.setStatsCollectRawDataSize(HiveConf.getBoolVar(conf,
-        HiveConf.ConfVars.HIVE_STATS_COLLECT_RAWDATASIZE));
-
-    // set the stats publishing/aggregating key prefix
-    // the same as directory name. The directory name
-    // can be changed in the optimizer but the key should not be changed
-    // it should be the same as the MoveWork's sourceDir.
-    fileSinkDesc.setStatsAggPrefix(fileSinkDesc.getDirName());
-
-    if (dest_part != null) {
-      try {
-        String staticSpec = Warehouse.makePartPath(dest_part.getSpec());
-        fileSinkDesc.setStaticSpec(staticSpec);
-      } catch (MetaException e) {
-        throw new SemanticException(e);
-      }
-    } else if (dpCtx != null) {
-      fileSinkDesc.setStaticSpec(dpCtx.getSPPath());
-    }
-
-    Operator ret = putOpInsertMap(OperatorFactory.getAndMakeChild(fileSinkDesc,
-        fsRS, input), inputRR);
-
-    if (ltd != null && SessionState.get() != null) {
-      SessionState.get().getLineageState()
-          .mapDirToFop(ltd.getSourceDir(), (FileSinkOperator) ret);
-    }
-
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Created FileSink Plan for clause: " + dest + "dest_path: "
-          + dest_path + " row schema: " + inputRR.toString());
-    }
-
-    fsopToTable.put((FileSinkOperator) ret, dest_tab);
-    return ret;
-
-  }
-
-  void changPCtx(ParseContext pCtx){
-
-    /*remove the topOps that are not involved in tmp table*/
-    HashMap<String, Operator<? extends OperatorDesc>> topOps = pCtx.getTopOps();
-    Iterator<Entry<String, Operator<? extends OperatorDesc>>> iter = topOps.entrySet().iterator();
-    while(iter.hasNext()){
-      String key = iter.next().getKey();
-      if(key.length() > crtTmpTablename.length()){
-        if(key.indexOf(crtTmpTablename.toLowerCase()) < 0){
-          iter.remove();
-        }
-      }else{
-        iter.remove();
-      }
-    }
-
-    /*following code is to clear the information about origianl FileSinkOperator
-     * if don't clear, MapReduceCompile and insert information to table after jobs done will be wrong*/
-    pCtx.getQB().setIsQuery(false);
-    //pCtx.getLoadFileWork().clear();
-    ctx.setResDir(null);
-    ctx.setResFile(null);
-  }
-
-  private void analyzeAlias(ASTNode curr, String destName, boolean isFinalAnalyze){
-    //need to handle select,where,groupby,orderby,joinCondition
-    String[] aliasToChanged = new String[topAliasInTmp.size()];
-    for(int i=0;i<topAliasInTmp.size();i++){
-      aliasToChanged[i] = topAliasInTmp.get(i);
-    }
-    if(curr == null){
-      return;
-    }
-    switch(curr.getToken().getType()){
-    case HiveParser.TOK_SELECT:
-    case HiveParser.TOK_SELECTDI:
-      for (int i = 0; i < curr.getChildCount(); ++i) {
-        ASTNode selExpr = (ASTNode) curr.getChild(i);
-        if (selExpr.getToken().getType() == HiveParser.TOK_SELEXPR) {
-          ASTNode tmp = (ASTNode)selExpr.getChild(0);
-          if(tmp.getToken().getType() == HiveParser.DOT && tmp.getChildCount() == 2){
-            changTabColwithDot(tmp,destName,aliasToChanged,false);
-          }else if(tmp.getToken().getType() == HiveParser.TOK_TABLE_OR_COL){
-            changColName(tmp,false);
-          }else{
-            changeConditionAlias(tmp,destName,aliasToChanged,false);
-          }
-        }
-      }
-      break;
-    case HiveParser.TOK_WHERE:
-      ASTNode condn = (ASTNode) curr.getChild(0);
-      boolean isDelete = changeConditionAlias(condn,destName,aliasToChanged,isFinalAnalyze);
-      if(isDelete){
-        int pos = curr.getChildIndex();
-        curr.getParent().deleteChild(pos);
-        LOG.info("delete TOK_WHRER");
-      }
-      break;
-    case HiveParser.TOK_GROUPBY:
-      for (int i = 0; i < curr.getChildCount(); ++i) {
-        ASTNode tmp = (ASTNode)curr.getChild(i);
-        if(tmp.getToken().getType() == HiveParser.DOT && tmp.getChildCount() == 2){
-          changTabColwithDot(tmp,destName,aliasToChanged,false);
-        }else if(tmp.getToken().getType() == HiveParser.TOK_TABLE_OR_COL){
-          changColName(tmp,false);
-        }
-      }
-      break;
-    case HiveParser.TOK_ORDERBY:
-      for (int i = 0; i < curr.getChildCount(); ++i) {
-        ASTNode orderExpr = (ASTNode) curr.getChild(i);
-        if (orderExpr.getToken().getType() == HiveParser.TOK_TABSORTCOLNAMEASC
-            ||orderExpr.getToken().getType() == HiveParser.TOK_TABSORTCOLNAMEDESC) {
-          ASTNode tmp = (ASTNode)orderExpr.getChild(0);
-          if(tmp.getToken().getType() == HiveParser.DOT && tmp.getChildCount() == 2){
-            changTabColwithDot(tmp,destName,aliasToChanged,false);
-          }else if(tmp.getToken().getType() == HiveParser.TOK_TABLE_OR_COL){
-            changColName(tmp,false);
-          }
-        }
-      }
-      break;
-    case HiveParser.TOK_JOIN:
-    case HiveParser.TOK_CROSSJOIN:
-    case HiveParser.TOK_LEFTOUTERJOIN:
-    case HiveParser.TOK_RIGHTOUTERJOIN:
-    case HiveParser.TOK_FULLOUTERJOIN:
-    case HiveParser.TOK_LEFTSEMIJOIN:
-    case HiveParser.TOK_UNIQUEJOIN:
-      if(isFinalAnalyze){
-        condn = (ASTNode) curr.getChild(2);
-        changeConditionAlias(condn,destName,aliasToChanged,true);
-      }
-      break;
-    default:
-
-    }
-
-
-    int child_count = curr.getChildCount();
-    for (int child_pos = 0; child_pos < child_count; ++child_pos) {
-      // Recurse
-      ASTNode next = (ASTNode) curr.getChild(child_pos);
-      if(addedSubQuery.contains(next)){
-        continue;
-      }
-      analyzeAlias(next,destName,isFinalAnalyze);
-    }
-  }
-
-  /**
-   * change the AST's tmpPos's TOK_JOIN to TOK_TABREF
-   */
-  void updateAST(){
-    for(int i=0; i< ASTtmpPos.size(); i++){
-      ASTNode parent = (ASTNode) ASTtmpPos.get(i).getParent();
-
-      ASTNode tabref = new ASTNode(new CommonToken(HiveParser.TOK_TABREF,"TOK_TABREF"));
-      ASTNode tabname = new ASTNode(new CommonToken(HiveParser.TOK_TABNAME,"TOK_TABNAME"));
-      ASTNode crtTmpTab = new ASTNode(new CommonToken(HiveParser.Identifier,crtTmpTablename));
-      ASTNode tmpAlias = new ASTNode(new CommonToken(HiveParser.Identifier,crtTmpTablename));
-
-      int tmpPos = -1;
-      for(int j = 0 ; j<parent.getChildCount(); j++){
-        if(parent.getChild(j) == ASTtmpPos.get(i)){
-          tmpPos = j;
-        }
-      }
-      parent.setChild(tmpPos, tabref);
-      tabref.addChild(tabname);
-      tabref.addChild(tmpAlias);
-      tabname.addChild(crtTmpTab);
-      crtTmpTab.setParent(tabname);
-      tabname.setParent(tabref);
-      tmpAlias.setParent(tabref);
-      tabref.setParent(parent);
-    }
-  }
-
-  /**
-   * create whole tmp AST, now not used
-   * @return
-   */
-  ASTNode crtInsertTmpAST(){
-    List<String> tmpColumns = new ArrayList<String>();
-    /*TOK_QUERY(TOK_FROM) TOK_FROM(child)*/
-    ASTNode query = new ASTNode(new CommonToken(HiveParser.TOK_QUERY,"TOK_QUERY"));
-    ASTNode from = new ASTNode(new CommonToken(HiveParser.TOK_FROM,"TOK_FROM"));
-    query.addChild(from);
-    from.setParent(query);
-    from.addChild(ASTtmpPos.get(0));
-    ASTtmpPos.get(0).setParent(from);
-
-    /*TOK_QUERY(TOK_INSERT)*/
-    ASTNode insert = new ASTNode(new CommonToken(HiveParser.TOK_INSERT,"TOK_INSERT"));
-    query.addChild(insert);
-    insert.setParent(query);
-
-    /*TOK_INSERT(TOK_DESTINATION (TOK_TAB (TOK_TABNAME crtTmpTablename)))*/
-    ASTNode tab = new ASTNode(new CommonToken(HiveParser.TOK_TAB,"TOK_TAB"));
-    ASTNode tabname = new ASTNode(new CommonToken(HiveParser.TOK_TABNAME,"TOK_TABNAME"));
-    ASTNode crtTmpTab = new ASTNode(new CommonToken(HiveParser.Identifier,crtTmpTablename));
-    tab.addChild(tabname);
-    tabname.addChild(crtTmpTab);
-    crtTmpTab.setParent(tabname);
-    tabname.setParent(tab);
-
-    ASTNode destination = new ASTNode (new CommonToken(HiveParser.TOK_DESTINATION,"TOK_DESTINATION"));
-    insert.addChild(destination);
-    destination.addChild(tab);
-    tab.setParent(destination);
-    destination.setParent(insert);
-
-
-    /*TOK_INSERT(TOK_SELECT)*/
-    ASTNode select = new ASTNode(new CommonToken(HiveParser.TOK_SELECT,"TOK_SELECT"));
-    insert.addChild(select);
-    select.setParent(insert);
-
-    /*(TOK_SELECT (TOK_SELEXPR ?)*/
-    for(int i=0; i< tmpColumns.size(); i++){
-      ASTNode selExpr = new ASTNode(new CommonToken(HiveParser.TOK_SELEXPR,"TOK_SELEXPR"));
-      ASTNode taborcol = new ASTNode(new CommonToken(HiveParser.TOK_TABLE_OR_COL,"TOK_TABLE_OR_COL"));
-      ASTNode columnName = new ASTNode(new CommonToken(HiveParser.Identifier,tmpColumns.get(i)));
-      select.addChild(selExpr);
-      selExpr.addChild(taborcol);
-      taborcol.addChild(columnName);
-      columnName.setParent(taborcol);
-      taborcol.setParent(selExpr);
-      selExpr.setParent(select);
-    }
-
-    return query;
-  }
-
-  /**
-   * create whole tmp AST and analyze it. now not used
-   * @return
-   * @throws SemanticException
-   */
-  Operator analyzeTmp() throws SemanticException{
-
-    ASTNode insert = crtInsertTmpAST();
-    LOG.info(" AST of insert data to tmp is: " + insert.dump());
-
-    init();
-    ASTNode child = insert;
-    this.ast = insert;
-    viewsExpanded = new ArrayList<String>();
-
-    LOG.info("Starting Semantic Analysis");
-
-    // analyze and process the position alias
-    processPositionAlias(ast);
-
-    // analyze create table command
-    if (ast.getToken().getType() == HiveParser.TOK_CREATETABLE) {
-      // if it is not CTAS, we don't need to go further and just return
-      if ((child = analyzeCreateTable(ast, qb)) == null) {
-        return null;
-      }
-    } else {
-      SessionState.get().setCommandType(HiveOperation.QUERY);
-    }
-
-    // analyze create view command
-    if (ast.getToken().getType() == HiveParser.TOK_CREATEVIEW ||
-        ast.getToken().getType() == HiveParser.TOK_ALTERVIEW_AS) {
-      child = analyzeCreateView(ast, qb);
-      SessionState.get().setCommandType(HiveOperation.CREATEVIEW);
-      if (child == null) {
-        return null;
-      }
-      viewSelect = child;
-      // prevent view from referencing itself
-      viewsExpanded.add(SessionState.get().getCurrentDatabase() + "." + createVwDesc.getViewName());
-    }
-
-    // continue analyzing from the child ASTNode.
-    Phase1Ctx ctx_1 = initPhase1Ctx();
-    if (!doPhase1(child, qb, ctx_1)) {
-      // if phase1Result false return
-      return null;
-    }
-
-    LOG.info("Completed phase 1 of Semantic Analysis");
-
-    getMetaData(qb);
-    LOG.info("Completed getting MetaData in Semantic Analysis");
-
-    // Save the result schema derived from the sink operator produced
-    // by genPlan. This has the correct column names, which clients
-    // such as JDBC would prefer instead of the c0, c1 we'll end
-    // up with later.
-    Operator sinkOp = genPlan(qb);
-
-
-    resultSchema =
-        convertRowSchemaToViewSchema(opParseCtx.get(sinkOp).getRowResolver());
-
-    if (createVwDesc != null) {
-      saveViewDefinition();
-
-      // validate the create view statement
-      // at this point, the createVwDesc gets all the information for semantic check
-      validateCreateView(createVwDesc);
-
-      // Since we're only creating a view (not executing it), we
-      // don't need to optimize or translate the plan (and in fact, those
-      // procedures can interfere with the view creation). So
-      // skip the rest of this method.
-      ctx.setResDir(null);
-      ctx.setResFile(null);
-      return null;
-    }
-
     ParseContext pCtx = new ParseContext(conf, qb, child, opToPartPruner,
         opToPartList, topOps, topSelOps, opParseCtx, joinContext, smbMapJoinContext,
         topToTable, topToTableProps, fsopToTable,
@@ -12685,7 +11090,7 @@
 
     LOG.info("Completed plan generation");
 
-    return sinkOp;
+    return;
   }
 
-}
+}
\ No newline at end of file
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g	(revision 106)
@@ -24,9 +24,17 @@
 backtrack=false;
 k=3;
 }
-import SelectClauseParser, FromClauseParser, IdentifiersParser;
+import IdentifiersParser,SelectClauseParser, FromClauseParser;
 
 tokens {
+
+TOK_CONSTANT;
+TOK_STARTTIME;
+TOK_STOPTIME;
+TOK_INTERVAL;
+TOK_INCRE;
+TOK_TIME;
+
 TOK_INSERT;
 TOK_QUERY;
 TOK_SELECT;
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/SelectClauseParser.g
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/SelectClauseParser.g	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/SelectClauseParser.g	(revision 106)
@@ -24,6 +24,10 @@
 k=3;
 }
 
+tokens {
+INT;
+}
+
 @members {
   @Override
   public Object recoverFromMismatchedSet(IntStream input,
@@ -103,6 +107,7 @@
 @after { gParent.msgs.pop(); }
     :
     KW_MAPJOIN -> TOK_MAPJOIN
+    | KW_INCRE -> TOK_INCRE
     | KW_STREAMTABLE -> TOK_STREAMTABLE
     | KW_HOLD_DDLTIME -> TOK_HOLD_DDLTIME
     ;
@@ -157,6 +162,86 @@
     selectExpression (COMMA selectExpression)* -> ^(TOK_EXPLIST selectExpression+)
     ;
 
+  
+//---------------------- Rules for incremental  -------------------------------
+
+incrementalClause
+@init { gParent.msgs.push("incremental clause"); }
+@after { gParent.msgs.pop(); }
+    :
+    DIVIDE STAR PLUS KW_INCRE LPAREN incrementalArgs RPAREN STAR DIVIDE 
+    	->  ^(TOK_INCRE incrementalArgs?)
+    ;
+
+incrementalArgs
+@init { gParent.msgs.push("incremental arguments"); }
+@after { gParent.msgs.pop(); }
+    :
+     bdate=betweenDate 
+    ->  $bdate
+    | KW_CONSTANT time=Number unit0=identifier (interval=KW_INTERVAL numerator=Number DIVIDE unit1=identifier)? 
+     ->{interval != null}?^(TOK_CONSTANT  $time $unit0 ) ^(TOK_INTERVAL $numerator $unit1)
+     ->^(TOK_CONSTANT  $time  $unit0) 
+    | KW_AFTER sdate=startDate (interval=KW_INTERVAL numerator=Number DIVIDE unit2=identifier)?
+     ->{interval != null}? $sdate ^(TOK_INTERVAL $numerator $unit2)
+     -> $sdate
+    ;
+
+intervalUnit
+@init { gParent.msgs.push("interval Unit"); }
+@after { gParent.msgs.pop(); }
+    :
+    KW_HOUR | KW_ SECOND | KW_DAY  | KW_WEEK | KW_MONTH | KW_MINUTE 
+    ;
+    
+betweenDate
+@init { gParent.msgs.push("between Date "); }
+@after { gParent.msgs.pop(); }
+    :
+    stime=dateArgs MINUS  etime=dateArgs
+    ->^(TOK_STARTTIME $stime)  ^(TOK_STOPTIME $etime)
+    ;
+
+startDate
+@init { gParent.msgs.push("startDate "); }
+@after { gParent.msgs.pop(); }
+    :
+     stime=dateArgs
+    ->^(TOK_STARTTIME $stime )
+    ;
+
+dateArgs
+@init { gParent.msgs.push("incremental arguments"); }
+@after { gParent.msgs.pop(); }
+    :
+     it=indefinite_date (c=COMMA et=explicit_time)?
+     ->{c != null}?^(TOK_DATETIME $it $et)
+     ->^(TOK_DATETIME $it )
+    ;
+    
+indefinite_date
+@init { gParent.msgs.push("indefinite_time"); }
+@after { gParent.msgs.pop(); }
+    :
+   (expr+=year_or_month)* day=Number
+     ->^(TOK_DATE  $expr*  $day)
+    ;
+
+year_or_month
+@init { gParent.msgs.push("year_month"); }
+@after { gParent.msgs.pop(); } 
+     :
+      Number DIVIDE 
+     ;
+     
+explicit_time
+@init { gParent.msgs.push("explicit_time"); }
+@after { gParent.msgs.pop(); }
+ : hour=Number COLON minute=Number (COLON second=Number)? 
+      -> ^(TOK_TIME $hour $minute $second?) 
+ ;
+ 
+
 //---------------------- Rules for windowing clauses -------------------------------
 window_clause 
 @init { gParent.msgs.push("window_clause"); }
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g	(revision 106)
@@ -31,6 +31,18 @@
 KW_IF : 'IF';
 KW_EXISTS : 'EXISTS';
 
+KW_CONSTANT : 'CONSTANT';
+KW_INTERVAL : 'INTERVAL';
+KW_INCRE : 'INCREMENTAL';
+// ********** date rules ********** 
+//KW_HOUR   : 'HR' ;
+//KW_MINUTE : 'MIN' ;
+//KW_SECOND : 'SEC' ;
+//KW_DAY    : 'DAY' ;
+//KW_WEEK   : 'WEK' ;
+//KW_MONTH  : 'MOH' ;
+//KW_YEAR   : 'YR' ;
+
 KW_ASC : 'ASC';
 KW_DESC : 'DESC';
 KW_ORDER : 'ORDER';
@@ -300,6 +312,7 @@
 QUESTION : '?';
 DOLLAR : '$';
 
+
 // LITERALS
 fragment
 Letter
@@ -373,6 +386,11 @@
     :
     (Digit)+ ( DOT (Digit)* (Exponent)? | Exponent)?
     ;
+
+TimeUnit
+    :
+    (Digit)+ (DOT (Digit)*)? '(' ('d' | 'h' | 'm' | 's' | 'D' | 'H' | 'M' | 'S') ')'
+    ;
     
 Identifier
     :
Index: ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java	(revision 106)
@@ -20,7 +20,6 @@
 
 import java.util.ArrayList;
 import java.util.Iterator;
-import java.util.LinkedList;
 import java.util.List;
 
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
Index: ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java	(revision 106)
@@ -23,12 +23,14 @@
 import java.net.URI;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Calendar;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
+import java.util.Set;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -963,4 +965,33 @@
       throw new RuntimeException("Cannot get path ", e);
     }
   }
-};
+
+  public ArrayList<Path> getIncFilePaths(Calendar  startDate , Calendar endDate){
+    ArrayList<Path> res = new ArrayList<Path>();
+    ArrayList<Path> paths = new ArrayList<Path>();
+    try {
+      if(this.isPartitioned()){
+        Set<Partition> parts = Hive.get().getAllPartitionsForPruner(this);
+        for( Partition part : parts){
+          paths.add(part.getPartitionPath());
+        }
+      }else{
+        paths.add(this.getPath());
+      }
+      for( Path p : paths){
+        String path = p.toString() + "/*";
+        FileSystem fs = FileSystem.get(p.toUri(), Hive.get().getConf());
+        MetaStoreUtils.addPath(fs , path , res ,  startDate ,  endDate);
+      }
+    } catch (Exception e) {
+      throw new RuntimeException("Cannot get path", e);
+    }
+    if( LOG.isDebugEnabled() ){
+      for( Path src : res){
+        LOG.info("Got incremtnal files : " + src);
+      }
+    }
+    return res;
+  }
+
+}
Index: ql/src/java/org/apache/hadoop/hive/ql/DriverContext.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/DriverContext.java	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/DriverContext.java	(revision 106)
@@ -23,7 +23,6 @@
 import java.util.Queue;
 
 import org.apache.hadoop.hive.ql.exec.Task;
-import org.apache.hadoop.mapred.JobConf;
 
 /**
  * DriverContext.
@@ -82,5 +81,4 @@
   public void incCurJobNo(int amount) {
     this.curJobNo = this.curJobNo + amount;
   }
-  
 }
Index: ql/src/java/org/apache/hadoop/hive/ql/log/PerfLogger.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/log/PerfLogger.java	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/log/PerfLogger.java	(revision 106)
@@ -57,6 +57,7 @@
   public static final String FAILURE_HOOK = "FailureHook.";
   public static final String DRIVER_RUN = "Driver.run";
   public static final String MULTIDRIVER_RUN = "MultiDriver.run";
+  public static final String INCRIVER_RUN = "IncDriver.run";
   public static final String TIME_TO_SUBMIT = "TimeToSubmit";
 
   protected static final ThreadLocal<PerfLogger> perfLogger = new ThreadLocal<PerfLogger>();
Index: ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java	(revision 106)
@@ -119,6 +119,11 @@
     this.maxRows = maxRows;
   }
 
+  // For incremental execution only
+  public void resetTotalRows() {
+    totalRows = 0;
+  }
+
   @Override
   public boolean fetch(ArrayList<String> res) throws IOException, CommandNeedRetryException {
     sink.reset(res);
Index: ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java	(revision 106)
@@ -506,4 +506,12 @@
   void setException(Throwable ex) {
     exception = ex;
   }
+  public void resetStatus(){
+    isdone = false;
+    started = false;
+    initialized = false;
+    queued = false;
+    this.taskCounters = new HashMap<String, Long>();
+    taskTag = Task.NO_TAG;
+  }
 }
Index: ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java	(revision 106)
@@ -1588,4 +1588,8 @@
         }
       }
   }
+
+  public void resetJobCloseDone() {
+     jobCloseDone = false;
+  }
 }
Index: ql/src/java/org/apache/hadoop/hive/ql/exec/ListSinkOperator.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/exec/ListSinkOperator.java	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/exec/ListSinkOperator.java	(revision 106)
@@ -86,6 +86,7 @@
     return numRows;
   }
 
+  @Override
   public void processOp(Object row, int tag) throws HiveException {
     try {
       res.add(mSerde.serialize(row, outputObjInspector).toString());
@@ -95,6 +96,7 @@
     }
   }
 
+  @Override
   public OperatorType getType() {
     return OperatorType.FORWARD;
   }
Index: ql/src/java/org/apache/hadoop/hive/ql/processors/CommandProcessorFactory.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/processors/CommandProcessorFactory.java	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/processors/CommandProcessorFactory.java	(revision 106)
@@ -25,6 +25,7 @@
 
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.IncDriver;
 import org.apache.hadoop.hive.ql.session.SessionState;
 
 /**
@@ -38,7 +39,7 @@
   }
 
   static Map<HiveConf, Driver> mapDrivers = new HashMap<HiveConf, Driver>();
-  //static Map<HiveConf, MultiDriver> multiMapDrivers = new HashMap<HiveConf, MultiDriver>();
+  static Map<HiveConf, IncDriver> mapIncDrivers = new HashMap<HiveConf, IncDriver>();
   public static CommandProcessor get(String cmd) {
     return get(cmd, null);
   }
@@ -57,19 +58,16 @@
       return new AddResourceProcessor();
     } else if ("delete".equals(cmdl)) {
       return new DeleteResourceProcessor();
-    } /*else if( ((HiveConf)conf).getBoolVar(HiveConf.ConfVars.HIVEMULTIQUERY)){
-      MultiDriver multiDrv = multiMapDrivers.get(conf);
-	  if ( multiDrv == null ){
-	      multiDrv = new MultiDriver();
-	      multiMapDrivers.put(conf , multiDrv);
-	  }
-	  multiDrv.init();
-	  return multiDrv;
-	} */else if (!isBlank(cmd)) {
+    }else if (!isBlank(cmd)) {
       if (conf == null) {
         return new Driver();
       }
-
+      if(((HiveConf)conf).getBoolVar(HiveConf.ConfVars.HIVEINCQUERY)){
+        IncDriver incDrv = new IncDriver();
+        incDrv.init();
+        mapIncDrivers.put(conf, incDrv);
+        return incDrv;
+      }
       Driver drv = mapDrivers.get(conf);
       if (drv == null) {
         drv = new Driver();
@@ -87,7 +85,19 @@
     if (drv != null) {
       drv.destroy();
     }
-
+    IncDriver incDrv = mapIncDrivers.get(conf);
+    if(incDrv != null){
+      incDrv.destroy();
+    }
     mapDrivers.remove(conf);
+    mapIncDrivers.remove(conf);
   }
+
+  public static IncDriver getIncDriver(HiveConf conf){
+    if(mapIncDrivers.containsKey(conf)){
+      return mapIncDrivers.get(conf);
+    }
+    return null;
+  }
+
 }
Index: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
===================================================================
--- ql/src/java/org/apache/hadoop/hive/ql/Driver.java	(revision 104)
+++ ql/src/java/org/apache/hadoop/hive/ql/Driver.java	(revision 106)
@@ -424,7 +424,6 @@
       ASTNode tree = pd.parse(command, ctx);
       tree = ParseUtils.findRootNonNullToken(tree);
       perfLogger.PerfLogEnd(LOG, PerfLogger.PARSE);
-
       perfLogger.PerfLogBegin(LOG, PerfLogger.ANALYZE);
       BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(conf, tree);
       List<HiveSemanticAnalyzerHook> saHooks =
@@ -438,21 +437,7 @@
         for (HiveSemanticAnalyzerHook hook : saHooks) {
           tree = hook.preAnalyze(hookCtx, tree);
         }
-        //kangyanli added begin
-        if(((HiveConf) conf).getBoolVar(HiveConf.ConfVars.HIVEINC) && sem instanceof SemanticAnalyzer){
-          ASTNode astInc =((SemanticAnalyzer) sem).genAndrunCrtTmpQuery(tree,ctx);
-          if(((HiveConf) conf).getBoolVar(HiveConf.ConfVars.HIVEINCTESTAST) && astInc != null){//when creat table, astInc is null
-            LOG.info("testAST Begin!");
-            LOG.info(" the ast is : "+ astInc.dump());
-            sem = SemanticAnalyzerFactory.get(conf, astInc);
-            sem.analyze(astInc, ctx);
-            LOG.info("testAST end! ");
-          }
-        }
-        //kangyanli added end
-        else{
-          sem.analyze(tree, ctx);
-        }
+        sem.analyze(tree, ctx);
         hookCtx.update(sem);
         for (HiveSemanticAnalyzerHook hook : saHooks) {
           hook.postAnalyze(hookCtx, sem.getRootTasks());
